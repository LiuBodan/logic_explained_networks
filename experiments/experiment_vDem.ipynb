{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "\n",
    "from lens.models.relu_nn import XReluNN\n",
    "from lens.models.psi_nn import PsiNetwork\n",
    "from lens.models.tree import XDecisionTreeClassifier\n",
    "from lens.models.brl import XBRLClassifier\n",
    "from lens.models.deep_red import XDeepRedClassifier\n",
    "from lens.utils.base import set_seed, ClassifierNotTrainedError, IncompatibleClassifierError\n",
    "from lens.utils.metrics import Accuracy, F1Score\n",
    "from lens.models.general_nn import XGeneralNN\n",
    "from lens.utils.datasets import StructuredDataset\n",
    "from lens.logic.base import test_explanation\n",
    "from lens.logic.metrics import complexity, fidelity, formula_consistency\n",
    "from data import VDEM\n",
    "from data.load_structured_datasets import load_vDem\n",
    "\n",
    "# n_sample = 100\n",
    "results_dir = f'results/vDem'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading VDEM data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_root = \"../data/\"\n",
    "dataset_name = VDEM\n",
    "print(dataset_root)\n",
    "print(results_dir)\n",
    "x, c, y, feature_names, concept_names, class_names = load_vDem(dataset_root)\n",
    "y = y.argmax(dim=1)\n",
    "n_features = x.shape[1]\n",
    "n_concepts = c.shape[1]\n",
    "n_classes = len(class_names)\n",
    "dataset_low = StructuredDataset(x, c, dataset_name=dataset_name, feature_names=feature_names, class_names=concept_names)\n",
    "print(\"Number of features\", n_features)\n",
    "print(\"Number of concepts\", n_concepts)\n",
    "print(\"Feature names\", feature_names)\n",
    "print(\"Concept names\", concept_names)\n",
    "print(\"Class names\", class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define loss, metrics and methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_low = BCEWithLogitsLoss()\n",
    "loss_high = CrossEntropyLoss()\n",
    "metric = Accuracy()\n",
    "expl_metric = F1Score()\n",
    "method_list = ['DTree', 'BRL', 'Psi', 'Relu', 'General']  # 'DeepRed']\n",
    "print(\"Methods\", method_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_processes = 4\n",
    "timeout = 60 * 60  # 1 h timeout\n",
    "l_r = 1e-3\n",
    "lr_scheduler = False\n",
    "top_k_explanations = None\n",
    "simplify = True\n",
    "seeds = [*range(5)]\n",
    "print(\"Seeds\", seeds)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "for method in method_list:\n",
    "\n",
    "    methods = []\n",
    "    splits = []\n",
    "    model_explanations = []\n",
    "    model_accuracies = []\n",
    "    explanation_accuracies = []\n",
    "    elapsed_times = []\n",
    "    explanation_fidelities = []\n",
    "    explanation_complexities = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=len(seeds), shuffle=True, random_state=0)\n",
    "\n",
    "    for seed, (trainval_index, test_index) in enumerate(skf.split(x.numpy(), y.numpy())):\n",
    "        set_seed(seed)\n",
    "        x_trainval, c_trainval, y_trainval = x[trainval_index], c[trainval_index], y[trainval_index]\n",
    "        x_test, c_test, y_test = x[test_index], c[test_index], y[test_index]\n",
    "        x_train, x_val, c_train, c_val, y_train, y_val = train_test_split(x_trainval, c_trainval, y_trainval,\n",
    "                                                                          test_size=0.3, random_state=0)\n",
    "        train_data_low = StructuredDataset(x_train, c_train, dataset_name, feature_names, concept_names)\n",
    "        val_data_low = StructuredDataset(x_val, c_val, dataset_name, feature_names, concept_names)\n",
    "        test_data_low = StructuredDataset(x_test, c_test, dataset_name, feature_names, concept_names)\n",
    "        data_low = StructuredDataset(x, c, dataset_name, feature_names, concept_names)\n",
    "\n",
    "        name_low = os.path.join(results_dir, f\"{method}_{seed}_low\")\n",
    "        name_high = os.path.join(results_dir, f\"{method}_{seed}_high\")\n",
    "\n",
    "        # Setting device\n",
    "        print(f\"Training {name_low} classifier...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method == 'DTree':\n",
    "            model_low = XDecisionTreeClassifier(name=name_low, n_classes=n_concepts,\n",
    "                                                n_features=n_features, max_depth=5)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(train_data_low, val_data_low, metric=metric, save=True)\n",
    "            c_predicted_train, _ = model_low.predict(train_data_low, device=device)\n",
    "            c_predicted_val, _ = model_low.predict(val_data_low, device=device)\n",
    "            c_predicted_test, _ = model_low.predict(test_data_low, device=device)\n",
    "            accuracy_low = model_low.evaluate(test_data_low, metric=metric)\n",
    "            train_data_high = StructuredDataset(c_predicted_train, y_train, dataset_name, feature_names, concept_names)\n",
    "            val_data_high = StructuredDataset(c_predicted_val, y_val, dataset_name, feature_names, concept_names)\n",
    "            test_data_high = StructuredDataset(c_predicted_test, y_test, dataset_name, feature_names, concept_names)\n",
    "            model_high = XDecisionTreeClassifier(name=name_high, n_classes=n_classes, n_features=n_concepts, max_depth=5)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_high.fit(train_data_high, val_data_high, metric=metric, save=True)\n",
    "            outputs, labels = model_high.predict(test_data_high, device=device)\n",
    "            accuracy = model_high.evaluate(test_data_high, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(i, concept_names)\n",
    "                class_output = torch.as_tensor((outputs > 0.5) == i)\n",
    "                class_label = torch.as_tensor(labels == i)\n",
    "                exp_fidelity = 100\n",
    "                exp_accuracy = expl_metric(class_output, class_label)\n",
    "                explanation_complexity = complexity(explanation)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "\n",
    "        elif method == 'BRL':\n",
    "            train_sample_rate = 1.0\n",
    "            model_low = XBRLClassifier(name=name_low, n_classes=n_concepts, n_features=n_features,\n",
    "                                       n_processes=n_processes, feature_names=feature_names, class_names=concept_names)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(train_data_low, train_sample_rate=train_sample_rate,\n",
    "                              verbose=True, eval=False)\n",
    "            c_predicted, _ = model_low.predict(data_low, device=device)\n",
    "            c_predicted_train, c_predicted_test = c_predicted[trainval_index], c_predicted[test_index]\n",
    "            accuracy_low = model_low.evaluate(test_data_low, metric=metric, outputs=c_predicted_test, labels=c_test)\n",
    "            train_data_high = StructuredDataset(c_predicted_train, y_trainval, dataset_name, feature_names, concept_names)\n",
    "            test_data_high = StructuredDataset(c_predicted_test, y_test, dataset_name, feature_names, concept_names)\n",
    "            model_high = XBRLClassifier(name=name_high, n_classes=n_classes, n_features=n_concepts,\n",
    "                                        n_processes=n_processes, feature_names=concept_names, class_names=class_names)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_high.fit(train_data_high, train_sample_rate=train_sample_rate, verbose=True,\n",
    "                               eval=False)\n",
    "            outputs, labels = model_high.predict(test_data_high, device=device)\n",
    "            accuracy = model_high.evaluate(test_data_high, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(i, concept_names)\n",
    "                exp_accuracy, exp_predictions = test_explanation(explanation, i, c_predicted_test, y_test, metric=expl_metric,\n",
    "                                                                 concept_names=concept_names)\n",
    "                exp_fidelity = 100\n",
    "                explanation_complexity = complexity(explanation, to_dnf=True)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "\n",
    "        elif method == 'DeepRed':\n",
    "            train_sample_rate = 0.1\n",
    "            model_low = XDeepRedClassifier(name=name_low, n_classes=n_concepts, n_features=n_features)\n",
    "            model_low.prepare_data(dataset_low, dataset_name + \"low\", seed, trainval_index, test_index, train_sample_rate)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(epochs,  train_sample_rate=train_sample_rate, verbose=True, eval=False)\n",
    "            c_predicted_train, _ = model_low.predict(train=True, device=device)\n",
    "            c_predicted_test, _ = model_low.predict(train=False, device=device)\n",
    "            accuracy_low = model_low.evaluate(train=False, outputs=c_predicted_test, labels=c_test, metric=metric)\n",
    "            model_low.finish()\n",
    "            c_predicted = torch.vstack((c_predicted_train, c_predicted_test))\n",
    "            y = torch.vstack((y_train, y_test))\n",
    "            dataset_high = StructuredDataset(c_predicted, y, dataset_name, feature_names, concept_names)\n",
    "            model_high = XDeepRedClassifier(n_classes, n_features, name=name_high)\n",
    "            model_high.prepare_data(dataset_high, dataset_name + \"high\", seed, trainval_index, test_index, train_sample_rate)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(epochs,  train_sample_rate=train_sample_rate, verbose=True, eval=False)\n",
    "            outputs, labels = model_high.predict(train=False, device=device)\n",
    "            accuracy = model_high.evaluate(train=False, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            print(\"Extracting rules...\")\n",
    "            t = time.time()\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(i, concept_names, simplify=simplify)\n",
    "                exp_accuracy, exp_predictions = test_explanation(explanation, i, c_predicted_test, y_test,\n",
    "                                                                 metric=expl_metric,\n",
    "                                                                 concept_names=concept_names, inequalities=True)\n",
    "                exp_predictions = torch.as_tensor(exp_predictions)\n",
    "                class_output = torch.as_tensor(outputs.argmax(dim=1) == i)\n",
    "                exp_fidelity = fidelity(exp_predictions, class_output, expl_metric)\n",
    "                explanation_complexity = complexity(explanation)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "                print(f\"{i + 1}/{n_classes} Rules extracted. Time {time.time() - t}\")\n",
    "\n",
    "        elif method == 'Psi':\n",
    "            # Network structures\n",
    "            l1_weight = 1e-4\n",
    "            hidden_neurons = [10, 5]\n",
    "            fan_in = 3\n",
    "            lr_psi = 1e-2\n",
    "            print(\"L1 weight\", l1_weight)\n",
    "            print(\"Hidden neurons\", hidden_neurons)\n",
    "            print(\"Fan in\", fan_in)\n",
    "            print(\"Learning rate\", lr_psi)\n",
    "            name_low = os.path.join(results_dir, f\"{method}_{seed}_{l1_weight}_{hidden_neurons}_{fan_in}_{lr_psi}_low\")\n",
    "            name_high = os.path.join(results_dir, f\"{method}_{seed}_{l1_weight}_{hidden_neurons}_{fan_in}_{lr_psi}_high\")\n",
    "            model_low = PsiNetwork(n_concepts, n_features, hidden_neurons, loss_low, l1_weight, name=name_low,\n",
    "                                   fan_in=fan_in)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(train_data_low, val_data_low, epochs=epochs, l_r=lr_psi,\n",
    "                              metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            c_predicted_train = model_low.predict(train_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_val = model_low.predict(val_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_test = model_low.predict(test_data_low, device=device)[0].detach().cpu()\n",
    "            accuracy_low = model_low.evaluate(test_data_low, outputs=c_predicted_test, labels=c_test, metric=metric)\n",
    "            train_data_high = StructuredDataset(c_predicted_train, y_train, dataset_name, feature_names, concept_names)\n",
    "            val_data_high = StructuredDataset(c_predicted_val, y_val, dataset_name, feature_names, concept_names)\n",
    "            test_data_high = StructuredDataset(c_predicted_test, y_test, dataset_name, feature_names, concept_names)\n",
    "            model_high = PsiNetwork(n_classes, n_concepts, hidden_neurons, loss_high, l1_weight,\n",
    "                                    name=name_high, fan_in=fan_in)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_high.fit(train_data_high, val_data_high, epochs=epochs, l_r=lr_psi,\n",
    "                               metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            outputs, labels = model_high.predict(test_data_high, device=device)\n",
    "            accuracy = model_high.evaluate(test_data_high, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(i, concept_names, simplify=simplify, x_train=c_predicted_train)\n",
    "                exp_accuracy, exp_predictions = test_explanation(explanation, i, c_predicted_test, y_test,\n",
    "                                                                 metric=expl_metric, concept_names=concept_names)\n",
    "                exp_predictions = torch.as_tensor(exp_predictions)\n",
    "                class_output = torch.as_tensor(outputs.argmax(dim=1) == i)\n",
    "                exp_fidelity = fidelity(exp_predictions, class_output, expl_metric)\n",
    "                explanation_complexity = complexity(explanation, to_dnf=True)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "\n",
    "        elif method == 'General':\n",
    "            # Network structures\n",
    "            l1_weight = 1e-3\n",
    "            hidden_neurons = [100, 30, 10]\n",
    "            fan_in = 5\n",
    "            top_k_explanations = None\n",
    "            name_low = os.path.join(results_dir, f\"{method}_{seed}_{l1_weight}_{hidden_neurons}_{fan_in}_low\")\n",
    "            name_high = os.path.join(results_dir, f\"{method}_{seed}_{l1_weight}_{hidden_neurons}_{fan_in}_high\")\n",
    "\n",
    "            model_low = XGeneralNN(n_concepts, n_features, hidden_neurons, fan_in=n_features,\n",
    "                                   loss=loss_low, name=name_low, l1_weight=l1_weight)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(train_data_low, val_data_low, epochs=epochs, l_r=l_r,\n",
    "                              metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            c_predicted_train = model_low.predict(train_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_val = model_low.predict(val_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_test = model_low.predict(test_data_low, device=device)[0].detach().cpu()\n",
    "            accuracy_low = model_low.evaluate(test_data_low, outputs=c_predicted_test, labels=c_test, metric=metric)\n",
    "            train_data_high = StructuredDataset(c_predicted_train, y_train, dataset_name, feature_names, concept_names)\n",
    "            val_data_high = StructuredDataset(c_predicted_val, y_val, dataset_name, feature_names, concept_names)\n",
    "            test_data_high = StructuredDataset(c_predicted_test, y_test, dataset_name, feature_names, concept_names)\n",
    "\n",
    "            model_high = XGeneralNN(n_classes, n_concepts, hidden_neurons, fan_in=fan_in,\n",
    "                                    loss=loss_high, name=name_high, l1_weight=l1_weight)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_high.fit(train_data_high, val_data_high, epochs=epochs, l_r=l_r*1e-1,\n",
    "                               metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            outputs, labels = model_high.predict(test_data_high, device=device)\n",
    "            accuracy = model_high.evaluate(test_data_high, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(c_predicted_train, y_train, i,\n",
    "                                                                top_k_explanations=top_k_explanations,\n",
    "                                                                concept_names=concept_names, simplify=simplify,\n",
    "                                                                metric=expl_metric, x_val=c_predicted_val,\n",
    "                                                                y_val=y_val)\n",
    "                exp_accuracy, exp_predictions = test_explanation(explanation, i, c_predicted_test, y_test,\n",
    "                                                                 metric=expl_metric, concept_names=concept_names)\n",
    "                exp_predictions = torch.as_tensor(exp_predictions)\n",
    "                class_output = torch.as_tensor(outputs.argmax(dim=1) == i)\n",
    "                exp_fidelity = fidelity(exp_predictions, class_output, expl_metric)\n",
    "                explanation_complexity = complexity(explanation, to_dnf=True)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "\n",
    "        elif method == 'Relu':\n",
    "            # Network structures\n",
    "            l1_weight = 1e-4\n",
    "            hidden_neurons = [100, 50, 30, 10]\n",
    "            dropout_rate = 0.01\n",
    "            print(\"l1 weight\", l1_weight)\n",
    "            print(\"hidden neurons\", hidden_neurons)\n",
    "            model_low = XReluNN(n_classes=n_concepts, n_features=n_features, name=name_low, dropout_rate=dropout_rate,\n",
    "                                hidden_neurons=hidden_neurons, loss=loss_low, l1_weight=l1_weight*1e-2)\n",
    "            try:\n",
    "                model_low.load(device)\n",
    "                print(f\"Model {name_low} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_low.fit(train_data_low, val_data_low, epochs=epochs, l_r=l_r,\n",
    "                              metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            c_predicted_train = model_low.predict(train_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_val = model_low.predict(val_data_low, device=device)[0].detach().cpu()\n",
    "            c_predicted_test = model_low.predict(test_data_low, device=device)[0].detach().cpu()\n",
    "            accuracy_low = model_low.evaluate(test_data_low, outputs=c_predicted_test, labels=c_test, metric=metric)\n",
    "            train_data_high = StructuredDataset(c_predicted_train, y_train, dataset_name, feature_names, concept_names)\n",
    "            val_data_high = StructuredDataset(c_predicted_val, y_val, dataset_name, feature_names, concept_names)\n",
    "            test_data_high = StructuredDataset(c_predicted_test, y_test, dataset_name, feature_names, concept_names)\n",
    "            model_high = XReluNN(n_classes=n_classes, n_features=n_concepts, name=name_high, dropout_rate=dropout_rate,\n",
    "                                hidden_neurons=hidden_neurons, loss=loss_high, l1_weight=l1_weight)\n",
    "            try:\n",
    "                model_high.load(device)\n",
    "                print(f\"Model {name_high} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                model_high.fit(train_data_high, val_data_high, epochs=epochs, l_r=l_r * 1e-1,\n",
    "                               metric=metric, lr_scheduler=lr_scheduler, device=device, verbose=True)\n",
    "            outputs, labels = model_high.predict(test_data_high, device=device)\n",
    "            accuracy = model_high.evaluate(test_data_high, metric=metric, outputs=outputs, labels=labels)\n",
    "            explanations, exp_accuracies, exp_fidelities, exp_complexities = [], [], [], []\n",
    "            for i in trange(n_classes):\n",
    "                explanation = model_high.get_global_explanation(c_predicted_train, y_train, i,\n",
    "                                                                top_k_explanations=top_k_explanations,\n",
    "                                                                concept_names=concept_names, simplify=simplify,\n",
    "                                                                metric=expl_metric, x_val=c_predicted_val, y_val=y_val)\n",
    "                exp_accuracy, exp_predictions = test_explanation(explanation, i, c_predicted_test, y_test,\n",
    "                                                                 metric=expl_metric, concept_names=concept_names)\n",
    "                exp_predictions = torch.as_tensor(exp_predictions)\n",
    "                class_output = torch.as_tensor(outputs.argmax(dim=1) == i)\n",
    "                exp_fidelity = fidelity(exp_predictions, class_output, expl_metric)\n",
    "                explanation_complexity = complexity(explanation, to_dnf=True)\n",
    "                explanations.append(explanation), exp_accuracies.append(exp_accuracy)\n",
    "                exp_fidelities.append(exp_fidelity), exp_complexities.append(explanation_complexity)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{method} not implemented\")\n",
    "\n",
    "        if model_high.time is None:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # In DeepRed and BRL the training is parallelized to speed up operation\n",
    "            if method == \"DeepRed\" or method == \"BRL\":\n",
    "                elapsed_time = elapsed_time * n_processes\n",
    "            model_high.time = elapsed_time\n",
    "            # To save the elapsed time and the explanations\n",
    "            model_high.save(device)\n",
    "        else:\n",
    "            elapsed_time = model_high.time\n",
    "\n",
    "        # To restore the original folder\n",
    "        if method == \"DeepRed\":\n",
    "            model_high.finish()\n",
    "\n",
    "        methods.append(method)\n",
    "        splits.append(seed)\n",
    "        model_explanations.append(explanations[0])\n",
    "        model_accuracies.append(accuracy)\n",
    "        elapsed_times.append(elapsed_time)\n",
    "        explanation_accuracies.append(np.mean(exp_accuracies))\n",
    "        explanation_fidelities.append(np.mean(exp_fidelities))\n",
    "        explanation_complexities.append(np.mean(exp_complexities))\n",
    "        print(\"Test model low accuracy\", accuracy_low)\n",
    "        print(\"Test model high accuracy\", accuracy)\n",
    "        print(\"Explanation time\", elapsed_time)\n",
    "        print(\"Explanation accuracy mean\", np.mean(exp_accuracies))\n",
    "        print(\"Explanation fidelity mean\", np.mean(exp_fidelities))\n",
    "        print(\"Explanation complexity mean\", np.mean(exp_complexities))\n",
    "\n",
    "    explanation_consistency = formula_consistency(model_explanations)\n",
    "    print(f'Consistency of explanations: {explanation_consistency:.4f}')\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'method': methods,\n",
    "        'split': splits,\n",
    "        'explanation': model_explanations,\n",
    "        'model_accuracy': model_accuracies,\n",
    "        'explanation_accuracy': explanation_accuracies,\n",
    "        'explanation_fidelity': explanation_fidelities,\n",
    "        'explanation_complexity': explanation_complexities,\n",
    "        'explanation_consistency': [explanation_consistency] * len(seeds),\n",
    "        'elapsed_time': elapsed_times,\n",
    "    })\n",
    "    results.to_csv(os.path.join(results_dir, f'results_{method}.csv'))\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = ['model_accuracy', 'explanation_accuracy', 'explanation_fidelity', 'explanation_complexity', 'elapsed_time',\n",
    "        'explanation_consistency']\n",
    "mean_cols = [f'{c}_mean' for c in cols]\n",
    "sem_cols = [f'{c}_sem' for c in cols]\n",
    "\n",
    "results_df = {}\n",
    "summaries = {}\n",
    "for m in method_list:\n",
    "    results_df[m] = pd.read_csv(os.path.join(results_dir, f\"results_{m}.csv\"))\n",
    "    df_mean = results_df[m][cols].mean()\n",
    "    df_sem = results_df[m][cols].sem()\n",
    "    df_mean.columns = mean_cols\n",
    "    df_sem.columns = sem_cols\n",
    "    summaries[m] = pd.concat([df_mean, df_sem])\n",
    "    summaries[m].name = m\n",
    "\n",
    "results_df = pd.concat([results_df[method] for method in method_list])\n",
    "results_df.to_csv(os.path.join(results_dir, f'results.csv'))\n",
    "\n",
    "summary = pd.concat([summaries[method] for method in method_list], axis=1).T\n",
    "summary.columns = mean_cols + sem_cols\n",
    "summary.to_csv(os.path.join(results_dir, 'summary_2.csv'))\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}