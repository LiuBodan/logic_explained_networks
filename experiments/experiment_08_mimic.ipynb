{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compound-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1776, 90])\n",
      "torch.Size([1776])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sympy import simplify_logic\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deep_logic.utils.base import validate_network, set_seed, tree_to_formula\n",
    "from deep_logic.utils.relu_nn import get_reduced_model, prune_features\n",
    "from deep_logic.utils.psi_nn import prune_equal_fanin\n",
    "from deep_logic import logic\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "#%%\n",
    "\n",
    "data = pd.read_csv('data/mimic-ii/full_cohort_data.csv')\n",
    "# data.drop('hgb_first')\n",
    "fs = [\n",
    "    'aline_flg',\n",
    "    'gender_num',\n",
    "    # 'hosp_exp_flg',\n",
    "    # 'icu_exp_flg',\n",
    "    # 'day_28_flg',\n",
    "    # 'censor_flg',\n",
    "    'sepsis_flg', 'chf_flg', 'afib_flg',\n",
    "    'renal_flg', 'liver_flg', 'copd_flg', 'cad_flg', 'stroke_flg',\n",
    "    'mal_flg', 'resp_flg',\n",
    "]\n",
    "features = fs\n",
    "data1 = data[fs].values\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data1 = imp_mean.fit_transform(data1)\n",
    "\n",
    "f2 = fs.copy()\n",
    "f2.append('day_icu_intime')\n",
    "f2.append('service_unit')\n",
    "f2.append('day_28_flg')\n",
    "f2.append('hospital_los_day')\n",
    "f2.append('icu_exp_flg')\n",
    "f2.append('hosp_exp_flg')\n",
    "f2.append('censor_flg')\n",
    "f2.append('mort_day_censored')\n",
    "f2 = data.columns.difference(f2)\n",
    "data2 = data[f2].values\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data2 = imp_mean.fit_transform(data2)\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "data2 = scaler.fit_transform(data2)\n",
    "features = features + list(f2)\n",
    "est = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='uniform')\n",
    "data2d = est.fit_transform(data2)\n",
    "f2d = []\n",
    "for feature in f2:\n",
    "    #f2d.append(feature + '_VLOW')\n",
    "    f2d.append(feature + '_LOW')\n",
    "    f2d.append(feature + '_NORMAL')\n",
    "    f2d.append(feature + '_HIGH')\n",
    "    #f2d.append(feature + '_VHIGH')\n",
    "features = fs + f2d\n",
    "\n",
    "datax = np.hstack((data1, data2d))\n",
    "datay = data['day_28_flg'].values\n",
    "\n",
    "x = torch.FloatTensor(datax)\n",
    "y = torch.LongTensor(datay)\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "front-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/mimic'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# concepts = [f'c{i:03}' for i in range(x.shape[1])]\n",
    "concepts =  features\n",
    "n_rep = 10\n",
    "tot_epochs = 5001\n",
    "prune_epochs = 2001\n",
    "seed = 42\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chinese-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(x_train, y_train, need_pruning, seed, device, l1=0.001, lr=0.001, relu=False, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 100),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(100, 10),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(10, 2),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_form = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += l1 * torch.norm(module.weight, 1)\n",
    "                loss += l1 * torch.norm(module.bias, 1)\n",
    "                break\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning and epoch % 1000 == 1:\n",
    "            prune_features(model, n_classes=1, device=device)\n",
    "            need_pruning = True\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worldwide-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_psi_nn(x_train, y_train, need_pruning, seed, device, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device).to(torch.float)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 10),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(10, 4),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(4, 1),\n",
    "        torch.nn.Sigmoid(),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.BCELoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train).squeeze()\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.00001 * torch.norm(module.weight, 1)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            model = prune_equal_fanin(model, 2, validate=True, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            y_pred_d = y_pred > 0.5\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlike-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_to_y(method, need_pruning, l1=0.001, lr=0.001, relu=False, verbose=False):\n",
    "    methods = []\n",
    "    splits = []\n",
    "    explanations = []\n",
    "    explanations_inv = []\n",
    "    model_accuracies = []\n",
    "    explanation_accuracies = []\n",
    "    explanation_accuracies_inv = []\n",
    "    elapsed_times = []\n",
    "    elapsed_times_inv = []\n",
    "    for split, (trainval_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(), y.cpu().detach().numpy())):\n",
    "        print(f'Split [{split+1}/{n_splits}]')\n",
    "        x_trainval, x_test = torch.FloatTensor(x[trainval_index]), torch.FloatTensor(x[test_index])\n",
    "        y_trainval, y_test = torch.LongTensor(y[trainval_index]), torch.LongTensor(y[test_index])\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.3, random_state=42)\n",
    "    \n",
    "        explanation, explanation_inv = '', ''\n",
    "        explanation_accuracy, explanation_accuracy_inv = 0, 0\n",
    "        \n",
    "        if method == 'tree':\n",
    "            classifier = DecisionTreeClassifier(random_state=seed)\n",
    "            classifier.fit(x_trainval.cpu().detach().numpy(), y_trainval.cpu().detach().numpy())\n",
    "            y_preds = classifier.predict(x_test.cpu().detach().numpy())\n",
    "            model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds)\n",
    "\n",
    "            target_class = 1\n",
    "            start = time.time()\n",
    "            explanation = tree_to_formula(classifier, concepts, target_class)\n",
    "            elapsed_time = time.time() - start\n",
    "            explanation_accuracy = model_accuracy\n",
    "\n",
    "            target_class_inv = 0\n",
    "            start = time.time()\n",
    "            explanation_inv = tree_to_formula(classifier, concepts, target_class_inv)\n",
    "            elapsed_time_inv = time.time() - start\n",
    "            explanation_accuracy_inv = model_accuracy\n",
    "        \n",
    "        else:\n",
    "            if method == 'psi':\n",
    "                # positive class\n",
    "                target_class = 1\n",
    "                model = train_psi_nn(x_trainval, y_trainval.eq(target_class), need_pruning, seed, device, verbose)\n",
    "                y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds>0.5)\n",
    "                \n",
    "            else:\n",
    "                model = train_nn(x_trainval, y_trainval, need_pruning, seed, device, l1, lr, relu, verbose)\n",
    "                y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds.argmax(axis=1))\n",
    "\n",
    "            # positive class\n",
    "            target_class = 1\n",
    "            start = time.time()\n",
    "            if method == 'psi':\n",
    "                global_explanation = logic.generate_fol_explanations(model, device)[0]\n",
    "            else:\n",
    "                global_explanation, _, _ = logic.relu_nn.combine_local_explanations(model, \n",
    "                                                                                   x_val.to(device), \n",
    "                                                                                   y_val.to(device), \n",
    "                                                                                   topk_explanations=3,\n",
    "                                                                                   target_class=target_class,\n",
    "                                                                                   method=method, device=device)\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            if global_explanation:\n",
    "                explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "                explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "\n",
    "#             # negative class\n",
    "#             target_class_inv = 0\n",
    "#             if method == 'psi':\n",
    "#                 model = train_psi_nn(x_trainval, y_trainval.eq(target_class_inv), need_pruning, seed, device, verbose)\n",
    "            \n",
    "#             start = time.time()\n",
    "#             if method == 'psi':\n",
    "#                 global_explanation_inv = logic.generate_fol_explanations(model, device)[0]\n",
    "#             else:\n",
    "#                 global_explanation_inv, _, _ = logic.relu_nn.combine_local_explanations(model, \n",
    "#                                                                                        x_val.to(device), \n",
    "#                                                                                        y_val.to(device), \n",
    "#                                                                                        topk_explanations=2,\n",
    "#                                                                                        target_class=target_class_inv,\n",
    "#                                                                                        method=method, device=device)\n",
    "#             elapsed_time_inv = time.time() - start\n",
    "#             if global_explanation_inv:\n",
    "#                 explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, \n",
    "#                                                                           target_class_inv, x_test, y_test)\n",
    "#                 explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "            print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "            print(f'\\t Elapsed time {elapsed_time}')\n",
    "#             print(f'\\t Class {target_class_inv} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "#             print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "\n",
    "        methods.append(method)\n",
    "        splits.append(split)\n",
    "        explanations.append(explanation)\n",
    "#         explanations_inv.append(explanation_inv)\n",
    "        model_accuracies.append(model_accuracy)\n",
    "        explanation_accuracies.append(explanation_accuracy)\n",
    "#         explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "        elapsed_times.append(elapsed_time)\n",
    "#         elapsed_times_inv.append(elapsed_time_inv)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'method': methods,\n",
    "        'split': splits,\n",
    "        'explanation': explanations,\n",
    "#         'explanation_inv': explanations_inv,\n",
    "        'model_accuracy': model_accuracies,\n",
    "        'explanation_accuracy': explanation_accuracies,\n",
    "#         'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "        'elapsed_time': elapsed_times,\n",
    "#         'elapsed_time_inv': elapsed_times_inv,\n",
    "    })\n",
    "    results.to_csv(os.path.join(results_dir, f'results_{method}.csv'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-glossary",
   "metadata": {},
   "source": [
    "# General pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intended-rolling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "Split [2/10]\n",
      "Split [3/10]\n",
      "Split [4/10]\n",
      "Split [5/10]\n",
      "Split [6/10]\n",
      "Split [7/10]\n",
      "Split [8/10]\n",
      "Split [9/10]\n",
      "Split [10/10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruning</td>\n",
       "      <td>0</td>\n",
       "      <td>stroke_flg</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>84.269663</td>\n",
       "      <td>0.074779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pruning</td>\n",
       "      <td>1</td>\n",
       "      <td>(stroke_flg &amp; bun_first_LOW &amp; ~sapsi_first_LOW...</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>83.707865</td>\n",
       "      <td>0.102725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pruning</td>\n",
       "      <td>2</td>\n",
       "      <td>stroke_flg</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>85.393258</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pruning</td>\n",
       "      <td>3</td>\n",
       "      <td>stroke_flg &amp; age_HIGH</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>86.516854</td>\n",
       "      <td>0.064825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pruning</td>\n",
       "      <td>4</td>\n",
       "      <td>(liver_flg &amp; stroke_flg &amp; bun_first_LOW &amp; ~age...</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>85.955056</td>\n",
       "      <td>0.180517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pruning</td>\n",
       "      <td>5</td>\n",
       "      <td>(stroke_flg &amp; age_HIGH &amp; bun_first_LOW) | (age...</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>86.516854</td>\n",
       "      <td>0.100696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pruning</td>\n",
       "      <td>6</td>\n",
       "      <td>(stroke_flg &amp; age_HIGH) | (age_HIGH &amp; ~bun_fir...</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>84.745763</td>\n",
       "      <td>0.110677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pruning</td>\n",
       "      <td>7</td>\n",
       "      <td>(stroke_flg &amp; age_HIGH) | (age_HIGH &amp; ~bun_fir...</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>86.440678</td>\n",
       "      <td>0.070780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pruning</td>\n",
       "      <td>8</td>\n",
       "      <td>(stroke_flg &amp; abg_count_LOW &amp; bun_first_LOW &amp; ...</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>82.485876</td>\n",
       "      <td>0.178492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pruning</td>\n",
       "      <td>9</td>\n",
       "      <td>stroke_flg &amp; ~sapsi_first_LOW</td>\n",
       "      <td>0.875706</td>\n",
       "      <td>87.570621</td>\n",
       "      <td>0.068815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    method  split                                        explanation  \\\n",
       "0  pruning      0                                         stroke_flg   \n",
       "1  pruning      1  (stroke_flg & bun_first_LOW & ~sapsi_first_LOW...   \n",
       "2  pruning      2                                         stroke_flg   \n",
       "3  pruning      3                              stroke_flg & age_HIGH   \n",
       "4  pruning      4  (liver_flg & stroke_flg & bun_first_LOW & ~age...   \n",
       "5  pruning      5  (stroke_flg & age_HIGH & bun_first_LOW) | (age...   \n",
       "6  pruning      6  (stroke_flg & age_HIGH) | (age_HIGH & ~bun_fir...   \n",
       "7  pruning      7  (stroke_flg & age_HIGH) | (age_HIGH & ~bun_fir...   \n",
       "8  pruning      8  (stroke_flg & abg_count_LOW & bun_first_LOW & ...   \n",
       "9  pruning      9                      stroke_flg & ~sapsi_first_LOW   \n",
       "\n",
       "   model_accuracy  explanation_accuracy  elapsed_time  \n",
       "0        0.842697             84.269663      0.074779  \n",
       "1        0.837079             83.707865      0.102725  \n",
       "2        0.853933             85.393258      0.074800  \n",
       "3        0.865169             86.516854      0.064825  \n",
       "4        0.865169             85.955056      0.180517  \n",
       "5        0.859551             86.516854      0.100696  \n",
       "6        0.847458             84.745763      0.110677  \n",
       "7        0.864407             86.440678      0.070780  \n",
       "8        0.830508             82.485876      0.178492  \n",
       "9        0.875706             87.570621      0.068815  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'pruning'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_pruning = c_to_y(method, need_pruning, relu=relu)\n",
    "results_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-flooring",
   "metadata": {},
   "source": [
    "# ReLUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "verified-sport",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "\t Epoch 0: train accuracy: 0.1589\n",
      "\t Epoch 500: train accuracy: 0.8411\n",
      "\t Epoch 1000: train accuracy: 0.8411\n",
      "\t Epoch 1500: train accuracy: 0.8411\n",
      "\t Epoch 2000: train accuracy: 0.8411\n",
      "\t Epoch 2500: train accuracy: 0.8411\n",
      "\t Epoch 3000: train accuracy: 0.8411\n",
      "\t Epoch 3500: train accuracy: 0.8411\n",
      "\t Epoch 4000: train accuracy: 0.8780\n",
      "\t Epoch 4500: train accuracy: 0.8842\n",
      "\t Epoch 5000: train accuracy: 0.8936\n",
      "\t Model's accuracy: 0.8596\n",
      "\t Class 1 - Global explanation: \"(tco2_first_NORMAL & ~renal_flg) | (age_HIGH & ~stroke_flg & ~weight_first_NORMAL)\" - Accuracy: 34.8315\n",
      "\t Elapsed time 0.41588807106018066\n",
      "Split [2/10]\n",
      "\t Epoch 0: train accuracy: 0.1589\n",
      "\t Epoch 500: train accuracy: 0.8411\n",
      "\t Epoch 1000: train accuracy: 0.8411\n",
      "\t Epoch 1500: train accuracy: 0.8411\n",
      "\t Epoch 2000: train accuracy: 0.8411\n",
      "\t Epoch 2500: train accuracy: 0.8411\n",
      "\t Epoch 3000: train accuracy: 0.8411\n",
      "\t Epoch 3500: train accuracy: 0.8411\n",
      "\t Epoch 4000: train accuracy: 0.8411\n",
      "\t Epoch 4500: train accuracy: 0.8411\n",
      "\t Epoch 5000: train accuracy: 0.8411\n",
      "\t Model's accuracy: 0.8371\n",
      "\t Class 1 - Global explanation: \"\" - Accuracy: 0.0000\n",
      "\t Elapsed time 0.012000560760498047\n",
      "Split [3/10]\n",
      "\t Epoch 0: train accuracy: 0.1589\n",
      "\t Epoch 500: train accuracy: 0.8411\n",
      "\t Epoch 1000: train accuracy: 0.8411\n",
      "\t Epoch 1500: train accuracy: 0.8411\n",
      "\t Epoch 2000: train accuracy: 0.8411\n",
      "\t Epoch 2500: train accuracy: 0.8411\n",
      "\t Epoch 3000: train accuracy: 0.8411\n",
      "\t Epoch 3500: train accuracy: 0.8411\n",
      "\t Epoch 4000: train accuracy: 0.8411\n",
      "\t Epoch 4500: train accuracy: 0.8855\n",
      "\t Epoch 5000: train accuracy: 0.9011\n",
      "\t Model's accuracy: 0.8596\n",
      "\t Class 1 - Global explanation: \"age_HIGH | (bun_first_LOW & ~mal_flg & ~age_LOW & ~sapsi_first_LOW & ~weight_first_NORMAL)\" - Accuracy: 65.7303\n",
      "\t Elapsed time 0.3979341983795166\n",
      "Split [4/10]\n",
      "\t Epoch 0: train accuracy: 0.1596\n",
      "\t Epoch 500: train accuracy: 0.8404\n",
      "\t Epoch 1000: train accuracy: 0.8404\n",
      "\t Epoch 1500: train accuracy: 0.8404\n",
      "\t Epoch 2000: train accuracy: 0.8404\n",
      "\t Epoch 2500: train accuracy: 0.8404\n",
      "\t Epoch 3000: train accuracy: 0.8404\n",
      "\t Epoch 3500: train accuracy: 0.8404\n",
      "\t Epoch 4000: train accuracy: 0.8404\n",
      "\t Epoch 4500: train accuracy: 0.8404\n",
      "\t Epoch 5000: train accuracy: 0.8404\n",
      "\t Model's accuracy: 0.8427\n",
      "\t Class 1 - Global explanation: \"\" - Accuracy: 0.0000\n",
      "\t Elapsed time 0.010942459106445312\n",
      "Split [5/10]\n",
      "\t Epoch 0: train accuracy: 0.1596\n",
      "\t Epoch 500: train accuracy: 0.8404\n",
      "\t Epoch 1000: train accuracy: 0.8404\n",
      "\t Epoch 1500: train accuracy: 0.8404\n",
      "\t Epoch 2000: train accuracy: 0.8404\n",
      "\t Epoch 2500: train accuracy: 0.8404\n",
      "\t Epoch 3000: train accuracy: 0.8404\n",
      "\t Epoch 3500: train accuracy: 0.8404\n",
      "\t Epoch 4000: train accuracy: 0.8404\n",
      "\t Epoch 4500: train accuracy: 0.8404\n",
      "\t Epoch 5000: train accuracy: 0.8404\n",
      "\t Model's accuracy: 0.8427\n",
      "\t Class 1 - Global explanation: \"\" - Accuracy: 0.0000\n",
      "\t Elapsed time 0.008973836898803711\n",
      "Split [6/10]\n",
      "\t Epoch 0: train accuracy: 0.1596\n",
      "\t Epoch 500: train accuracy: 0.8404\n",
      "\t Epoch 1000: train accuracy: 0.8404\n",
      "\t Epoch 1500: train accuracy: 0.8404\n",
      "\t Epoch 2000: train accuracy: 0.8404\n",
      "\t Epoch 2500: train accuracy: 0.8404\n",
      "\t Epoch 3000: train accuracy: 0.8404\n",
      "\t Epoch 3500: train accuracy: 0.8404\n",
      "\t Epoch 4000: train accuracy: 0.8867\n",
      "\t Epoch 4500: train accuracy: 0.8949\n",
      "\t Epoch 5000: train accuracy: 0.8999\n",
      "\t Model's accuracy: 0.8596\n",
      "\t Class 1 - Global explanation: \"~stroke_flg | ~sofa_first_LOW | (bun_first_LOW & ~sapsi_first_LOW & ~weight_first_NORMAL)\" - Accuracy: 16.8539\n",
      "\t Elapsed time 0.24037861824035645\n",
      "Split [7/10]\n",
      "\t Epoch 0: train accuracy: 0.1595\n",
      "\t Epoch 500: train accuracy: 0.8405\n",
      "\t Epoch 1000: train accuracy: 0.8405\n",
      "\t Epoch 1500: train accuracy: 0.8405\n",
      "\t Epoch 2000: train accuracy: 0.8405\n",
      "\t Epoch 2500: train accuracy: 0.8405\n",
      "\t Epoch 3000: train accuracy: 0.8405\n",
      "\t Epoch 3500: train accuracy: 0.8405\n",
      "\t Epoch 4000: train accuracy: 0.8405\n",
      "\t Epoch 4500: train accuracy: 0.8405\n",
      "\t Epoch 5000: train accuracy: 0.8405\n",
      "\t Model's accuracy: 0.8418\n",
      "\t Class 1 - Global explanation: \"\" - Accuracy: 0.0000\n",
      "\t Elapsed time 0.0169527530670166\n",
      "Split [8/10]\n",
      "\t Epoch 0: train accuracy: 0.1595\n",
      "\t Epoch 500: train accuracy: 0.8405\n",
      "\t Epoch 1000: train accuracy: 0.8405\n",
      "\t Epoch 1500: train accuracy: 0.8405\n",
      "\t Epoch 2000: train accuracy: 0.8405\n",
      "\t Epoch 2500: train accuracy: 0.8405\n",
      "\t Epoch 3000: train accuracy: 0.8405\n",
      "\t Epoch 3500: train accuracy: 0.8405\n",
      "\t Epoch 4000: train accuracy: 0.8405\n",
      "\t Epoch 4500: train accuracy: 0.8487\n",
      "\t Epoch 5000: train accuracy: 0.8799\n",
      "\t Model's accuracy: 0.8814\n",
      "\t Class 1 - Global explanation: \"age_HIGH | (stroke_flg & hgb_first_HIGH & sofa_first_NORMAL & ~age_NORMAL & ~hgb_first_NORMAL & ~hr_1st_LOW & ~icu_los_day_NORMAL)\" - Accuracy: 77.4011\n",
      "\t Elapsed time 1.4690852165222168\n",
      "Split [9/10]\n",
      "\t Epoch 0: train accuracy: 0.1595\n",
      "\t Epoch 500: train accuracy: 0.8405\n",
      "\t Epoch 1000: train accuracy: 0.8405\n",
      "\t Epoch 1500: train accuracy: 0.8405\n",
      "\t Epoch 2000: train accuracy: 0.8405\n",
      "\t Epoch 2500: train accuracy: 0.8405\n",
      "\t Epoch 3000: train accuracy: 0.8405\n",
      "\t Epoch 3500: train accuracy: 0.8405\n",
      "\t Epoch 4000: train accuracy: 0.8893\n",
      "\t Epoch 4500: train accuracy: 0.8949\n",
      "\t Epoch 5000: train accuracy: 0.8974\n",
      "\t Model's accuracy: 0.8418\n",
      "\t Class 1 - Global explanation: \"(bun_first_LOW & sofa_first_NORMAL) | (hour_icu_intime_LOW & sofa_first_NORMAL) | (age_HIGH & ~stroke_flg)\" - Accuracy: 46.3277\n",
      "\t Elapsed time 0.33211612701416016\n",
      "Split [10/10]\n",
      "\t Epoch 0: train accuracy: 0.1595\n",
      "\t Epoch 500: train accuracy: 0.8405\n",
      "\t Epoch 1000: train accuracy: 0.8405\n",
      "\t Epoch 1500: train accuracy: 0.8405\n",
      "\t Epoch 2000: train accuracy: 0.8405\n",
      "\t Epoch 2500: train accuracy: 0.8405\n",
      "\t Epoch 3000: train accuracy: 0.8405\n",
      "\t Epoch 3500: train accuracy: 0.8737\n",
      "\t Epoch 4000: train accuracy: 0.8843\n",
      "\t Epoch 4500: train accuracy: 0.8887\n",
      "\t Epoch 5000: train accuracy: 0.8918\n",
      "\t Model's accuracy: 0.8757\n",
      "\t Class 1 - Global explanation: \"bun_first_LOW | ~stroke_flg\" - Accuracy: 15.8192\n",
      "\t Elapsed time 0.6392934322357178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weights</td>\n",
       "      <td>0</td>\n",
       "      <td>(tco2_first_NORMAL &amp; ~renal_flg) | (age_HIGH &amp;...</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>34.831461</td>\n",
       "      <td>0.415888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weights</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weights</td>\n",
       "      <td>2</td>\n",
       "      <td>age_HIGH | (bun_first_LOW &amp; ~mal_flg &amp; ~age_LO...</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>65.730337</td>\n",
       "      <td>0.397934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weights</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weights</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weights</td>\n",
       "      <td>5</td>\n",
       "      <td>~stroke_flg | ~sofa_first_LOW | (bun_first_LOW...</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>16.853933</td>\n",
       "      <td>0.240379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weights</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weights</td>\n",
       "      <td>7</td>\n",
       "      <td>age_HIGH | (stroke_flg &amp; hgb_first_HIGH &amp; sofa...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>77.401130</td>\n",
       "      <td>1.469085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weights</td>\n",
       "      <td>8</td>\n",
       "      <td>(bun_first_LOW &amp; sofa_first_NORMAL) | (hour_ic...</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>46.327684</td>\n",
       "      <td>0.332116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weights</td>\n",
       "      <td>9</td>\n",
       "      <td>bun_first_LOW | ~stroke_flg</td>\n",
       "      <td>0.875706</td>\n",
       "      <td>15.819209</td>\n",
       "      <td>0.639293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    method  split                                        explanation  \\\n",
       "0  weights      0  (tco2_first_NORMAL & ~renal_flg) | (age_HIGH &...   \n",
       "1  weights      1                                                      \n",
       "2  weights      2  age_HIGH | (bun_first_LOW & ~mal_flg & ~age_LO...   \n",
       "3  weights      3                                                      \n",
       "4  weights      4                                                      \n",
       "5  weights      5  ~stroke_flg | ~sofa_first_LOW | (bun_first_LOW...   \n",
       "6  weights      6                                                      \n",
       "7  weights      7  age_HIGH | (stroke_flg & hgb_first_HIGH & sofa...   \n",
       "8  weights      8  (bun_first_LOW & sofa_first_NORMAL) | (hour_ic...   \n",
       "9  weights      9                        bun_first_LOW | ~stroke_flg   \n",
       "\n",
       "   model_accuracy  explanation_accuracy  elapsed_time  \n",
       "0        0.859551             34.831461      0.415888  \n",
       "1        0.837079              0.000000      0.012001  \n",
       "2        0.859551             65.730337      0.397934  \n",
       "3        0.842697              0.000000      0.010942  \n",
       "4        0.842697              0.000000      0.008974  \n",
       "5        0.859551             16.853933      0.240379  \n",
       "6        0.841808              0.000000      0.016953  \n",
       "7        0.881356             77.401130      1.469085  \n",
       "8        0.841808             46.327684      0.332116  \n",
       "9        0.875706             15.819209      0.639293  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'weights'\n",
    "need_pruning = False\n",
    "relu = True\n",
    "results_weights = c_to_y(method, need_pruning, 0.008, lr=0.0005, relu=relu, verbose=True)\n",
    "results_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-beijing",
   "metadata": {},
   "source": [
    "# Psi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'psi'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_psi = c_to_y(method, need_pruning, relu, verbose=False)\n",
    "results_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-powell",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'tree'\n",
    "need_pruning = False\n",
    "relu = False\n",
    "results_tree = c_to_y(method, need_pruning, relu)\n",
    "results_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-spray",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model_accuracy', 'explanation_accuracy', 'explanation_accuracy_inv', 'elapsed_time', 'elapsed_time_inv']\n",
    "mean_cols = [f'{c}_mean' for c in cols]\n",
    "sem_cols = [f'{c}_sem' for c in cols]\n",
    "\n",
    "# pruning\n",
    "df_mean = results_pruning[cols].mean()\n",
    "df_sem = results_pruning[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_pruning = pd.concat([df_mean, df_sem])\n",
    "summary_pruning.name = 'pruning'\n",
    "\n",
    "# # lime\n",
    "# df_mean = results_lime[cols].mean()\n",
    "# df_sem = results_lime[cols].sem()\n",
    "# df_mean.columns = mean_cols\n",
    "# df_sem.columns = sem_cols\n",
    "# summary_lime = pd.concat([df_mean, df_sem])\n",
    "# summary_lime.name = 'lime'\n",
    "\n",
    "# weights\n",
    "df_mean = results_weights[cols].mean()\n",
    "df_sem = results_weights[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_weights = pd.concat([df_mean, df_sem])\n",
    "summary_weights.name = 'weights'\n",
    "\n",
    "# psi\n",
    "df_mean = results_psi[cols].mean()\n",
    "df_sem = results_psi[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_psi = pd.concat([df_mean, df_sem])\n",
    "summary_psi.name = 'psi'\n",
    "\n",
    "# tree\n",
    "df_mean = results_tree[cols].mean()\n",
    "df_sem = results_tree[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_tree = pd.concat([df_mean, df_sem])\n",
    "summary_tree.name = 'tree'\n",
    "\n",
    "summary = pd.concat([summary_pruning, \n",
    "#                      summary_lime, \n",
    "                     summary_weights, \n",
    "                     summary_psi, \n",
    "                     summary_tree], axis=1).T\n",
    "summary.columns = mean_cols + sem_cols\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(os.path.join(results_dir, 'summary.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
