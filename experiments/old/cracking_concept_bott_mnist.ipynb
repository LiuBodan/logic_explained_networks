{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomRotateMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root: str):\n",
    "        super().__init__(root, download=True)\n",
    "        self.transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = super().__getitem__(item)\n",
    "        rotate = torch.rand(1) > 0.5\n",
    "        if rotate:\n",
    "            image = image.rotate(90)\n",
    "        # rotate = torch.randint(0, 3, [1]).item()\n",
    "        # rotate = 350\n",
    "        # rotate = 0\n",
    "        # image = image.rotate(rotate * 90)\n",
    "\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        return image, label, rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": "([3, 4, 3, 1, 3, 4, 1, 9],\n [tensor([True]),\n  tensor([False]),\n  tensor([True]),\n  tensor([False]),\n  tensor([False]),\n  tensor([False]),\n  tensor([True]),\n  tensor([True])])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = torchvision.transforms.ToPILImage()(img)\n",
    "        axs[0, i].imshow(np.asarray (img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "data_root = os.path.join(\"..\", \"data\")\n",
    "mnist_dataset = RandomRotateMNIST(data_root)\n",
    "data = [mnist_dataset[i] for i in [10, 20, 30, 40, 50, 60, 70, 80]]\n",
    "images = [d[0] for d in data]\n",
    "labels = [d[1] for d in data]\n",
    "rotate = [d[2] for d in data]\n",
    "input_size = images[0].shape\n",
    "print(\"Input size\", input_size)\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "show(grid)\n",
    "labels, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([8, 10, 100])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_width = 100\n",
    "digit_classes = 10\n",
    "\n",
    "class ConceptEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim=input_size,\n",
    "                 n_classes=digit_classes, output_dim=output_width):\n",
    "        super(ConceptEmbedder, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        input_dim = np.prod(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, n_classes * output_dim)\n",
    "        self.unflatten = nn.Unflatten(1, (n_classes, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = ConceptEmbedder()\n",
    "out = net(torch.stack(images))\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3517],\n        [0.3610],\n        [0.3583],\n        [0.3575],\n        [0.3552],\n        [0.3590],\n        [0.3594],\n        [0.3540]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_classes=1, input_width=(digit_classes,output_width)):\n",
    "        super(MLP, self).__init__()\n",
    "        input_dim = np.prod(input_width)\n",
    "        self.linear1 = nn.Linear(input_dim, output_width)\n",
    "        self.linear2 = nn.Linear(output_width, 10)\n",
    "        self.linear3 = nn.Linear(10, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        # return torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net2 = MLP()\n",
    "out2 = net2(out)\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorial_loss(f:torch.Tensor, y:np.ndarray):\n",
    "    norm_f = torch.norm(f, dim=2)\n",
    "    l = torch.nn.CrossEntropyLoss()(norm_f, y)\n",
    "    return l\n",
    "\n",
    "mlp_loss = nn.MSELoss()\n",
    "def train(model, model2, device, train_loader, optimizer, epoch,\n",
    "          model_loss=vectorial_loss):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, rotate_target) in enumerate(train_loader):\n",
    "        data = data.to(device) \n",
    "        target = target.to(device)\n",
    "        rotate_target = rotate_target.to(device).to(torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output2 = model2(output)\n",
    "\n",
    "        loss1 = model_loss(output, target)\n",
    "        loss2 = mlp_loss(output2, rotate_target)\n",
    "        loss2 = torch.sqrt(loss2)\n",
    "\n",
    "        loss = loss1 + 0.5 * loss2 if epoch > epochs//5 else loss1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx) % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\"\n",
    "                  f\"({100. * batch_idx / (len(train_loader)):.0f}%)]\"\n",
    "                  f\"\\tLoss: {loss.item():.6f} {loss1.item():.6f} {loss2.item():.6f} \")\n",
    "\n",
    "\n",
    "def test(model, model2, device, test_loader, model_loss=vectorial_loss):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    test_loss2, correct2 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, rotate_target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            rotate_target = rotate_target.to(device).to(torch.float)\n",
    "\n",
    "            output = model(data)\n",
    "            output2 = model2(output)\n",
    "\n",
    "            test_loss += model_loss(output, target)  # sum up batch loss\n",
    "            output = output.norm(dim=2)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss2 += mlp_loss(output2, rotate_target) # sum up batch loss\n",
    "            # test_loss2 = mlp_loss(output2, rotate_target)\n",
    "            # test_loss2 = torch.sqrt(test_loss2)\n",
    "            pred2 = output2 > 0.5  # get the index of the max log-probability\n",
    "            correct2 += pred2.eq(rotate_target.view_as(pred2)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_loss2 /= len(test_loader)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f} {:.4f}, Accuracy: ({:.0f}%) ({:.0f}%) \\n'.format(\n",
    "        test_loss, test_loss2,\n",
    "        100. * correct / len(test_loader.dataset),\n",
    "        100. * correct2 / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.5 * len(mnist_dataset))\n",
    "test_size = len(mnist_dataset) - train_size\n",
    "train_set, test_set = random_split(mnist_dataset,\n",
    "                                   [train_size, test_size])\n",
    "\n",
    "epochs = 100\n",
    "batch_size= 1024\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() \\\n",
    "    else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# net = ConceptEmbedder().to(device)\n",
    "# net2 = MLP().to(device)\n",
    "#\n",
    "# optimizer = torch.optim.AdamW(itertools.chain(net.parameters(), net2.parameters()))\n",
    "#\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(net, net2, device, train_loader, optimizer, epoch)\n",
    "#     test(net, net2, device, test_loader)\n",
    "#     torch.save(net.state_dict(), \"mnist_cnn.pt\")\n",
    "#     torch.save(net2.state_dict(), \"mlp.pt\")\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data = [mnist_dataset[i] for i in torch.randint(0, len(mnist_dataset), (8,))]\n",
    "# images = [d[0] for d in data]\n",
    "# labels = [d[1] for d in data]\n",
    "# rot_lab = [d[2] for d in data]\n",
    "# grid = torchvision.utils.make_grid(images)\n",
    "# show(grid)\n",
    "# out = net(torch.stack(images).to(device))\n",
    "# print(\"Output\", out.squeeze())\n",
    "# print(\"Predicted numbers\", out.norm(dim=2).argmax(dim=1))\n",
    "# print(\"Labels\", labels)\n",
    "# out2 = net2(out)\n",
    "# # print(\"Predicted rotations\", [\"Rotated\" if o > 0.5 else \"Not Rotated\" for o in out2])\n",
    "# print(\"Predicted rotations\", out2.squeeze())\n",
    "# print(\"Rotation labels\", rot_lab)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class FuzzyConcepts(nn.Module):\n",
    "    def __init__(self, input_dim=input_size,\n",
    "                 n_classes=digit_classes):\n",
    "        super(FuzzyConcepts, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        input_dim = np.prod(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = FuzzyConcepts()\n",
    "out = net(torch.stack(images))\n",
    "out.size()\n",
    "\n",
    "\n",
    "net2 = MLP(input_width=(1,digit_classes))\n",
    "out2 = net2(out)\n",
    "out2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000](0%)]\tLoss: 2.301888 2.301888 1.098146 \n",
      "Train Epoch: 1 [10240/30000](33%)]\tLoss: 1.956718 1.956718 1.010136 \n",
      "Train Epoch: 1 [20480/30000](67%)]\tLoss: 1.753410 1.753410 0.981772 \n",
      "\n",
      "Test set: Average loss: 1.7110 1.0466, Accuracy: (76%) (50%) \n",
      "\n",
      "Train Epoch: 2 [0/30000](0%)]\tLoss: 1.719484 1.719484 1.011275 \n",
      "Train Epoch: 2 [10240/30000](33%)]\tLoss: 1.677587 1.677587 1.045286 \n",
      "Train Epoch: 2 [20480/30000](67%)]\tLoss: 1.659177 1.659177 1.017114 \n",
      "\n",
      "Test set: Average loss: 1.6481 1.0473, Accuracy: (81%) (50%) \n",
      "\n",
      "Train Epoch: 3 [0/30000](0%)]\tLoss: 1.631517 1.631517 1.021284 \n",
      "Train Epoch: 3 [10240/30000](33%)]\tLoss: 1.634380 1.634380 1.035916 \n",
      "Train Epoch: 3 [20480/30000](67%)]\tLoss: 1.611085 1.611085 0.995174 \n",
      "\n",
      "Test set: Average loss: 1.6086 1.0415, Accuracy: (85%) (50%) \n",
      "\n",
      "Train Epoch: 4 [0/30000](0%)]\tLoss: 1.599005 1.599005 1.044858 \n",
      "Train Epoch: 4 [10240/30000](33%)]\tLoss: 1.601208 1.601208 1.036110 \n",
      "Train Epoch: 4 [20480/30000](67%)]\tLoss: 1.586302 1.586302 1.026264 \n",
      "\n",
      "Test set: Average loss: 1.5845 1.0351, Accuracy: (87%) (50%) \n",
      "\n",
      "Train Epoch: 5 [0/30000](0%)]\tLoss: 1.583944 1.583944 0.995988 \n",
      "Train Epoch: 5 [10240/30000](33%)]\tLoss: 1.566902 1.566902 1.012819 \n",
      "Train Epoch: 5 [20480/30000](67%)]\tLoss: 1.567694 1.567694 0.984590 \n",
      "\n",
      "Test set: Average loss: 1.5695 1.0476, Accuracy: (89%) (50%) \n",
      "\n",
      "Train Epoch: 6 [0/30000](0%)]\tLoss: 1.560513 1.560513 1.031784 \n",
      "Train Epoch: 6 [10240/30000](33%)]\tLoss: 1.568331 1.568331 0.995766 \n",
      "Train Epoch: 6 [20480/30000](67%)]\tLoss: 1.566631 1.566631 1.017047 \n",
      "\n",
      "Test set: Average loss: 1.5583 1.0406, Accuracy: (90%) (50%) \n",
      "\n",
      "Train Epoch: 7 [0/30000](0%)]\tLoss: 1.550098 1.550098 1.022344 \n",
      "Train Epoch: 7 [10240/30000](33%)]\tLoss: 1.551954 1.551954 1.010828 \n",
      "Train Epoch: 7 [20480/30000](67%)]\tLoss: 1.552512 1.552512 1.043910 \n",
      "\n",
      "Test set: Average loss: 1.5489 1.0333, Accuracy: (91%) (50%) \n",
      "\n",
      "Train Epoch: 8 [0/30000](0%)]\tLoss: 1.541979 1.541979 1.033904 \n",
      "Train Epoch: 8 [10240/30000](33%)]\tLoss: 1.531279 1.531279 1.016508 \n",
      "Train Epoch: 8 [20480/30000](67%)]\tLoss: 1.538406 1.538406 1.022250 \n",
      "\n",
      "Test set: Average loss: 1.5432 1.0435, Accuracy: (91%) (50%) \n",
      "\n",
      "Train Epoch: 9 [0/30000](0%)]\tLoss: 1.537852 1.537852 1.027469 \n",
      "Train Epoch: 9 [10240/30000](33%)]\tLoss: 1.535946 1.535946 1.027303 \n",
      "Train Epoch: 9 [20480/30000](67%)]\tLoss: 1.536355 1.536355 1.006636 \n",
      "\n",
      "Test set: Average loss: 1.5369 1.0345, Accuracy: (92%) (50%) \n",
      "\n",
      "Train Epoch: 10 [0/30000](0%)]\tLoss: 1.531864 1.531864 1.020800 \n",
      "Train Epoch: 10 [10240/30000](33%)]\tLoss: 1.516514 1.516514 1.012944 \n",
      "Train Epoch: 10 [20480/30000](67%)]\tLoss: 1.517893 1.517893 0.994523 \n",
      "\n",
      "Test set: Average loss: 1.5319 1.0346, Accuracy: (93%) (50%) \n",
      "\n",
      "Train Epoch: 11 [0/30000](0%)]\tLoss: 1.521103 1.521103 1.018894 \n",
      "Train Epoch: 11 [10240/30000](33%)]\tLoss: 1.527752 1.527752 1.016607 \n",
      "Train Epoch: 11 [20480/30000](67%)]\tLoss: 1.527047 1.527047 1.026781 \n",
      "\n",
      "Test set: Average loss: 1.5294 1.0335, Accuracy: (93%) (50%) \n",
      "\n",
      "Train Epoch: 12 [0/30000](0%)]\tLoss: 1.522823 1.522823 1.006844 \n",
      "Train Epoch: 12 [10240/30000](33%)]\tLoss: 1.520765 1.520765 1.015764 \n",
      "Train Epoch: 12 [20480/30000](67%)]\tLoss: 1.512432 1.512432 1.002531 \n",
      "\n",
      "Test set: Average loss: 1.5270 1.0438, Accuracy: (93%) (50%) \n",
      "\n",
      "Train Epoch: 13 [0/30000](0%)]\tLoss: 1.515372 1.515372 1.002112 \n",
      "Train Epoch: 13 [10240/30000](33%)]\tLoss: 1.505097 1.505097 1.029225 \n",
      "Train Epoch: 13 [20480/30000](67%)]\tLoss: 1.504723 1.504723 1.014087 \n",
      "\n",
      "Test set: Average loss: 1.5226 1.0406, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 14 [0/30000](0%)]\tLoss: 1.507167 1.507167 1.007414 \n",
      "Train Epoch: 14 [10240/30000](33%)]\tLoss: 1.509489 1.509489 1.013461 \n",
      "Train Epoch: 14 [20480/30000](67%)]\tLoss: 1.505934 1.505934 1.009079 \n",
      "\n",
      "Test set: Average loss: 1.5206 1.0312, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 15 [0/30000](0%)]\tLoss: 1.520883 1.520883 1.008608 \n",
      "Train Epoch: 15 [10240/30000](33%)]\tLoss: 1.504544 1.504544 1.003082 \n",
      "Train Epoch: 15 [20480/30000](67%)]\tLoss: 1.494768 1.494768 0.998180 \n",
      "\n",
      "Test set: Average loss: 1.5180 1.0355, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 16 [0/30000](0%)]\tLoss: 1.500393 1.500393 1.021557 \n",
      "Train Epoch: 16 [10240/30000](33%)]\tLoss: 1.503948 1.503948 1.013955 \n",
      "Train Epoch: 16 [20480/30000](67%)]\tLoss: 1.508141 1.508141 1.004974 \n",
      "\n",
      "Test set: Average loss: 1.5164 1.0348, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 17 [0/30000](0%)]\tLoss: 1.500006 1.500006 1.021601 \n",
      "Train Epoch: 17 [10240/30000](33%)]\tLoss: 1.500155 1.500155 0.984895 \n",
      "Train Epoch: 17 [20480/30000](67%)]\tLoss: 1.501227 1.501227 1.003875 \n",
      "\n",
      "Test set: Average loss: 1.5159 1.0295, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 18 [0/30000](0%)]\tLoss: 1.496610 1.496610 1.047785 \n",
      "Train Epoch: 18 [10240/30000](33%)]\tLoss: 1.502406 1.502406 0.998575 \n",
      "Train Epoch: 18 [20480/30000](67%)]\tLoss: 1.503037 1.503037 1.023687 \n",
      "\n",
      "Test set: Average loss: 1.5130 1.0346, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 19 [0/30000](0%)]\tLoss: 1.496529 1.496529 1.026395 \n",
      "Train Epoch: 19 [10240/30000](33%)]\tLoss: 1.506352 1.506352 1.010908 \n",
      "Train Epoch: 19 [20480/30000](67%)]\tLoss: 1.494238 1.494238 1.018805 \n",
      "\n",
      "Test set: Average loss: 1.5125 1.0265, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 20 [0/30000](0%)]\tLoss: 1.494409 1.494409 1.040652 \n",
      "Train Epoch: 20 [10240/30000](33%)]\tLoss: 1.490163 1.490163 1.004794 \n",
      "Train Epoch: 20 [20480/30000](67%)]\tLoss: 1.498786 1.498786 1.023653 \n",
      "\n",
      "Test set: Average loss: 1.5117 1.0380, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 21 [0/30000](0%)]\tLoss: 2.002970 1.494910 1.016120 \n",
      "Train Epoch: 21 [10240/30000](33%)]\tLoss: 1.848067 1.493200 0.709732 \n",
      "Train Epoch: 21 [20480/30000](67%)]\tLoss: 1.751242 1.496166 0.510153 \n",
      "\n",
      "Test set: Average loss: 1.5121 0.2684, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 22 [0/30000](0%)]\tLoss: 1.754835 1.495618 0.518433 \n",
      "Train Epoch: 22 [10240/30000](33%)]\tLoss: 1.751997 1.501797 0.500400 \n",
      "Train Epoch: 22 [20480/30000](67%)]\tLoss: 1.741442 1.491340 0.500205 \n",
      "\n",
      "Test set: Average loss: 1.5089 0.2487, Accuracy: (95%) (52%) \n",
      "\n",
      "Train Epoch: 23 [0/30000](0%)]\tLoss: 1.745336 1.495157 0.500359 \n",
      "Train Epoch: 23 [10240/30000](33%)]\tLoss: 1.742655 1.492665 0.499980 \n",
      "Train Epoch: 23 [20480/30000](67%)]\tLoss: 1.740625 1.492021 0.497208 \n",
      "\n",
      "Test set: Average loss: 1.5091 0.2478, Accuracy: (95%) (53%) \n",
      "\n",
      "Train Epoch: 24 [0/30000](0%)]\tLoss: 1.741766 1.492997 0.497538 \n",
      "Train Epoch: 24 [10240/30000](33%)]\tLoss: 1.740887 1.492349 0.497078 \n",
      "Train Epoch: 24 [20480/30000](67%)]\tLoss: 1.747023 1.498732 0.496581 \n",
      "\n",
      "Test set: Average loss: 1.5095 0.2471, Accuracy: (95%) (55%) \n",
      "\n",
      "Train Epoch: 25 [0/30000](0%)]\tLoss: 1.735990 1.487982 0.496016 \n",
      "Train Epoch: 25 [10240/30000](33%)]\tLoss: 1.742508 1.494126 0.496764 \n",
      "Train Epoch: 25 [20480/30000](67%)]\tLoss: 1.735514 1.487012 0.497004 \n",
      "\n",
      "Test set: Average loss: 1.5074 0.2465, Accuracy: (95%) (54%) \n",
      "\n",
      "Train Epoch: 26 [0/30000](0%)]\tLoss: 1.745927 1.497812 0.496229 \n",
      "Train Epoch: 26 [10240/30000](33%)]\tLoss: 1.738173 1.491141 0.494063 \n",
      "Train Epoch: 26 [20480/30000](67%)]\tLoss: 1.733483 1.485474 0.496018 \n",
      "\n",
      "Test set: Average loss: 1.5073 0.2449, Accuracy: (95%) (56%) \n",
      "\n",
      "Train Epoch: 27 [0/30000](0%)]\tLoss: 1.738967 1.491434 0.495067 \n",
      "Train Epoch: 27 [10240/30000](33%)]\tLoss: 1.737111 1.489225 0.495771 \n",
      "Train Epoch: 27 [20480/30000](67%)]\tLoss: 1.735561 1.487751 0.495620 \n",
      "\n",
      "Test set: Average loss: 1.5075 0.2419, Accuracy: (95%) (59%) \n",
      "\n",
      "Train Epoch: 28 [0/30000](0%)]\tLoss: 1.734079 1.486780 0.494599 \n",
      "Train Epoch: 28 [10240/30000](33%)]\tLoss: 1.734970 1.488974 0.491993 \n",
      "Train Epoch: 28 [20480/30000](67%)]\tLoss: 1.730512 1.483049 0.494926 \n",
      "\n",
      "Test set: Average loss: 1.5080 0.2413, Accuracy: (95%) (58%) \n",
      "\n",
      "Train Epoch: 29 [0/30000](0%)]\tLoss: 1.730433 1.485061 0.490743 \n",
      "Train Epoch: 29 [10240/30000](33%)]\tLoss: 1.732589 1.487860 0.489457 \n",
      "Train Epoch: 29 [20480/30000](67%)]\tLoss: 1.738426 1.493188 0.490476 \n",
      "\n",
      "Test set: Average loss: 1.5089 0.2316, Accuracy: (95%) (64%) \n",
      "\n",
      "Train Epoch: 30 [0/30000](0%)]\tLoss: 1.730805 1.491279 0.479052 \n",
      "Train Epoch: 30 [10240/30000](33%)]\tLoss: 1.730438 1.497750 0.465375 \n",
      "Train Epoch: 30 [20480/30000](67%)]\tLoss: 1.726006 1.512567 0.426876 \n",
      "\n",
      "Test set: Average loss: 1.5445 0.1424, Accuracy: (91%) (81%) \n",
      "\n",
      "Train Epoch: 31 [0/30000](0%)]\tLoss: 1.711793 1.524444 0.374699 \n",
      "Train Epoch: 31 [10240/30000](33%)]\tLoss: 1.706471 1.549220 0.314503 \n",
      "Train Epoch: 31 [20480/30000](67%)]\tLoss: 1.685781 1.555428 0.260708 \n",
      "\n",
      "Test set: Average loss: 1.5695 0.0540, Accuracy: (86%) (93%) \n",
      "\n",
      "Train Epoch: 32 [0/30000](0%)]\tLoss: 1.661831 1.553607 0.216447 \n",
      "Train Epoch: 32 [10240/30000](33%)]\tLoss: 1.645763 1.551719 0.188088 \n",
      "Train Epoch: 32 [20480/30000](67%)]\tLoss: 1.642775 1.552197 0.181156 \n",
      "\n",
      "Test set: Average loss: 1.5742 0.0297, Accuracy: (86%) (98%) \n",
      "\n",
      "Train Epoch: 33 [0/30000](0%)]\tLoss: 1.621580 1.546591 0.149977 \n",
      "Train Epoch: 33 [10240/30000](33%)]\tLoss: 1.628191 1.554254 0.147875 \n",
      "Train Epoch: 33 [20480/30000](67%)]\tLoss: 1.621898 1.555335 0.133126 \n",
      "\n",
      "Test set: Average loss: 1.5751 0.0216, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 34 [0/30000](0%)]\tLoss: 1.622871 1.557522 0.130699 \n",
      "Train Epoch: 34 [10240/30000](33%)]\tLoss: 1.630904 1.563537 0.134734 \n",
      "Train Epoch: 34 [20480/30000](67%)]\tLoss: 1.623391 1.555339 0.136105 \n",
      "\n",
      "Test set: Average loss: 1.5695 0.0220, Accuracy: (90%) (98%) \n",
      "\n",
      "Train Epoch: 35 [0/30000](0%)]\tLoss: 1.614222 1.551532 0.125380 \n",
      "Train Epoch: 35 [10240/30000](33%)]\tLoss: 1.610476 1.550045 0.120862 \n",
      "Train Epoch: 35 [20480/30000](67%)]\tLoss: 1.618145 1.557345 0.121599 \n",
      "\n",
      "Test set: Average loss: 1.5692 0.0191, Accuracy: (90%) (99%) \n",
      "\n",
      "Train Epoch: 36 [0/30000](0%)]\tLoss: 1.614324 1.556378 0.115892 \n",
      "Train Epoch: 36 [10240/30000](33%)]\tLoss: 1.616287 1.558020 0.116534 \n",
      "Train Epoch: 36 [20480/30000](67%)]\tLoss: 1.607059 1.552316 0.109485 \n",
      "\n",
      "Test set: Average loss: 1.5698 0.0180, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 37 [0/30000](0%)]\tLoss: 1.603361 1.550989 0.104744 \n",
      "Train Epoch: 37 [10240/30000](33%)]\tLoss: 1.601300 1.552288 0.098024 \n",
      "Train Epoch: 37 [20480/30000](67%)]\tLoss: 1.607277 1.555108 0.104339 \n",
      "\n",
      "Test set: Average loss: 1.5687 0.0164, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 38 [0/30000](0%)]\tLoss: 1.594514 1.547067 0.094896 \n",
      "Train Epoch: 38 [10240/30000](33%)]\tLoss: 1.599025 1.549369 0.099312 \n",
      "Train Epoch: 38 [20480/30000](67%)]\tLoss: 1.600150 1.554092 0.092117 \n",
      "\n",
      "Test set: Average loss: 1.5721 0.0159, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 39 [0/30000](0%)]\tLoss: 1.602623 1.554194 0.096858 \n",
      "Train Epoch: 39 [10240/30000](33%)]\tLoss: 1.597755 1.551533 0.092444 \n",
      "Train Epoch: 39 [20480/30000](67%)]\tLoss: 1.601289 1.553644 0.095289 \n",
      "\n",
      "Test set: Average loss: 1.5693 0.0154, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 40 [0/30000](0%)]\tLoss: 1.584268 1.544044 0.080448 \n",
      "Train Epoch: 40 [10240/30000](33%)]\tLoss: 1.594260 1.548529 0.091461 \n",
      "Train Epoch: 40 [20480/30000](67%)]\tLoss: 1.594412 1.549430 0.089964 \n",
      "\n",
      "Test set: Average loss: 1.5685 0.0152, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 41 [0/30000](0%)]\tLoss: 1.597026 1.554684 0.084686 \n",
      "Train Epoch: 41 [10240/30000](33%)]\tLoss: 1.591268 1.548130 0.086277 \n",
      "Train Epoch: 41 [20480/30000](67%)]\tLoss: 1.599770 1.552378 0.094785 \n",
      "\n",
      "Test set: Average loss: 1.5703 0.0151, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 42 [0/30000](0%)]\tLoss: 1.586691 1.545205 0.082972 \n",
      "Train Epoch: 42 [10240/30000](33%)]\tLoss: 1.601961 1.554928 0.094066 \n",
      "Train Epoch: 42 [20480/30000](67%)]\tLoss: 1.603054 1.557376 0.091355 \n",
      "\n",
      "Test set: Average loss: 1.5680 0.0147, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 43 [0/30000](0%)]\tLoss: 1.584124 1.546535 0.075180 \n",
      "Train Epoch: 43 [10240/30000](33%)]\tLoss: 1.591465 1.555623 0.071684 \n",
      "Train Epoch: 43 [20480/30000](67%)]\tLoss: 1.591690 1.550207 0.082967 \n",
      "\n",
      "Test set: Average loss: 1.5686 0.0137, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 44 [0/30000](0%)]\tLoss: 1.594013 1.551250 0.085525 \n",
      "Train Epoch: 44 [10240/30000](33%)]\tLoss: 1.592566 1.555251 0.074629 \n",
      "Train Epoch: 44 [20480/30000](67%)]\tLoss: 1.585451 1.551059 0.068783 \n",
      "\n",
      "Test set: Average loss: 1.5673 0.0136, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 45 [0/30000](0%)]\tLoss: 1.584109 1.552957 0.062303 \n",
      "Train Epoch: 45 [10240/30000](33%)]\tLoss: 1.596378 1.552995 0.086766 \n",
      "Train Epoch: 45 [20480/30000](67%)]\tLoss: 1.580447 1.550059 0.060775 \n",
      "\n",
      "Test set: Average loss: 1.5652 0.0135, Accuracy: (90%) (99%) \n",
      "\n",
      "Train Epoch: 46 [0/30000](0%)]\tLoss: 1.583897 1.551317 0.065161 \n",
      "Train Epoch: 46 [10240/30000](33%)]\tLoss: 1.577522 1.547570 0.059905 \n",
      "Train Epoch: 46 [20480/30000](67%)]\tLoss: 1.575540 1.548129 0.054822 \n",
      "\n",
      "Test set: Average loss: 1.5670 0.0147, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 47 [0/30000](0%)]\tLoss: 1.588739 1.555488 0.066503 \n",
      "Train Epoch: 47 [10240/30000](33%)]\tLoss: 1.573473 1.540741 0.065465 \n",
      "Train Epoch: 47 [20480/30000](67%)]\tLoss: 1.589549 1.549328 0.080442 \n",
      "\n",
      "Test set: Average loss: 1.5658 0.0136, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 48 [0/30000](0%)]\tLoss: 1.574321 1.546613 0.055416 \n",
      "Train Epoch: 48 [10240/30000](33%)]\tLoss: 1.580400 1.549740 0.061321 \n",
      "Train Epoch: 48 [20480/30000](67%)]\tLoss: 1.587034 1.548128 0.077812 \n",
      "\n",
      "Test set: Average loss: 1.5639 0.0126, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 49 [0/30000](0%)]\tLoss: 1.584410 1.553041 0.062738 \n",
      "Train Epoch: 49 [10240/30000](33%)]\tLoss: 1.573868 1.546206 0.055324 \n",
      "Train Epoch: 49 [20480/30000](67%)]\tLoss: 1.572607 1.543545 0.058122 \n",
      "\n",
      "Test set: Average loss: 1.5653 0.0124, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 50 [0/30000](0%)]\tLoss: 1.570097 1.541935 0.056322 \n",
      "Train Epoch: 50 [10240/30000](33%)]\tLoss: 1.581166 1.552881 0.056570 \n",
      "Train Epoch: 50 [20480/30000](67%)]\tLoss: 1.574040 1.542679 0.062722 \n",
      "\n",
      "Test set: Average loss: 1.5661 0.0126, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 51 [0/30000](0%)]\tLoss: 1.575633 1.545705 0.059856 \n",
      "Train Epoch: 51 [10240/30000](33%)]\tLoss: 1.578851 1.547842 0.062018 \n",
      "Train Epoch: 51 [20480/30000](67%)]\tLoss: 1.563723 1.541880 0.043685 \n",
      "\n",
      "Test set: Average loss: 1.5656 0.0136, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 52 [0/30000](0%)]\tLoss: 1.571790 1.543502 0.056576 \n",
      "Train Epoch: 52 [10240/30000](33%)]\tLoss: 1.574955 1.548870 0.052170 \n",
      "Train Epoch: 52 [20480/30000](67%)]\tLoss: 1.576522 1.545725 0.061594 \n",
      "\n",
      "Test set: Average loss: 1.5632 0.0134, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 53 [0/30000](0%)]\tLoss: 1.562955 1.542567 0.040775 \n",
      "Train Epoch: 53 [10240/30000](33%)]\tLoss: 1.568731 1.547239 0.042983 \n",
      "Train Epoch: 53 [20480/30000](67%)]\tLoss: 1.573372 1.549270 0.048203 \n",
      "\n",
      "Test set: Average loss: 1.5656 0.0132, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 54 [0/30000](0%)]\tLoss: 1.569192 1.543775 0.050832 \n",
      "Train Epoch: 54 [10240/30000](33%)]\tLoss: 1.580682 1.549283 0.062798 \n",
      "Train Epoch: 54 [20480/30000](67%)]\tLoss: 1.572626 1.549160 0.046931 \n",
      "\n",
      "Test set: Average loss: 1.5658 0.0123, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 55 [0/30000](0%)]\tLoss: 1.572218 1.545576 0.053284 \n",
      "Train Epoch: 55 [10240/30000](33%)]\tLoss: 1.575130 1.547213 0.055835 \n",
      "Train Epoch: 55 [20480/30000](67%)]\tLoss: 1.568924 1.543934 0.049981 \n",
      "\n",
      "Test set: Average loss: 1.5660 0.0131, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 56 [0/30000](0%)]\tLoss: 1.559329 1.541489 0.035681 \n",
      "Train Epoch: 56 [10240/30000](33%)]\tLoss: 1.574530 1.552930 0.043201 \n",
      "Train Epoch: 56 [20480/30000](67%)]\tLoss: 1.579127 1.545846 0.066562 \n",
      "\n",
      "Test set: Average loss: 1.5664 0.0136, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 57 [0/30000](0%)]\tLoss: 1.566136 1.543977 0.044318 \n",
      "Train Epoch: 57 [10240/30000](33%)]\tLoss: 1.584966 1.555325 0.059283 \n",
      "Train Epoch: 57 [20480/30000](67%)]\tLoss: 1.567001 1.543190 0.047623 \n",
      "\n",
      "Test set: Average loss: 1.5632 0.0134, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 58 [0/30000](0%)]\tLoss: 1.573518 1.552391 0.042255 \n",
      "Train Epoch: 58 [10240/30000](33%)]\tLoss: 1.576725 1.552601 0.048249 \n",
      "Train Epoch: 58 [20480/30000](67%)]\tLoss: 1.573715 1.547856 0.051717 \n",
      "\n",
      "Test set: Average loss: 1.5644 0.0131, Accuracy: (89%) (99%) \n",
      "\n",
      "Train Epoch: 59 [0/30000](0%)]\tLoss: 1.570716 1.548412 0.044606 \n",
      "Train Epoch: 59 [10240/30000](33%)]\tLoss: 1.573207 1.552969 0.040477 \n",
      "Train Epoch: 59 [20480/30000](67%)]\tLoss: 1.566110 1.545787 0.040646 \n",
      "\n",
      "Test set: Average loss: 1.5633 0.0132, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 60 [0/30000](0%)]\tLoss: 1.562178 1.540133 0.044090 \n",
      "Train Epoch: 60 [10240/30000](33%)]\tLoss: 1.569660 1.542856 0.053608 \n",
      "Train Epoch: 60 [20480/30000](67%)]\tLoss: 1.567855 1.548441 0.038828 \n",
      "\n",
      "Test set: Average loss: 1.5643 0.0133, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 61 [0/30000](0%)]\tLoss: 1.576505 1.552315 0.048381 \n",
      "Train Epoch: 61 [10240/30000](33%)]\tLoss: 1.575888 1.548077 0.055620 \n",
      "Train Epoch: 61 [20480/30000](67%)]\tLoss: 1.562676 1.541247 0.042857 \n",
      "\n",
      "Test set: Average loss: 1.5640 0.0130, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 62 [0/30000](0%)]\tLoss: 1.564726 1.547460 0.034532 \n",
      "Train Epoch: 62 [10240/30000](33%)]\tLoss: 1.575251 1.550401 0.049700 \n",
      "Train Epoch: 62 [20480/30000](67%)]\tLoss: 1.574579 1.548474 0.052210 \n",
      "\n",
      "Test set: Average loss: 1.5639 0.0117, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 63 [0/30000](0%)]\tLoss: 1.571317 1.542491 0.057651 \n",
      "Train Epoch: 63 [10240/30000](33%)]\tLoss: 1.583937 1.555510 0.056853 \n",
      "Train Epoch: 63 [20480/30000](67%)]\tLoss: 1.570601 1.546136 0.048929 \n",
      "\n",
      "Test set: Average loss: 1.5661 0.0129, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 64 [0/30000](0%)]\tLoss: 1.564252 1.545962 0.036580 \n",
      "Train Epoch: 64 [10240/30000](33%)]\tLoss: 1.566289 1.544092 0.044393 \n",
      "Train Epoch: 64 [20480/30000](67%)]\tLoss: 1.563801 1.546087 0.035428 \n",
      "\n",
      "Test set: Average loss: 1.5651 0.0122, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 65 [0/30000](0%)]\tLoss: 1.568483 1.549108 0.038751 \n",
      "Train Epoch: 65 [10240/30000](33%)]\tLoss: 1.568330 1.544892 0.046877 \n",
      "Train Epoch: 65 [20480/30000](67%)]\tLoss: 1.567296 1.546986 0.040621 \n",
      "\n",
      "Test set: Average loss: 1.5630 0.0118, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 66 [0/30000](0%)]\tLoss: 1.560479 1.545584 0.029790 \n",
      "Train Epoch: 66 [10240/30000](33%)]\tLoss: 1.561395 1.544443 0.033904 \n",
      "Train Epoch: 66 [20480/30000](67%)]\tLoss: 1.563013 1.543062 0.039903 \n",
      "\n",
      "Test set: Average loss: 1.5642 0.0130, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 67 [0/30000](0%)]\tLoss: 1.570707 1.544533 0.052347 \n",
      "Train Epoch: 67 [10240/30000](33%)]\tLoss: 1.574131 1.548244 0.051774 \n",
      "Train Epoch: 67 [20480/30000](67%)]\tLoss: 1.565579 1.542333 0.046492 \n",
      "\n",
      "Test set: Average loss: 1.5656 0.0132, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 68 [0/30000](0%)]\tLoss: 1.569354 1.548913 0.040883 \n",
      "Train Epoch: 68 [10240/30000](33%)]\tLoss: 1.575073 1.553411 0.043323 \n",
      "Train Epoch: 68 [20480/30000](67%)]\tLoss: 1.585692 1.550768 0.069848 \n",
      "\n",
      "Test set: Average loss: 1.5662 0.0132, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 69 [0/30000](0%)]\tLoss: 1.557186 1.540913 0.032545 \n",
      "Train Epoch: 69 [10240/30000](33%)]\tLoss: 1.577619 1.552924 0.049389 \n",
      "Train Epoch: 69 [20480/30000](67%)]\tLoss: 1.561759 1.541113 0.041291 \n",
      "\n",
      "Test set: Average loss: 1.5626 0.0128, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 70 [0/30000](0%)]\tLoss: 1.570661 1.547188 0.046946 \n",
      "Train Epoch: 70 [10240/30000](33%)]\tLoss: 1.564650 1.543314 0.042673 \n",
      "Train Epoch: 70 [20480/30000](67%)]\tLoss: 1.568697 1.544250 0.048894 \n",
      "\n",
      "Test set: Average loss: 1.5638 0.0128, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 71 [0/30000](0%)]\tLoss: 1.561620 1.542624 0.037991 \n",
      "Train Epoch: 71 [10240/30000](33%)]\tLoss: 1.560563 1.541478 0.038170 \n",
      "Train Epoch: 71 [20480/30000](67%)]\tLoss: 1.566885 1.551755 0.030261 \n",
      "\n",
      "Test set: Average loss: 1.5631 0.0126, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 72 [0/30000](0%)]\tLoss: 1.557603 1.541247 0.032711 \n",
      "Train Epoch: 72 [10240/30000](33%)]\tLoss: 1.559318 1.541323 0.035991 \n",
      "Train Epoch: 72 [20480/30000](67%)]\tLoss: 1.560714 1.545139 0.031151 \n",
      "\n",
      "Test set: Average loss: 1.5635 0.0135, Accuracy: (88%) (99%) \n",
      "\n",
      "Train Epoch: 73 [0/30000](0%)]\tLoss: 1.567733 1.545808 0.043851 \n",
      "Train Epoch: 73 [10240/30000](33%)]\tLoss: 1.566177 1.544481 0.043392 \n",
      "Train Epoch: 73 [20480/30000](67%)]\tLoss: 1.563176 1.549869 0.026615 \n",
      "\n",
      "Test set: Average loss: 1.5640 0.0128, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 74 [0/30000](0%)]\tLoss: 1.573133 1.549099 0.048068 \n",
      "Train Epoch: 74 [10240/30000](33%)]\tLoss: 1.557092 1.542787 0.028610 \n",
      "Train Epoch: 74 [20480/30000](67%)]\tLoss: 1.570004 1.546340 0.047330 \n",
      "\n",
      "Test set: Average loss: 1.5637 0.0117, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 75 [0/30000](0%)]\tLoss: 1.551552 1.537996 0.027112 \n",
      "Train Epoch: 75 [10240/30000](33%)]\tLoss: 1.565319 1.551961 0.026715 \n",
      "Train Epoch: 75 [20480/30000](67%)]\tLoss: 1.567719 1.544821 0.045796 \n",
      "\n",
      "Test set: Average loss: 1.5631 0.0130, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 76 [0/30000](0%)]\tLoss: 1.563133 1.544125 0.038017 \n",
      "Train Epoch: 76 [10240/30000](33%)]\tLoss: 1.568782 1.546588 0.044388 \n",
      "Train Epoch: 76 [20480/30000](67%)]\tLoss: 1.564244 1.544083 0.040323 \n",
      "\n",
      "Test set: Average loss: 1.5634 0.0126, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 77 [0/30000](0%)]\tLoss: 1.560210 1.545003 0.030413 \n",
      "Train Epoch: 77 [10240/30000](33%)]\tLoss: 1.556893 1.543266 0.027254 \n",
      "Train Epoch: 77 [20480/30000](67%)]\tLoss: 1.548604 1.537272 0.022663 \n",
      "\n",
      "Test set: Average loss: 1.5636 0.0129, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 78 [0/30000](0%)]\tLoss: 1.578946 1.552616 0.052659 \n",
      "Train Epoch: 78 [10240/30000](33%)]\tLoss: 1.574938 1.550857 0.048161 \n",
      "Train Epoch: 78 [20480/30000](67%)]\tLoss: 1.555651 1.540702 0.029898 \n",
      "\n",
      "Test set: Average loss: 1.5636 0.0135, Accuracy: (86%) (99%) \n",
      "\n",
      "Train Epoch: 79 [0/30000](0%)]\tLoss: 1.563837 1.548262 0.031149 \n",
      "Train Epoch: 79 [10240/30000](33%)]\tLoss: 1.555118 1.540771 0.028695 \n",
      "Train Epoch: 79 [20480/30000](67%)]\tLoss: 1.548158 1.536712 0.022893 \n",
      "\n",
      "Test set: Average loss: 1.5636 0.0129, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 80 [0/30000](0%)]\tLoss: 1.556195 1.542756 0.026877 \n",
      "Train Epoch: 80 [10240/30000](33%)]\tLoss: 1.559484 1.538897 0.041174 \n",
      "Train Epoch: 80 [20480/30000](67%)]\tLoss: 1.565166 1.543947 0.042438 \n",
      "\n",
      "Test set: Average loss: 1.5634 0.0121, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 81 [0/30000](0%)]\tLoss: 1.560237 1.540744 0.038987 \n",
      "Train Epoch: 81 [10240/30000](33%)]\tLoss: 1.555733 1.541321 0.028824 \n",
      "Train Epoch: 81 [20480/30000](67%)]\tLoss: 1.561999 1.540254 0.043490 \n",
      "\n",
      "Test set: Average loss: 1.5634 0.0123, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 82 [0/30000](0%)]\tLoss: 1.560072 1.545128 0.029888 \n",
      "Train Epoch: 82 [10240/30000](33%)]\tLoss: 1.564760 1.547321 0.034879 \n",
      "Train Epoch: 82 [20480/30000](67%)]\tLoss: 1.549936 1.536776 0.026320 \n",
      "\n",
      "Test set: Average loss: 1.5631 0.0133, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 83 [0/30000](0%)]\tLoss: 1.566296 1.542987 0.046619 \n",
      "Train Epoch: 83 [10240/30000](33%)]\tLoss: 1.564361 1.543206 0.042309 \n",
      "Train Epoch: 83 [20480/30000](67%)]\tLoss: 1.560860 1.544062 0.033596 \n",
      "\n",
      "Test set: Average loss: 1.5637 0.0125, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 84 [0/30000](0%)]\tLoss: 1.558693 1.545113 0.027160 \n",
      "Train Epoch: 84 [10240/30000](33%)]\tLoss: 1.560348 1.543929 0.032838 \n",
      "Train Epoch: 84 [20480/30000](67%)]\tLoss: 1.557612 1.546891 0.021442 \n",
      "\n",
      "Test set: Average loss: 1.5643 0.0138, Accuracy: (82%) (99%) \n",
      "\n",
      "Train Epoch: 85 [0/30000](0%)]\tLoss: 1.548784 1.534480 0.028608 \n",
      "Train Epoch: 85 [10240/30000](33%)]\tLoss: 1.562021 1.543944 0.036155 \n",
      "Train Epoch: 85 [20480/30000](67%)]\tLoss: 1.560379 1.543131 0.034495 \n",
      "\n",
      "Test set: Average loss: 1.5640 0.0131, Accuracy: (80%) (99%) \n",
      "\n",
      "Train Epoch: 86 [0/30000](0%)]\tLoss: 1.576152 1.555985 0.040335 \n",
      "Train Epoch: 86 [10240/30000](33%)]\tLoss: 1.559992 1.542780 0.034424 \n",
      "Train Epoch: 86 [20480/30000](67%)]\tLoss: 1.561263 1.548684 0.025158 \n",
      "\n",
      "Test set: Average loss: 1.5637 0.0137, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 87 [0/30000](0%)]\tLoss: 1.568592 1.548007 0.041171 \n",
      "Train Epoch: 87 [10240/30000](33%)]\tLoss: 1.557791 1.539714 0.036154 \n",
      "Train Epoch: 87 [20480/30000](67%)]\tLoss: 1.564403 1.547347 0.034111 \n",
      "\n",
      "Test set: Average loss: 1.5626 0.0133, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 88 [0/30000](0%)]\tLoss: 1.560245 1.545653 0.029186 \n",
      "Train Epoch: 88 [10240/30000](33%)]\tLoss: 1.549230 1.540764 0.016931 \n",
      "Train Epoch: 88 [20480/30000](67%)]\tLoss: 1.554269 1.542793 0.022952 \n",
      "\n",
      "Test set: Average loss: 1.5642 0.0136, Accuracy: (83%) (99%) \n",
      "\n",
      "Train Epoch: 89 [0/30000](0%)]\tLoss: 1.554199 1.544715 0.018968 \n",
      "Train Epoch: 89 [10240/30000](33%)]\tLoss: 1.557216 1.544722 0.024988 \n",
      "Train Epoch: 89 [20480/30000](67%)]\tLoss: 1.554297 1.542574 0.023446 \n",
      "\n",
      "Test set: Average loss: 1.5637 0.0134, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 90 [0/30000](0%)]\tLoss: 1.557575 1.544585 0.025981 \n",
      "Train Epoch: 90 [10240/30000](33%)]\tLoss: 1.565066 1.548642 0.032848 \n",
      "Train Epoch: 90 [20480/30000](67%)]\tLoss: 1.553026 1.544010 0.018032 \n",
      "\n",
      "Test set: Average loss: 1.5634 0.0139, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 91 [0/30000](0%)]\tLoss: 1.560015 1.542962 0.034106 \n",
      "Train Epoch: 91 [10240/30000](33%)]\tLoss: 1.559897 1.543654 0.032486 \n",
      "Train Epoch: 91 [20480/30000](67%)]\tLoss: 1.550559 1.540250 0.020618 \n",
      "\n",
      "Test set: Average loss: 1.5648 0.0123, Accuracy: (83%) (99%) \n",
      "\n",
      "Train Epoch: 92 [0/30000](0%)]\tLoss: 1.554327 1.542387 0.023878 \n",
      "Train Epoch: 92 [10240/30000](33%)]\tLoss: 1.561197 1.539962 0.042471 \n",
      "Train Epoch: 92 [20480/30000](67%)]\tLoss: 1.562833 1.547959 0.029748 \n",
      "\n",
      "Test set: Average loss: 1.5620 0.0128, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 93 [0/30000](0%)]\tLoss: 1.563819 1.551748 0.024143 \n",
      "Train Epoch: 93 [10240/30000](33%)]\tLoss: 1.559458 1.546285 0.026347 \n",
      "Train Epoch: 93 [20480/30000](67%)]\tLoss: 1.565533 1.548053 0.034960 \n",
      "\n",
      "Test set: Average loss: 1.5636 0.0131, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 94 [0/30000](0%)]\tLoss: 1.567041 1.544093 0.045896 \n",
      "Train Epoch: 94 [10240/30000](33%)]\tLoss: 1.558695 1.538848 0.039694 \n",
      "Train Epoch: 94 [20480/30000](67%)]\tLoss: 1.560969 1.545954 0.030029 \n",
      "\n",
      "Test set: Average loss: 1.5649 0.0130, Accuracy: (82%) (99%) \n",
      "\n",
      "Train Epoch: 95 [0/30000](0%)]\tLoss: 1.556276 1.540766 0.031020 \n",
      "Train Epoch: 95 [10240/30000](33%)]\tLoss: 1.556508 1.542010 0.028996 \n",
      "Train Epoch: 95 [20480/30000](67%)]\tLoss: 1.556783 1.541921 0.029725 \n",
      "\n",
      "Test set: Average loss: 1.5623 0.0141, Accuracy: (87%) (99%) \n",
      "\n",
      "Train Epoch: 96 [0/30000](0%)]\tLoss: 1.566318 1.545924 0.040789 \n",
      "Train Epoch: 96 [10240/30000](33%)]\tLoss: 1.555893 1.541017 0.029752 \n",
      "Train Epoch: 96 [20480/30000](67%)]\tLoss: 1.558365 1.543939 0.028851 \n",
      "\n",
      "Test set: Average loss: 1.5633 0.0136, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 97 [0/30000](0%)]\tLoss: 1.548973 1.535450 0.027045 \n",
      "Train Epoch: 97 [10240/30000](33%)]\tLoss: 1.557418 1.543825 0.027186 \n",
      "Train Epoch: 97 [20480/30000](67%)]\tLoss: 1.556990 1.540974 0.032031 \n",
      "\n",
      "Test set: Average loss: 1.5640 0.0127, Accuracy: (85%) (99%) \n",
      "\n",
      "Train Epoch: 98 [0/30000](0%)]\tLoss: 1.560214 1.545736 0.028956 \n",
      "Train Epoch: 98 [10240/30000](33%)]\tLoss: 1.557902 1.541296 0.033212 \n",
      "Train Epoch: 98 [20480/30000](67%)]\tLoss: 1.574386 1.547127 0.054516 \n",
      "\n",
      "Test set: Average loss: 1.5640 0.0120, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 99 [0/30000](0%)]\tLoss: 1.550262 1.539630 0.021265 \n",
      "Train Epoch: 99 [10240/30000](33%)]\tLoss: 1.549510 1.538639 0.021741 \n",
      "Train Epoch: 99 [20480/30000](67%)]\tLoss: 1.556792 1.541563 0.030458 \n",
      "\n",
      "Test set: Average loss: 1.5635 0.0125, Accuracy: (84%) (99%) \n",
      "\n",
      "Train Epoch: 100 [0/30000](0%)]\tLoss: 1.559897 1.548903 0.021989 \n",
      "Train Epoch: 100 [10240/30000](33%)]\tLoss: 1.561470 1.542871 0.037198 \n",
      "Train Epoch: 100 [20480/30000](67%)]\tLoss: 1.554947 1.541602 0.026690 \n",
      "\n",
      "Test set: Average loss: 1.5635 0.0136, Accuracy: (85%) (99%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, model2, device, test_loader, model_loss=vectorial_loss):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    test_loss2, correct2 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, rotate_target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            rotate_target = rotate_target.to(device).to(torch.float)\n",
    "\n",
    "            output = model(data)\n",
    "            output2 = model2(output)\n",
    "\n",
    "            test_loss += model_loss(output, target)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss2 += mlp_loss(output2, rotate_target) # sum up batch loss\n",
    "            # test_loss2 = mlp_loss(output2, rotate_target)\n",
    "            # test_loss2 = torch.sqrt(test_loss2)\n",
    "            pred2 = output2 > 0.5  # get the index of the max log-probability\n",
    "            correct2 += pred2.eq(rotate_target.view_as(pred2)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_loss2 /= len(test_loader)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f} {:.4f}, Accuracy: ({:.0f}%) ({:.0f}%) \\n'.format(\n",
    "        test_loss, test_loss2,\n",
    "        100. * correct / len(test_loader.dataset),\n",
    "        100. * correct2 / len(test_loader.dataset)))\n",
    "\n",
    "net = FuzzyConcepts().to(device)\n",
    "net2 = MLP(input_width=(1,digit_classes))\n",
    "\n",
    "optimizer = torch.optim.AdamW(itertools.chain(net.parameters(), net2.parameters()))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(net, net2, device, train_loader, optimizer,\n",
    "          epoch, torch.nn.CrossEntropyLoss())\n",
    "    test(net, net2, device, test_loader,\n",
    "         torch.nn.CrossEntropyLoss())\n",
    "    torch.save(net.state_dict(), \"mnist_cnn.pt\")\n",
    "    torch.save(net2.state_dict(), \"mlp.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2523],\n        [-0.1694],\n        [-0.1779],\n        [-0.1585],\n        [-0.1482],\n        [-0.1254],\n        [-0.1511],\n        [-0.1771]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def steep_sigmoid(input, k=100, b=0.5):\n",
    "    output = 1 / (1 + torch.exp(-k*(input-b)))\n",
    "    return output\n",
    "\n",
    "class BooleanConcepts(FuzzyConcepts):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = steep_sigmoid(x)\n",
    "        return x.float()\n",
    "\n",
    "net = BooleanConcepts()\n",
    "out = net(torch.stack(images))\n",
    "out.size()\n",
    "\n",
    "\n",
    "net2 = MLP(input_width=(1,digit_classes))\n",
    "out2 = net2(out)\n",
    "out2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000](0%)]\tLoss: 2.397697 2.397697 0.876359 \n",
      "Train Epoch: 1 [10240/30000](33%)]\tLoss: 1.723414 1.723414 0.748506 \n",
      "Train Epoch: 1 [20480/30000](67%)]\tLoss: 1.637810 1.637810 0.737359 \n",
      "\n",
      "Test set: Average loss: 1.5999 0.5588, Accuracy: (85%) (50%) \n",
      "\n",
      "Train Epoch: 2 [0/30000](0%)]\tLoss: 1.584323 1.584323 0.747430 \n",
      "Train Epoch: 2 [10240/30000](33%)]\tLoss: 1.574102 1.574102 0.748998 \n",
      "Train Epoch: 2 [20480/30000](67%)]\tLoss: 1.558846 1.558846 0.743932 \n",
      "\n",
      "Test set: Average loss: 1.5542 0.5473, Accuracy: (90%) (50%) \n",
      "\n",
      "Train Epoch: 3 [0/30000](0%)]\tLoss: 1.545489 1.545489 0.742184 \n",
      "Train Epoch: 3 [10240/30000](33%)]\tLoss: 1.540559 1.540559 0.723988 \n",
      "Train Epoch: 3 [20480/30000](67%)]\tLoss: 1.537461 1.537461 0.747698 \n",
      "\n",
      "Test set: Average loss: 1.5413 0.5514, Accuracy: (92%) (50%) \n",
      "\n",
      "Train Epoch: 4 [0/30000](0%)]\tLoss: 1.532568 1.532568 0.742775 \n",
      "Train Epoch: 4 [10240/30000](33%)]\tLoss: 1.524132 1.524132 0.717395 \n",
      "Train Epoch: 4 [20480/30000](67%)]\tLoss: 1.521142 1.521142 0.745510 \n",
      "\n",
      "Test set: Average loss: 1.5296 0.5566, Accuracy: (93%) (50%) \n",
      "\n",
      "Train Epoch: 5 [0/30000](0%)]\tLoss: 1.522139 1.522139 0.747190 \n",
      "Train Epoch: 5 [10240/30000](33%)]\tLoss: 1.517119 1.517119 0.763411 \n",
      "Train Epoch: 5 [20480/30000](67%)]\tLoss: 1.509796 1.509796 0.745657 \n",
      "\n",
      "Test set: Average loss: 1.5236 0.5540, Accuracy: (93%) (50%) \n",
      "\n",
      "Train Epoch: 6 [0/30000](0%)]\tLoss: 1.516237 1.516237 0.757963 \n",
      "Train Epoch: 6 [10240/30000](33%)]\tLoss: 1.503232 1.503232 0.739249 \n",
      "Train Epoch: 6 [20480/30000](67%)]\tLoss: 1.513681 1.513681 0.745039 \n",
      "\n",
      "Test set: Average loss: 1.5200 0.5509, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 7 [0/30000](0%)]\tLoss: 1.507264 1.507264 0.726198 \n",
      "Train Epoch: 7 [10240/30000](33%)]\tLoss: 1.509864 1.509864 0.736232 \n",
      "Train Epoch: 7 [20480/30000](67%)]\tLoss: 1.500766 1.500766 0.750927 \n",
      "\n",
      "Test set: Average loss: 1.5139 0.5481, Accuracy: (94%) (50%) \n",
      "\n",
      "Train Epoch: 8 [0/30000](0%)]\tLoss: 1.501395 1.501395 0.731312 \n",
      "Train Epoch: 8 [10240/30000](33%)]\tLoss: 1.503824 1.503824 0.718901 \n",
      "Train Epoch: 8 [20480/30000](67%)]\tLoss: 1.507046 1.507046 0.736653 \n",
      "\n",
      "Test set: Average loss: 1.5092 0.5528, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 9 [0/30000](0%)]\tLoss: 1.496085 1.496085 0.746723 \n",
      "Train Epoch: 9 [10240/30000](33%)]\tLoss: 1.491481 1.491481 0.740584 \n",
      "Train Epoch: 9 [20480/30000](67%)]\tLoss: 1.495970 1.495970 0.747396 \n",
      "\n",
      "Test set: Average loss: 1.5091 0.5513, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 10 [0/30000](0%)]\tLoss: 1.496892 1.496892 0.732810 \n",
      "Train Epoch: 10 [10240/30000](33%)]\tLoss: 1.488807 1.488807 0.743784 \n",
      "Train Epoch: 10 [20480/30000](67%)]\tLoss: 1.485219 1.485219 0.739575 \n",
      "\n",
      "Test set: Average loss: 1.5076 0.5522, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 11 [0/30000](0%)]\tLoss: 1.495297 1.495297 0.754301 \n",
      "Train Epoch: 11 [10240/30000](33%)]\tLoss: 1.492064 1.492064 0.737260 \n",
      "Train Epoch: 11 [20480/30000](67%)]\tLoss: 1.492506 1.492506 0.735356 \n",
      "\n",
      "Test set: Average loss: 1.5053 0.5552, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 12 [0/30000](0%)]\tLoss: 1.490115 1.490115 0.754332 \n",
      "Train Epoch: 12 [10240/30000](33%)]\tLoss: 1.488537 1.488537 0.751243 \n",
      "Train Epoch: 12 [20480/30000](67%)]\tLoss: 1.488541 1.488541 0.739645 \n",
      "\n",
      "Test set: Average loss: 1.5039 0.5470, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 13 [0/30000](0%)]\tLoss: 1.487310 1.487310 0.725340 \n",
      "Train Epoch: 13 [10240/30000](33%)]\tLoss: 1.480590 1.480590 0.744360 \n",
      "Train Epoch: 13 [20480/30000](67%)]\tLoss: 1.483584 1.483584 0.746124 \n",
      "\n",
      "Test set: Average loss: 1.5045 0.5549, Accuracy: (95%) (50%) \n",
      "\n",
      "Train Epoch: 14 [0/30000](0%)]\tLoss: 1.490812 1.490812 0.722205 \n",
      "Train Epoch: 14 [10240/30000](33%)]\tLoss: 1.486516 1.486516 0.747160 \n",
      "Train Epoch: 14 [20480/30000](67%)]\tLoss: 1.486452 1.486452 0.759762 \n",
      "\n",
      "Test set: Average loss: 1.5011 0.5473, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 15 [0/30000](0%)]\tLoss: 1.476472 1.476472 0.738167 \n",
      "Train Epoch: 15 [10240/30000](33%)]\tLoss: 1.485683 1.485683 0.763149 \n",
      "Train Epoch: 15 [20480/30000](67%)]\tLoss: 1.491379 1.491379 0.737921 \n",
      "\n",
      "Test set: Average loss: 1.4994 0.5486, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 16 [0/30000](0%)]\tLoss: 1.483279 1.483279 0.740490 \n",
      "Train Epoch: 16 [10240/30000](33%)]\tLoss: 1.482215 1.482215 0.743893 \n",
      "Train Epoch: 16 [20480/30000](67%)]\tLoss: 1.487920 1.487920 0.729796 \n",
      "\n",
      "Test set: Average loss: 1.5005 0.5429, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 17 [0/30000](0%)]\tLoss: 1.482789 1.482789 0.741168 \n",
      "Train Epoch: 17 [10240/30000](33%)]\tLoss: 1.481242 1.481242 0.754970 \n",
      "Train Epoch: 17 [20480/30000](67%)]\tLoss: 1.486763 1.486763 0.745311 \n",
      "\n",
      "Test set: Average loss: 1.5003 0.5504, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 18 [0/30000](0%)]\tLoss: 1.482390 1.482390 0.731993 \n",
      "Train Epoch: 18 [10240/30000](33%)]\tLoss: 1.482789 1.482789 0.744411 \n",
      "Train Epoch: 18 [20480/30000](67%)]\tLoss: 1.480058 1.480058 0.750695 \n",
      "\n",
      "Test set: Average loss: 1.5013 0.5478, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 19 [0/30000](0%)]\tLoss: 1.483749 1.483749 0.749032 \n",
      "Train Epoch: 19 [10240/30000](33%)]\tLoss: 1.486393 1.486393 0.746398 \n",
      "Train Epoch: 19 [20480/30000](67%)]\tLoss: 1.479734 1.479734 0.742036 \n",
      "\n",
      "Test set: Average loss: 1.4988 0.5536, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 20 [0/30000](0%)]\tLoss: 1.477419 1.477419 0.744724 \n",
      "Train Epoch: 20 [10240/30000](33%)]\tLoss: 1.477607 1.477607 0.750323 \n",
      "Train Epoch: 20 [20480/30000](67%)]\tLoss: 1.480218 1.480218 0.732219 \n",
      "\n",
      "Test set: Average loss: 1.4978 0.5530, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 21 [0/30000](0%)]\tLoss: 1.855208 1.476988 0.756439 \n",
      "Train Epoch: 21 [10240/30000](33%)]\tLoss: 1.757519 1.476881 0.561275 \n",
      "Train Epoch: 21 [20480/30000](67%)]\tLoss: 1.726537 1.475824 0.501427 \n",
      "\n",
      "Test set: Average loss: 1.4981 0.2563, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 22 [0/30000](0%)]\tLoss: 1.731579 1.478211 0.506737 \n",
      "Train Epoch: 22 [10240/30000](33%)]\tLoss: 1.728572 1.478442 0.500260 \n",
      "Train Epoch: 22 [20480/30000](67%)]\tLoss: 1.731129 1.481319 0.499620 \n",
      "\n",
      "Test set: Average loss: 1.4969 0.2504, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 23 [0/30000](0%)]\tLoss: 1.728682 1.478548 0.500266 \n",
      "Train Epoch: 23 [10240/30000](33%)]\tLoss: 1.724507 1.474683 0.499650 \n",
      "Train Epoch: 23 [20480/30000](67%)]\tLoss: 1.729126 1.479304 0.499644 \n",
      "\n",
      "Test set: Average loss: 1.4975 0.2501, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 24 [0/30000](0%)]\tLoss: 1.722439 1.472326 0.500226 \n",
      "Train Epoch: 24 [10240/30000](33%)]\tLoss: 1.726418 1.476296 0.500245 \n",
      "Train Epoch: 24 [20480/30000](67%)]\tLoss: 1.727015 1.477234 0.499564 \n",
      "\n",
      "Test set: Average loss: 1.4964 0.2499, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 25 [0/30000](0%)]\tLoss: 1.726674 1.476679 0.499992 \n",
      "Train Epoch: 25 [10240/30000](33%)]\tLoss: 1.721720 1.472288 0.498862 \n",
      "Train Epoch: 25 [20480/30000](67%)]\tLoss: 1.730685 1.481101 0.499168 \n",
      "\n",
      "Test set: Average loss: 1.4962 0.2497, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 26 [0/30000](0%)]\tLoss: 1.726872 1.477274 0.499195 \n",
      "Train Epoch: 26 [10240/30000](33%)]\tLoss: 1.724327 1.474631 0.499391 \n",
      "Train Epoch: 26 [20480/30000](67%)]\tLoss: 1.730549 1.480120 0.500858 \n",
      "\n",
      "Test set: Average loss: 1.4950 0.2500, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 27 [0/30000](0%)]\tLoss: 1.725845 1.475571 0.500548 \n",
      "Train Epoch: 27 [10240/30000](33%)]\tLoss: 1.728872 1.478713 0.500318 \n",
      "Train Epoch: 27 [20480/30000](67%)]\tLoss: 1.729845 1.479711 0.500267 \n",
      "\n",
      "Test set: Average loss: 1.4974 0.2495, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 28 [0/30000](0%)]\tLoss: 1.728266 1.478610 0.499312 \n",
      "Train Epoch: 28 [10240/30000](33%)]\tLoss: 1.726192 1.476184 0.500016 \n",
      "Train Epoch: 28 [20480/30000](67%)]\tLoss: 1.724262 1.474331 0.499861 \n",
      "\n",
      "Test set: Average loss: 1.4957 0.2500, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 29 [0/30000](0%)]\tLoss: 1.724571 1.474346 0.500449 \n",
      "Train Epoch: 29 [10240/30000](33%)]\tLoss: 1.722171 1.471754 0.500835 \n",
      "Train Epoch: 29 [20480/30000](67%)]\tLoss: 1.725946 1.476325 0.499242 \n",
      "\n",
      "Test set: Average loss: 1.4947 0.2499, Accuracy: (97%) (50%) \n",
      "\n",
      "Train Epoch: 30 [0/30000](0%)]\tLoss: 1.729182 1.478995 0.500373 \n",
      "Train Epoch: 30 [10240/30000](33%)]\tLoss: 1.720261 1.470413 0.499696 \n",
      "Train Epoch: 30 [20480/30000](67%)]\tLoss: 1.728978 1.478723 0.500510 \n",
      "\n",
      "Test set: Average loss: 1.4961 0.2497, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 31 [0/30000](0%)]\tLoss: 1.722250 1.472333 0.499833 \n",
      "Train Epoch: 31 [10240/30000](33%)]\tLoss: 1.729541 1.479379 0.500325 \n",
      "Train Epoch: 31 [20480/30000](67%)]\tLoss: 1.721787 1.472481 0.498611 \n",
      "\n",
      "Test set: Average loss: 1.4957 0.2499, Accuracy: (96%) (50%) \n",
      "\n",
      "Train Epoch: 32 [0/30000](0%)]\tLoss: 1.725447 1.475570 0.499754 \n",
      "Train Epoch: 32 [10240/30000](33%)]\tLoss: 1.720309 1.470297 0.500023 \n",
      "Train Epoch: 32 [20480/30000](67%)]\tLoss: 1.728730 1.478978 0.499503 \n",
      "\n",
      "Test set: Average loss: 1.4954 0.2493, Accuracy: (97%) (51%) \n",
      "\n",
      "Train Epoch: 33 [0/30000](0%)]\tLoss: 1.717604 1.468750 0.497706 \n",
      "Train Epoch: 33 [10240/30000](33%)]\tLoss: 1.720913 1.471288 0.499249 \n",
      "Train Epoch: 33 [20480/30000](67%)]\tLoss: 1.722121 1.472152 0.499938 \n",
      "\n",
      "Test set: Average loss: 1.4940 0.2502, Accuracy: (97%) (50%) \n",
      "\n",
      "Train Epoch: 34 [0/30000](0%)]\tLoss: 1.720335 1.470858 0.498953 \n",
      "Train Epoch: 34 [10240/30000](33%)]\tLoss: 1.721304 1.471597 0.499413 \n",
      "Train Epoch: 34 [20480/30000](67%)]\tLoss: 1.719937 1.470092 0.499690 \n",
      "\n",
      "Test set: Average loss: 1.4946 0.2493, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 35 [0/30000](0%)]\tLoss: 1.718350 1.468219 0.500262 \n",
      "Train Epoch: 35 [10240/30000](33%)]\tLoss: 1.720264 1.470104 0.500319 \n",
      "Train Epoch: 35 [20480/30000](67%)]\tLoss: 1.724255 1.474903 0.498705 \n",
      "\n",
      "Test set: Average loss: 1.4955 0.2493, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 36 [0/30000](0%)]\tLoss: 1.718365 1.468750 0.499229 \n",
      "Train Epoch: 36 [10240/30000](33%)]\tLoss: 1.719278 1.469458 0.499640 \n",
      "Train Epoch: 36 [20480/30000](67%)]\tLoss: 1.719332 1.469971 0.498723 \n",
      "\n",
      "Test set: Average loss: 1.4954 0.2482, Accuracy: (96%) (52%) \n",
      "\n",
      "Train Epoch: 37 [0/30000](0%)]\tLoss: 1.719550 1.470027 0.499046 \n",
      "Train Epoch: 37 [10240/30000](33%)]\tLoss: 1.721890 1.472530 0.498719 \n",
      "Train Epoch: 37 [20480/30000](67%)]\tLoss: 1.724317 1.474615 0.499404 \n",
      "\n",
      "Test set: Average loss: 1.4939 0.2488, Accuracy: (96%) (51%) \n",
      "\n",
      "Train Epoch: 38 [0/30000](0%)]\tLoss: 1.718996 1.469447 0.499098 \n",
      "Train Epoch: 38 [10240/30000](33%)]\tLoss: 1.718237 1.468507 0.499460 \n",
      "Train Epoch: 38 [20480/30000](67%)]\tLoss: 1.723445 1.474579 0.497731 \n",
      "\n",
      "Test set: Average loss: 1.4943 0.2486, Accuracy: (97%) (52%) \n",
      "\n",
      "Train Epoch: 39 [0/30000](0%)]\tLoss: 1.726431 1.477265 0.498332 \n",
      "Train Epoch: 39 [10240/30000](33%)]\tLoss: 1.722882 1.473974 0.497815 \n",
      "Train Epoch: 39 [20480/30000](67%)]\tLoss: 1.722217 1.472262 0.499910 \n",
      "\n",
      "Test set: Average loss: 1.4939 0.2485, Accuracy: (97%) (51%) \n",
      "\n",
      "Train Epoch: 40 [0/30000](0%)]\tLoss: 1.723635 1.474303 0.498665 \n",
      "Train Epoch: 40 [10240/30000](33%)]\tLoss: 1.723215 1.473416 0.499600 \n",
      "Train Epoch: 40 [20480/30000](67%)]\tLoss: 1.720056 1.471366 0.497381 \n",
      "\n",
      "Test set: Average loss: 1.4955 0.2487, Accuracy: (97%) (51%) \n",
      "\n",
      "Train Epoch: 41 [0/30000](0%)]\tLoss: 1.718172 1.468815 0.498714 \n",
      "Train Epoch: 41 [10240/30000](33%)]\tLoss: 1.719615 1.470999 0.497232 \n",
      "Train Epoch: 41 [20480/30000](67%)]\tLoss: 1.718506 1.469378 0.498256 \n",
      "\n",
      "Test set: Average loss: 1.4939 0.2491, Accuracy: (97%) (51%) \n",
      "\n",
      "Train Epoch: 42 [0/30000](0%)]\tLoss: 1.721045 1.471533 0.499025 \n",
      "Train Epoch: 42 [10240/30000](33%)]\tLoss: 1.719341 1.470541 0.497598 \n",
      "Train Epoch: 42 [20480/30000](67%)]\tLoss: 1.718293 1.469491 0.497604 \n",
      "\n",
      "Test set: Average loss: 1.4955 0.2474, Accuracy: (97%) (52%) \n",
      "\n",
      "Train Epoch: 43 [0/30000](0%)]\tLoss: 1.714039 1.465489 0.497099 \n",
      "Train Epoch: 43 [10240/30000](33%)]\tLoss: 1.723316 1.474891 0.496851 \n",
      "Train Epoch: 43 [20480/30000](67%)]\tLoss: 1.717763 1.470026 0.495475 \n",
      "\n",
      "Test set: Average loss: 1.4965 0.2466, Accuracy: (96%) (53%) \n",
      "\n",
      "Train Epoch: 44 [0/30000](0%)]\tLoss: 1.718090 1.469515 0.497151 \n",
      "Train Epoch: 44 [10240/30000](33%)]\tLoss: 1.721081 1.472050 0.498062 \n",
      "Train Epoch: 44 [20480/30000](67%)]\tLoss: 1.720542 1.472552 0.495980 \n",
      "\n",
      "Test set: Average loss: 1.4944 0.2470, Accuracy: (97%) (53%) \n",
      "\n",
      "Train Epoch: 45 [0/30000](0%)]\tLoss: 1.719169 1.470499 0.497342 \n",
      "Train Epoch: 45 [10240/30000](33%)]\tLoss: 1.719899 1.472646 0.494505 \n",
      "Train Epoch: 45 [20480/30000](67%)]\tLoss: 1.720101 1.473143 0.493915 \n",
      "\n",
      "Test set: Average loss: 1.4940 0.2452, Accuracy: (96%) (54%) \n",
      "\n",
      "Train Epoch: 46 [0/30000](0%)]\tLoss: 1.716226 1.468288 0.495875 \n",
      "Train Epoch: 46 [10240/30000](33%)]\tLoss: 1.721416 1.475395 0.492042 \n",
      "Train Epoch: 46 [20480/30000](67%)]\tLoss: 1.717361 1.472081 0.490560 \n",
      "\n",
      "Test set: Average loss: 1.4996 0.2352, Accuracy: (97%) (58%) \n",
      "\n",
      "Train Epoch: 47 [0/30000](0%)]\tLoss: 1.720036 1.476964 0.486144 \n",
      "Train Epoch: 47 [10240/30000](33%)]\tLoss: 1.716616 1.478031 0.477170 \n",
      "Train Epoch: 47 [20480/30000](67%)]\tLoss: 1.712521 1.483898 0.457246 \n",
      "\n",
      "Test set: Average loss: 1.5212 0.1763, Accuracy: (93%) (73%) \n",
      "\n",
      "Train Epoch: 48 [0/30000](0%)]\tLoss: 1.711480 1.500016 0.422928 \n",
      "Train Epoch: 48 [10240/30000](33%)]\tLoss: 1.696081 1.516314 0.359534 \n",
      "Train Epoch: 48 [20480/30000](67%)]\tLoss: 1.676212 1.535291 0.281842 \n",
      "\n",
      "Test set: Average loss: 1.5644 0.0620, Accuracy: (74%) (93%) \n",
      "\n",
      "Train Epoch: 49 [0/30000](0%)]\tLoss: 1.676100 1.548182 0.255837 \n",
      "Train Epoch: 49 [10240/30000](33%)]\tLoss: 1.669653 1.553189 0.232927 \n",
      "Train Epoch: 49 [20480/30000](67%)]\tLoss: 1.656758 1.558014 0.197489 \n",
      "\n",
      "Test set: Average loss: 1.5675 0.0425, Accuracy: (73%) (97%) \n",
      "\n",
      "Train Epoch: 50 [0/30000](0%)]\tLoss: 1.639562 1.550009 0.179107 \n",
      "Train Epoch: 50 [10240/30000](33%)]\tLoss: 1.660122 1.558098 0.204047 \n",
      "Train Epoch: 50 [20480/30000](67%)]\tLoss: 1.653561 1.560522 0.186077 \n",
      "\n",
      "Test set: Average loss: 1.5804 0.0378, Accuracy: (69%) (97%) \n",
      "\n",
      "Train Epoch: 51 [0/30000](0%)]\tLoss: 1.642079 1.553690 0.176777 \n",
      "Train Epoch: 51 [10240/30000](33%)]\tLoss: 1.649354 1.559308 0.180093 \n",
      "Train Epoch: 51 [20480/30000](67%)]\tLoss: 1.637277 1.552701 0.169153 \n",
      "\n",
      "Test set: Average loss: 1.5735 0.0343, Accuracy: (74%) (97%) \n",
      "\n",
      "Train Epoch: 52 [0/30000](0%)]\tLoss: 1.659129 1.564846 0.188566 \n",
      "Train Epoch: 52 [10240/30000](33%)]\tLoss: 1.628973 1.555679 0.146587 \n",
      "Train Epoch: 52 [20480/30000](67%)]\tLoss: 1.632968 1.550297 0.165342 \n",
      "\n",
      "Test set: Average loss: 1.5683 0.0336, Accuracy: (75%) (97%) \n",
      "\n",
      "Train Epoch: 53 [0/30000](0%)]\tLoss: 1.636999 1.560926 0.152146 \n",
      "Train Epoch: 53 [10240/30000](33%)]\tLoss: 1.638931 1.554258 0.169346 \n",
      "Train Epoch: 53 [20480/30000](67%)]\tLoss: 1.645771 1.564289 0.162964 \n",
      "\n",
      "Test set: Average loss: 1.5742 0.0270, Accuracy: (70%) (98%) \n",
      "\n",
      "Train Epoch: 54 [0/30000](0%)]\tLoss: 1.634121 1.562664 0.142913 \n",
      "Train Epoch: 54 [10240/30000](33%)]\tLoss: 1.652668 1.562104 0.181128 \n",
      "Train Epoch: 54 [20480/30000](67%)]\tLoss: 1.627697 1.554723 0.145948 \n",
      "\n",
      "Test set: Average loss: 1.5712 0.0326, Accuracy: (76%) (97%) \n",
      "\n",
      "Train Epoch: 55 [0/30000](0%)]\tLoss: 1.632346 1.546950 0.170792 \n",
      "Train Epoch: 55 [10240/30000](33%)]\tLoss: 1.632782 1.559019 0.147526 \n",
      "Train Epoch: 55 [20480/30000](67%)]\tLoss: 1.625516 1.556732 0.137568 \n",
      "\n",
      "Test set: Average loss: 1.5714 0.0260, Accuracy: (74%) (98%) \n",
      "\n",
      "Train Epoch: 56 [0/30000](0%)]\tLoss: 1.617782 1.555894 0.123775 \n",
      "Train Epoch: 56 [10240/30000](33%)]\tLoss: 1.617204 1.548218 0.137973 \n",
      "Train Epoch: 56 [20480/30000](67%)]\tLoss: 1.615177 1.554114 0.122124 \n",
      "\n",
      "Test set: Average loss: 1.5750 0.0294, Accuracy: (71%) (97%) \n",
      "\n",
      "Train Epoch: 57 [0/30000](0%)]\tLoss: 1.623814 1.550990 0.145649 \n",
      "Train Epoch: 57 [10240/30000](33%)]\tLoss: 1.630739 1.561982 0.137513 \n",
      "Train Epoch: 57 [20480/30000](67%)]\tLoss: 1.626292 1.555078 0.142428 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-44-aaabfd2fdfbc>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m           epoch, torch.nn.CrossEntropyLoss())\n\u001B[0;32m      9\u001B[0m     test(net, net2, device, test_loader,\n\u001B[1;32m---> 10\u001B[1;33m          torch.nn.CrossEntropyLoss())\n\u001B[0m\u001B[0;32m     11\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"mnist_cnn.pt\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"mlp.pt\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-42-6476778eb828>\u001B[0m in \u001B[0;36mtest\u001B[1;34m(model, model2, device, test_loader, model_loss)\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mtest_loss2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrect2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrotate_target\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m             \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 435\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    473\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    474\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 475\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    476\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    477\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    270\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    271\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 272\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    273\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    274\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-1f6cb4d94f64>\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[1;31m# image = image.rotate(rotate * 90)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrotate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    102\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConverted\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    103\u001B[0m         \"\"\"\n\u001B[1;32m--> 104\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    105\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001B[0m in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    101\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 102\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m255\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    103\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "net = BooleanConcepts().to(device)\n",
    "net2 = MLP(input_width=(1,digit_classes))\n",
    "\n",
    "optimizer = torch.optim.AdamW(itertools.chain(net.parameters(), net2.parameters()))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(net, net2, device, train_loader, optimizer,\n",
    "          epoch, torch.nn.CrossEntropyLoss())\n",
    "    test(net, net2, device, test_loader,\n",
    "         torch.nn.CrossEntropyLoss())\n",
    "    torch.save(net.state_dict(), \"mnist_cnn.pt\")\n",
    "    torch.save(net2.state_dict(), \"mlp.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}