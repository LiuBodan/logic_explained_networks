{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import simplify_logic\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import _tree, export_text\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deep_logic.utils.base import validate_network, set_seed, tree_to_formula\n",
    "from deep_logic.utils.relu_nn import get_reduced_model, prune_features\n",
    "from deep_logic.utils.psi_nn import prune_equal_fanin\n",
    "from deep_logic import logic\n",
    "\n",
    "results_dir = './results/mnist'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "concepts = [f'c{i}' for i in range(10)]\n",
    "n_rep = 10\n",
    "tot_epochs = 4001\n",
    "prune_epochs = 2000\n",
    "concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f7ad9c710b413a8c4a824b3807a9a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-bee155ed6c3a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mtransforms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNormalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.5\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.5\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m ])\n\u001B[1;32m---> 15\u001B[1;33m \u001B[0mtrain_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMNIST\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'./data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_transforms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMNIST\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'./data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_transforms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdownload\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 79\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     80\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_exists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001B[0m in \u001B[0;36mdownload\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    144\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmd5\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresources\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m             \u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrpartition\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'/'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 146\u001B[1;33m             \u001B[0mdownload_and_extract_archive\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload_root\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraw_folder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmd5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m         \u001B[1;31m# process and save as torch files\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001B[0m in \u001B[0;36mdownload_and_extract_archive\u001B[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001B[0m\n\u001B[0;32m    254\u001B[0m         \u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbasename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 256\u001B[1;33m     \u001B[0mdownload_url\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload_root\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    257\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    258\u001B[0m     \u001B[0marchive\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdownload_root\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001B[0m in \u001B[0;36mdownload_url\u001B[1;34m(url, root, filename, md5)\u001B[0m\n\u001B[0;32m     70\u001B[0m             urllib.request.urlretrieve(\n\u001B[0;32m     71\u001B[0m                 \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfpath\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m                 \u001B[0mreporthook\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgen_bar_updater\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m             )\n\u001B[0;32m     74\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0murllib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mURLError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# type: ignore[attr-defined]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36murlretrieve\u001B[1;34m(url, filename, reporthook, data)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[0murl_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msplittype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 247\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mcontextlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclosing\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murlopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    248\u001B[0m         \u001B[0mheaders\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    220\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \u001B[0mopener\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_opener\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mopener\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0minstall_opener\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    523\u001B[0m             \u001B[0mreq\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmeth\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    524\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 525\u001B[1;33m         \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    526\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    527\u001B[0m         \u001B[1;31m# post-process response\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36m_open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    541\u001B[0m         \u001B[0mprotocol\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    542\u001B[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001B[1;32m--> 543\u001B[1;33m                                   '_open', req)\n\u001B[0m\u001B[0;32m    544\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    545\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36m_call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    501\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhandler\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mhandlers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    502\u001B[0m             \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeth_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 503\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    504\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    505\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mhttp_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1345\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1346\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mhttp_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1347\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhttp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mHTTPConnection\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1348\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1349\u001B[0m     \u001B[0mhttp_request\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAbstractHTTPHandler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_request_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mdo_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1320\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;31m# timeout error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1321\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mURLError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1322\u001B[1;33m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetresponse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1323\u001B[0m         \u001B[1;32mexcept\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1324\u001B[0m             \u001B[0mh\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\http\\client.py\u001B[0m in \u001B[0;36mgetresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1342\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1343\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1344\u001B[1;33m                 \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbegin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1345\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1346\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\http\\client.py\u001B[0m in \u001B[0;36mbegin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    304\u001B[0m         \u001B[1;31m# read until we get a non-100 response\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    305\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 306\u001B[1;33m             \u001B[0mversion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstatus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreason\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    307\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mstatus\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mCONTINUE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m                 \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\http\\client.py\u001B[0m in \u001B[0;36m_read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    266\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_read_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 267\u001B[1;33m         \u001B[0mline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_MAXLINE\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"iso-8859-1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    268\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0m_MAXLINE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    269\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mLineTooLong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"status line\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    587\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 589\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    590\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    591\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# MNIST problem\n",
    "num_workers = 0\n",
    "batch_size = 128\n",
    "valid_size = 0.2\n",
    "# Data augmentation for train data + conversion to tensor\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "   \n",
    "])# Data augmentation for test data + conversion to tensor\n",
    "test_transforms= transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding indices for validation set\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "#Randomize indices\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(num_train*valid_size))\n",
    "train_index, test_index = indices[split:], indices[:split]# Making samplers for training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(test_index)# Creating data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, 3, padding =1)\n",
    "        # linear layers\n",
    "        self.fc1 = torch.nn.Linear(784, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, 10) \n",
    "        # dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        # max pooling\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convolutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flattening the image\n",
    "        x = x.view(-1, 7*7*16)\n",
    "        # linear layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "model = Net()\n",
    "print(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "if os.path.isfile('./models/mnist/trained_model.pt'):\n",
    "    model.load_state_dict(torch.load('./models/mnist/trained_model.pt'))\n",
    "\n",
    "else:\n",
    "    # epochs to train for\n",
    "    epochs = 25\n",
    "    set_seed(0)\n",
    "\n",
    "    # tracks validation loss change after each epoch\n",
    "    minimum_validation_loss = np.inf \n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # training steps\n",
    "        model.train()\n",
    "        for batch_index, (data, target) in enumerate(train_loader):\n",
    "            # moves tensors to GPU\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # clears gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # loss in batch\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass for loss gradient\n",
    "            loss.backward()\n",
    "            # update paremeters\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # validation steps\n",
    "        model.eval()\n",
    "        for batch_index, (data, target) in enumerate(valid_loader):\n",
    "            # moves tensors to GPU\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # loss in batch\n",
    "            loss = criterion(output, target)\n",
    "            # update validation loss\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # average loss calculations\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # Display loss statistics\n",
    "        print(f'Current Epoch: {epoch}\\nTraining Loss: {round(train_loss, 6)}\\nValidation Loss: {round(valid_loss, 6)}')\n",
    "\n",
    "        # Saving model every time validation loss decreases\n",
    "        if valid_loss <= minimum_validation_loss and epoch > 20:\n",
    "            print(f'Validation loss decreased from {round(minimum_validation_loss, 6)} to {round(valid_loss, 6)}')\n",
    "            torch.save(model.state_dict(), './models/mnist/trained_model.pt')\n",
    "            minimum_validation_loss = valid_loss\n",
    "            print('Saving New Model')\n",
    "    \n",
    "    model.load_state_dict(torch.load('./models/mnist/trained_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # move tensors to GPU\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    # batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # test loss update\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(10):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print(f'Test Loss: {round(test_loss, 6)}')\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Test Accuracy of {classes[i]}: {round(100*class_correct[i]/class_total[i], 2)}%')\n",
    "    else:\n",
    "        print(f'Test Accuracy of {classes[i]}s: N/A (no training examples)')\n",
    "        \n",
    "        \n",
    "print(f'Full Test Accuracy: {round(100. * np.sum(class_correct) / np.sum(class_total), 2)}% {np.sum(class_correct)} out of {np.sum(class_total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c_train = []\n",
    "true_c_train = []\n",
    "model.eval()\n",
    "for batch_index, (data, target) in enumerate(train_loader):\n",
    "    # moves tensors to GPU\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    pred = torch.argmax(output, 1)\n",
    "    pred_c_train.append(pred)\n",
    "    true_c_train.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c_train = torch.cat(pred_c_train)\n",
    "true_c_train = torch.cat(true_c_train)\n",
    "pred_c_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = (true_c_train % 2 == 1).to(torch.long)\n",
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainval = OneHotEncoder(sparse=False).fit_transform(pred_c_train.cpu().detach().numpy().reshape(-1, 1))\n",
    "x_trainval = torch.FloatTensor(x_trainval)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=300, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c_test = []\n",
    "true_c_test = []\n",
    "model.eval()\n",
    "for batch_index, (data, target) in enumerate(test_loader):\n",
    "    # moves tensors to GPU\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    pred = torch.argmax(output, 1)\n",
    "    pred_c_test.append(pred)\n",
    "    true_c_test.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c_test = torch.cat(pred_c_test)\n",
    "true_c_test = torch.cat(true_c_test)\n",
    "pred_c_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = (true_c_test % 2 == 1).to(torch.long)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = OneHotEncoder(sparse=False).fit_transform(true_c_test.cpu().detach().numpy().reshape(-1, 1))\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(x_train, y_train, need_pruning, seed, device, relu=False, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 100),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(100, 50),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(50, 30),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(30, 2),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.001 * torch.norm(module.weight, 1)\n",
    "                loss += 0.001 * torch.norm(module.bias, 1)\n",
    "                break\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            prune_features(model, n_classes=1, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_psi_nn(x_train, y_train, need_pruning, seed, device, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device).to(torch.float)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 10),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(10, 4),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(4, 1),\n",
    "        torch.nn.Sigmoid(),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.BCELoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train).squeeze()\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.00001 * torch.norm(module.weight, 1)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            model = prune_equal_fanin(model, 2, validate=True, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            y_pred_d = y_pred > 0.5\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_to_y(method, need_pruning, relu, verbose=False):\n",
    "    methods = []\n",
    "    splits = []\n",
    "    explanations = []\n",
    "    explanations_inv = []\n",
    "    model_accuracies = []\n",
    "    explanation_accuracies = []\n",
    "    explanation_accuracies_inv = []\n",
    "    elapsed_times = []\n",
    "    elapsed_times_inv = []\n",
    "    for seed in range(n_rep):\n",
    "        explanation, explanation_inv = '', ''\n",
    "        explanation_accuracy, explanation_accuracy_inv = 0, 0\n",
    "        \n",
    "        print(f'Seed [{seed+1}/{n_rep}]')\n",
    "        \n",
    "        if method == 'tree':\n",
    "            classifier = DecisionTreeClassifier(random_state=seed)\n",
    "            classifier.fit(x_train.cpu().detach().numpy(), y_train.cpu().detach().numpy())\n",
    "            y_preds = classifier.predict(x_test.cpu().detach().numpy())\n",
    "            model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds)\n",
    "\n",
    "            target_class = 1\n",
    "            start = time.time()\n",
    "            explanation = tree_to_formula(classifier, concepts, target_class)\n",
    "            elapsed_time = time.time() - start\n",
    "            explanation_accuracy = model_accuracy\n",
    "\n",
    "            target_class_inv = 0\n",
    "            start = time.time()\n",
    "            explanation_inv = tree_to_formula(classifier, concepts, target_class_inv)\n",
    "            elapsed_time_inv = time.time() - start\n",
    "            explanation_accuracy_inv = model_accuracy\n",
    "        \n",
    "        else:\n",
    "            if method == 'psi':\n",
    "                # positive class\n",
    "                target_class = 1\n",
    "                model = train_psi_nn(x_train, y_train.eq(target_class), need_pruning, seed, device, verbose)\n",
    "                y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds>0.5)\n",
    "                \n",
    "            else:\n",
    "                model = train_nn(x_train, y_train, need_pruning, seed, device, relu, verbose)\n",
    "                y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds.argmax(axis=1))\n",
    "\n",
    "            # positive class\n",
    "            target_class = 1\n",
    "            start = time.time()\n",
    "            if method == 'psi':\n",
    "                global_explanation = logic.generate_fol_explanations(model, device)[0]\n",
    "            else:\n",
    "                global_explanation, _, _ = logic.relu_nn.combine_local_explanations(model, \n",
    "                                                                                   x_val.to(device), \n",
    "                                                                                   y_val.to(device), \n",
    "                                                                                   topk_explanations=5,\n",
    "                                                                                   target_class=target_class,\n",
    "                                                                                   method=method, device=device)\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            if global_explanation:\n",
    "                explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "                explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "\n",
    "            # negative class\n",
    "            target_class_inv = 0\n",
    "            if method == 'psi':\n",
    "                model = train_psi_nn(x_train, y_train.eq(target_class_inv), need_pruning, seed, device, verbose)\n",
    "            \n",
    "            start = time.time()\n",
    "            if method == 'psi':\n",
    "                global_explanation_inv = logic.generate_fol_explanations(model, device)[0]\n",
    "            else:\n",
    "                global_explanation_inv, _, _ = logic.relu_nn.combine_local_explanations(model, \n",
    "                                                                                       x_val.to(device), \n",
    "                                                                                       y_val.to(device), \n",
    "                                                                                       topk_explanations=5,\n",
    "                                                                                       target_class=target_class_inv,\n",
    "                                                                                       method=method, device=device)\n",
    "            elapsed_time_inv = time.time() - start\n",
    "            if global_explanation_inv:\n",
    "                explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, \n",
    "                                                                          target_class_inv, x_test, y_test)\n",
    "                explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "            print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "            print(f'\\t Elapsed time {elapsed_time}')\n",
    "            print(f'\\t Class {target_class_inv} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "            print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "\n",
    "        methods.append(method)\n",
    "        splits.append(seed)\n",
    "        explanations.append(explanation)\n",
    "        explanations_inv.append(explanation_inv)\n",
    "        model_accuracies.append(model_accuracy)\n",
    "        explanation_accuracies.append(explanation_accuracy)\n",
    "        explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "        elapsed_times.append(elapsed_time)\n",
    "        elapsed_times_inv.append(elapsed_time_inv)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'method': methods,\n",
    "        'split': splits,\n",
    "        'explanation': explanations,\n",
    "        'explanation_inv': explanations_inv,\n",
    "        'model_accuracy': model_accuracies,\n",
    "        'explanation_accuracy': explanation_accuracies,\n",
    "        'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "        'elapsed_time': elapsed_times,\n",
    "        'elapsed_time_inv': elapsed_times_inv,\n",
    "    })\n",
    "    results.to_csv(os.path.join(results_dir, f'results_{method}.csv'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'pruning'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_pruning = c_to_y(method, need_pruning, relu)\n",
    "results_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'lime'\n",
    "# need_pruning = False\n",
    "# relu = False\n",
    "# results_lime = c_to_y(method, need_pruning, relu)\n",
    "# results_lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'weights'\n",
    "need_pruning = False\n",
    "relu = True\n",
    "results_weights = c_to_y(method, need_pruning, relu)\n",
    "results_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'psi'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_psi = c_to_y(method, need_pruning, relu, verbose=False)\n",
    "results_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'tree'\n",
    "need_pruning = False\n",
    "relu = False\n",
    "results_tree = c_to_y(method, need_pruning, relu)\n",
    "results_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model_accuracy', 'explanation_accuracy', 'explanation_accuracy_inv', 'elapsed_time', 'elapsed_time_inv']\n",
    "mean_cols = [f'{c}_mean' for c in cols]\n",
    "sem_cols = [f'{c}_sem' for c in cols]\n",
    "\n",
    "# pruning\n",
    "df_mean = results_pruning[cols].mean()\n",
    "df_sem = results_pruning[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_pruning = pd.concat([df_mean, df_sem])\n",
    "summary_pruning.name = 'pruning'\n",
    "\n",
    "# # lime\n",
    "# df_mean = results_lime[cols].mean()\n",
    "# df_sem = results_lime[cols].sem()\n",
    "# df_mean.columns = mean_cols\n",
    "# df_sem.columns = sem_cols\n",
    "# summary_lime = pd.concat([df_mean, df_sem])\n",
    "# summary_lime.name = 'lime'\n",
    "\n",
    "# weights\n",
    "df_mean = results_weights[cols].mean()\n",
    "df_sem = results_weights[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_weights = pd.concat([df_mean, df_sem])\n",
    "summary_weights.name = 'weights'\n",
    "\n",
    "# psi\n",
    "df_mean = results_psi[cols].mean()\n",
    "df_sem = results_psi[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_psi = pd.concat([df_mean, df_sem])\n",
    "summary_psi.name = 'psi'\n",
    "\n",
    "# tree\n",
    "df_mean = results_tree[cols].mean()\n",
    "df_sem = results_tree[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_tree = pd.concat([df_mean, df_sem])\n",
    "summary_tree.name = 'tree'\n",
    "\n",
    "summary = pd.concat([summary_pruning, \n",
    "#                      summary_lime, \n",
    "                     summary_weights, summary_psi, summary_tree], axis=1).T\n",
    "summary.columns = mean_cols + sem_cols\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(os.path.join(results_dir, 'summary.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}