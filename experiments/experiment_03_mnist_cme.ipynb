{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import simplify_logic\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import _tree, export_text\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from deep_logic.utils.base import validate_network, set_seed, tree_to_formula\n",
    "from deep_logic.utils.relunn import get_reduced_model, prune_features\n",
    "from deep_logic.utils.sigmoidnn import prune_equal_fanin\n",
    "from deep_logic import logic\n",
    "\n",
    "results_dir = 'results/mnist_h'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_rep = 10\n",
    "tot_epochs = 4001\n",
    "prune_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST problem\n",
    "num_workers = 0\n",
    "batch_size = 128\n",
    "valid_size = 0.2\n",
    "# Data augmentation for train data + conversion to tensor\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "   \n",
    "])# Data augmentation for test data + conversion to tensor\n",
    "test_transforms= transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding indices for validation set\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "#Randomize indices\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(num_train*valid_size))\n",
    "train_index, test_index = indices[split:], indices[:split]# Making samplers for training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(test_index)# Creating data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, 3, padding =1)\n",
    "        # linear layers\n",
    "        self.fc1 = torch.nn.Linear(784, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, 2) \n",
    "        # dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        # max pooling\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convolutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flattening the image\n",
    "        x = x.view(-1, 7*7*16)\n",
    "        # linear layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "model = Net()\n",
    "print(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "if os.path.isfile('./models/mnist_h/trained_model_h.pt'):\n",
    "    model.load_state_dict(torch.load('./models/mnist_h/trained_model_h.pt'))\n",
    "\n",
    "else:\n",
    "    # epochs to train for\n",
    "    epochs = 10\n",
    "    set_seed(0)\n",
    "\n",
    "    # tracks validation loss change after each epoch\n",
    "    minimum_validation_loss = np.inf \n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # training steps\n",
    "        model.train()\n",
    "        for batch_index, (data, target) in enumerate(train_loader):\n",
    "            # moves tensors to GPU\n",
    "            data, target = data.cuda(), target.cuda() % 2\n",
    "            # clears gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # loss in batch\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass for loss gradient\n",
    "            loss.backward()\n",
    "            # update paremeters\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # validation steps\n",
    "        model.eval()\n",
    "        for batch_index, (data, target) in enumerate(valid_loader):\n",
    "            # moves tensors to GPU\n",
    "            data, target = data.cuda(), target.cuda() % 2\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # loss in batch\n",
    "            loss = criterion(output, target)\n",
    "            # update validation loss\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # average loss calculations\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # Display loss statistics\n",
    "        print(f'Current Epoch: {epoch}\\nTraining Loss: {round(train_loss, 6)}\\nValidation Loss: {round(valid_loss, 6)}')\n",
    "\n",
    "        # Saving model every time validation loss decreases\n",
    "        if valid_loss <= minimum_validation_loss and epoch > 5:\n",
    "            print(f'Validation loss decreased from {round(minimum_validation_loss, 6)} to {round(valid_loss, 6)}')\n",
    "            torch.save(model.state_dict(), './models/mnist_h/trained_model_h.pt')\n",
    "            minimum_validation_loss = valid_loss\n",
    "            print('Saving New Model')\n",
    "    \n",
    "    model.load_state_dict(torch.load('./models/mnist_h/trained_model_h.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.320503\n",
      "Test Accuracy of 0: 100.0%\n",
      "Test Accuracy of 1: 97.4%\n",
      "Full Test Accuracy: 98.73% 156.0 out of 158.0\n"
     ]
    }
   ],
   "source": [
    "# tracking test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "classes = [0, 1]\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # move tensors to GPU\n",
    "    data, target = data.cuda(), target.cuda() % 2\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    # batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # test loss update\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(2):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print(f'Test Loss: {round(test_loss, 6)}')\n",
    "\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Test Accuracy of {classes[i]}: {round(100*class_correct[i]/class_total[i], 2)}%')\n",
    "    else:\n",
    "        print(f'Test Accuracy of {classes[i]}s: N/A (no training examples)')\n",
    "        \n",
    "        \n",
    "print(f'Full Test Accuracy: {round(100. * np.sum(class_correct) / np.sum(class_total), 2)}% {np.sum(class_correct)} out of {np.sum(class_total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract concepts from hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 128])\n",
      "torch.Size([48000])\n"
     ]
    }
   ],
   "source": [
    "pred_h_train = []\n",
    "true_y_train = []\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "for batch_index, (data, target) in enumerate(train_loader):\n",
    "    # moves tensors to GPU\n",
    "    data, target = data.cpu(), target.cpu() % 2\n",
    "    # forward pass\n",
    "    #output = model(data)\n",
    "    #pred = torch.argmax(output, 1)\n",
    "    \n",
    "    # get hidden layer's projection of the input\n",
    "    x = model.pool(F.relu(model.conv1(data)))\n",
    "    x = model.pool(F.relu(model.conv2(x)))\n",
    "    # flattening the image\n",
    "    x = x.view(-1, 7*7*16)\n",
    "    # linear layers\n",
    "    x = model.dropout(F.relu(model.fc1(x)))\n",
    "    x = model.dropout(F.relu(model.fc2(x)))\n",
    "    \n",
    "    pred_h_train.append(x.cpu())\n",
    "    true_y_train.append(target.cpu())\n",
    "\n",
    "pred_h_train = torch.cat(pred_h_train)\n",
    "true_y_train = torch.cat(true_y_train)\n",
    "print(pred_h_train.shape)\n",
    "print(true_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 128])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "pred_h_test = []\n",
    "true_y_test = []\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "for batch_index, (data, target) in enumerate(test_loader):\n",
    "    # moves tensors to GPU\n",
    "    data, target = data.cpu(), target.cpu() % 2\n",
    "    # forward pass\n",
    "    #output = model(data)\n",
    "    #pred = torch.argmax(output, 1)\n",
    "    \n",
    "    # get hidden layer's projection of the input\n",
    "    x = model.pool(F.relu(model.conv1(data)))\n",
    "    x = model.pool(F.relu(model.conv2(x)))\n",
    "    # flattening the image\n",
    "    x = x.view(-1, 7*7*16)\n",
    "    # linear layers\n",
    "    x = model.dropout(F.relu(model.fc1(x)))\n",
    "    x = model.dropout(F.relu(model.fc2(x)))\n",
    "    \n",
    "    pred_h_test.append(x.cpu())\n",
    "    true_y_test.append(target.cpu())\n",
    "\n",
    "pred_h_test = torch.cat(pred_h_test)\n",
    "true_y_test = torch.cat(true_y_test)\n",
    "print(pred_h_test.shape)\n",
    "print(true_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(pred_h_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(pred_h_train.detach().numpy())\n",
    "X_train = kmeans.predict(pred_h_train.detach().numpy())\n",
    "X_test = kmeans.predict(pred_h_test.detach().numpy())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=X_train)\n",
    "plt.savefig('mnist_hidden.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 10])\n",
      "torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor(OneHotEncoder(sparse=False).fit_transform(X_train.reshape(-1, 1)))\n",
    "x_test = torch.FloatTensor(OneHotEncoder(sparse=False).fit_transform(X_test.reshape(-1, 1)))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "y_train = (true_y_train % 2 == 1).to(torch.long)\n",
    "y_test = (true_y_test % 2 == 1).to(torch.long)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts = [f'h{i}' for i in range(x_train.shape[1])]\n",
    "concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(x_train, y_train, need_pruning, seed, device, relu=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 100),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(100, 50),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(50, 30),\n",
    "        torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(30, 2),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.01 * torch.norm(module.weight, 1)\n",
    "                loss += 0.01 * torch.norm(module.bias, 1)\n",
    "                break\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            prune_features(model, n_classes=1, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0:\n",
    "            y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed [1/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 36.361433029174805\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 35.48980164527893\n",
      "Seed [2/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 37.497822523117065\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 35.82303237915039\n",
      "Seed [3/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 39.869996309280396\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 36.92844867706299\n",
      "Seed [4/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 45.667250633239746\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 41.919816970825195\n",
      "Seed [5/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 37.819796562194824\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 36.97877240180969\n",
      "Seed [6/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 45.66890025138855\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 38.729487657547\n",
      "Seed [7/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 38.311485290527344\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 35.54310321807861\n",
      "Seed [8/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 38.26169276237488\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 37.81199550628662\n",
      "Seed [9/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 40.87632632255554\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 40.63102889060974\n",
      "Seed [10/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 44.42517709732056\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 45.02134656906128\n"
     ]
    }
   ],
   "source": [
    "need_pruning = True\n",
    "method = 'pruning'\n",
    "methods = []\n",
    "splits = []\n",
    "explanations = []\n",
    "explanations_inv = []\n",
    "model_accuracies = []\n",
    "explanation_accuracies = []\n",
    "explanation_accuracies_inv = []\n",
    "elapsed_times = []\n",
    "elapsed_times_inv = []\n",
    "for seed in range(n_rep):\n",
    "    print(f'Seed [{seed+1}/{n_rep}]')\n",
    "    \n",
    "    model = train_nn(x_train, y_train, need_pruning, seed, device)\n",
    "    \n",
    "    y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "    model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds.argmax(axis=1))\n",
    "    print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "    # positive class\n",
    "    target_class = 1\n",
    "    start = time.time()\n",
    "    global_explanation, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "                                                                       x_train.to(device), y_train.to(device), \n",
    "                                                                       target_class=target_class,\n",
    "                                                                       topk_explanations=10,\n",
    "                                                                       method=method, device=device)\n",
    "    elapsed_time = time.time() - start\n",
    "    if global_explanation:\n",
    "        explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "        explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time}')\n",
    "        \n",
    "    # negative class\n",
    "    target_class = 0\n",
    "    start = time.time()\n",
    "    global_explanation_inv, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "                                                                           x_train.to(device), y_train.to(device), \n",
    "                                                                           target_class=target_class,\n",
    "                                                                           topk_explanations=10,\n",
    "                                                                           method=method, device=device)\n",
    "    elapsed_time_inv = time.time() - start\n",
    "    if global_explanation_inv:\n",
    "        explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, target_class, x_test, y_test)\n",
    "        explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "    \n",
    "    methods.append(method)\n",
    "    splits.append(seed)\n",
    "    explanations.append(explanation)\n",
    "    explanations_inv.append(explanation_inv)\n",
    "    model_accuracies.append(model_accuracy)\n",
    "    explanation_accuracies.append(explanation_accuracy)\n",
    "    explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "    elapsed_times.append(elapsed_time)\n",
    "    elapsed_times_inv.append(elapsed_time_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>explanation_inv</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_accuracy_inv</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>elapsed_time_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pruning</td>\n",
       "      <td>0</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>36.361433</td>\n",
       "      <td>35.489802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pruning</td>\n",
       "      <td>1</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>37.497823</td>\n",
       "      <td>35.823032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pruning</td>\n",
       "      <td>2</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>39.869996</td>\n",
       "      <td>36.928449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pruning</td>\n",
       "      <td>3</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>45.667251</td>\n",
       "      <td>41.919817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pruning</td>\n",
       "      <td>4</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>37.819797</td>\n",
       "      <td>36.978772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pruning</td>\n",
       "      <td>5</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>45.668900</td>\n",
       "      <td>38.729488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pruning</td>\n",
       "      <td>6</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>38.311485</td>\n",
       "      <td>35.543103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pruning</td>\n",
       "      <td>7</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>38.261693</td>\n",
       "      <td>37.811996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pruning</td>\n",
       "      <td>8</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>40.876326</td>\n",
       "      <td>40.631029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pruning</td>\n",
       "      <td>9</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>44.425177</td>\n",
       "      <td>45.021347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    method  split            explanation    explanation_inv  model_accuracy  \\\n",
       "0  pruning      0  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "1  pruning      1  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "2  pruning      2  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "3  pruning      3  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "4  pruning      4  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "5  pruning      5  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "6  pruning      6  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "7  pruning      7  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "8  pruning      8  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "9  pruning      9  ~h1 & ~h3 & ~h7 & ~h9  h1 | h3 | h7 | h9          0.9929   \n",
       "\n",
       "   explanation_accuracy  explanation_accuracy_inv  elapsed_time  \\\n",
       "0                0.9929                    0.9929     36.361433   \n",
       "1                0.9929                    0.9929     37.497823   \n",
       "2                0.9929                    0.9929     39.869996   \n",
       "3                0.9929                    0.9929     45.667251   \n",
       "4                0.9929                    0.9929     37.819797   \n",
       "5                0.9929                    0.9929     45.668900   \n",
       "6                0.9929                    0.9929     38.311485   \n",
       "7                0.9929                    0.9929     38.261693   \n",
       "8                0.9929                    0.9929     40.876326   \n",
       "9                0.9929                    0.9929     44.425177   \n",
       "\n",
       "   elapsed_time_inv  \n",
       "0         35.489802  \n",
       "1         35.823032  \n",
       "2         36.928449  \n",
       "3         41.919817  \n",
       "4         36.978772  \n",
       "5         38.729488  \n",
       "6         35.543103  \n",
       "7         37.811996  \n",
       "8         40.631029  \n",
       "9         45.021347  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pruning = pd.DataFrame({\n",
    "    'method': methods,\n",
    "    'split': splits,\n",
    "    'explanation': explanations,\n",
    "    'explanation_inv': explanations_inv,\n",
    "    'model_accuracy': model_accuracies,\n",
    "    'explanation_accuracy': explanation_accuracies,\n",
    "    'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "    'elapsed_time': elapsed_times,\n",
    "    'elapsed_time_inv': elapsed_times_inv,\n",
    "})\n",
    "results_pruning.to_csv(os.path.join(results_dir, 'results_pruning.csv'))\n",
    "results_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_pruning = False\n",
    "# method = 'lime'\n",
    "# methods = []\n",
    "# splits = []\n",
    "# explanations = []\n",
    "# explanations_inv = []\n",
    "# model_accuracies = []\n",
    "# explanation_accuracies = []\n",
    "# explanation_accuracies_inv = []\n",
    "# elapsed_times = []\n",
    "# elapsed_times_inv = []\n",
    "# for seed in range(n_rep):\n",
    "#     print(f'Seed [{seed+1}/{n_rep}]')\n",
    "    \n",
    "#     model = train_nn(x_train, y_train, need_pruning, seed, device)\n",
    "    \n",
    "#     y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "#     model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds.argmax(axis=1))\n",
    "#     print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "#     # positive class\n",
    "#     target_class = 1\n",
    "#     start = time.time()\n",
    "#     global_explanation, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "#                                                                        x_train.to(device), y_train.to(device), \n",
    "#                                                                        target_class=target_class,\n",
    "#                                                                        method=method, device=device)\n",
    "#     elapsed_time = time.time() - start\n",
    "#     if global_explanation:\n",
    "#         explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "#         explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "#     print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "#     print(f'\\t Elapsed time {elapsed_time}')\n",
    "        \n",
    "#     # negative class\n",
    "#     target_class = 0\n",
    "#     start = time.time()\n",
    "#     global_explanation_inv, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "#                                                                            x_train.to(device), y_train.to(device), \n",
    "#                                                                            target_class=target_class,\n",
    "#                                                                            method=method, device=device)\n",
    "#     elapsed_time_inv = time.time() - start\n",
    "#     if global_explanation_inv:\n",
    "#         explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, target_class, x_test, y_test)\n",
    "#         explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "#     print(f'\\t Class {target_class} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "#     print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "    \n",
    "#     methods.append(method)\n",
    "#     splits.append(seed)\n",
    "#     explanations.append(explanation)\n",
    "#     explanations_inv.append(explanation_inv)\n",
    "#     model_accuracies.append(model_accuracy)\n",
    "#     explanation_accuracies.append(explanation_accuracy)\n",
    "#     explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "#     elapsed_times.append(elapsed_time)\n",
    "#     elapsed_times_inv.append(elapsed_time_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_lime = pd.DataFrame({\n",
    "#     'method': methods,\n",
    "#     'split': splits,\n",
    "#     'explanation': explanations,\n",
    "#     'explanation_inv': explanations_inv,\n",
    "#     'model_accuracy': model_accuracies,\n",
    "#     'explanation_accuracy': explanation_accuracies,\n",
    "#     'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "#     'elapsed_time': elapsed_times,\n",
    "#     'elapsed_time_inv': elapsed_times_inv,\n",
    "# })\n",
    "# results_lime.to_csv(os.path.join(results_dir, 'results_lime.csv'))\n",
    "# results_lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed [1/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 92.64708042144775\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 157.65220427513123\n",
      "Seed [2/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h2 | (~h1 & ~h3 & ~h7 & ~h9)\" - Accuracy: 0.1656\n",
      "\t Elapsed time 179.99838256835938\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 150.28664684295654\n",
      "Seed [3/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 189.27468252182007\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 132.84589910507202\n",
      "Seed [4/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"h6 | ~h1\" - Accuracy: 0.6519\n",
      "\t Elapsed time 144.5922782421112\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 165.00939846038818\n",
      "Seed [5/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h7) | (~h1 & ~h3 & ~h5 & ~h9)\" - Accuracy: 0.3031\n",
      "\t Elapsed time 181.8785548210144\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 206.43106746673584\n",
      "Seed [6/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 217.8573887348175\n",
      "\t Class 0 - Global explanation: \"h3 | h7 | h9 | (~h0 & ~h2)\" - Accuracy: 0.7353\n",
      "\t Elapsed time 182.25601887702942\n",
      "Seed [7/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h3 & ~h6) | (~h1 & ~h3 & ~h7 & ~h9)\" - Accuracy: 0.3512\n",
      "\t Elapsed time 193.57076478004456\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 181.54654502868652\n",
      "Seed [8/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h3 & ~h7 & ~h9) | (~h4 & ~h5 & ~h6 & ~h9)\" - Accuracy: 0.3313\n",
      "\t Elapsed time 185.84072279930115\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 375.35427474975586\n",
      "Seed [9/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h2 & ~h4 & ~h6) | (~h1 & ~h3 & ~h7 & ~h9)\" - Accuracy: 0.2652\n",
      "\t Elapsed time 178.68566393852234\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 86.55202984809875\n",
      "Seed [10/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"~h1 & ~h3 & ~h7 & ~h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 77.79216718673706\n",
      "\t Class 0 - Global explanation: \"h1 | h3 | h7 | h9\" - Accuracy: 0.9929\n",
      "\t Elapsed time 75.93377256393433\n"
     ]
    }
   ],
   "source": [
    "need_pruning = False\n",
    "method = 'weights'\n",
    "methods = []\n",
    "splits = []\n",
    "explanations = []\n",
    "explanations_inv = []\n",
    "model_accuracies = []\n",
    "explanation_accuracies = []\n",
    "explanation_accuracies_inv = []\n",
    "elapsed_times = []\n",
    "elapsed_times_inv = []\n",
    "for seed in range(n_rep):\n",
    "    print(f'Seed [{seed+1}/{n_rep}]')\n",
    "    \n",
    "    model = train_nn(x_train, y_train, need_pruning, seed, device, relu=True)\n",
    "    \n",
    "    y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "    model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds.argmax(axis=1))\n",
    "    print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "    # positive class\n",
    "    target_class = 1\n",
    "    start = time.time()\n",
    "    global_explanation, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "                                                                       x_train.to(device), y_train.to(device),\n",
    "                                                                       topk_explanations=10, \n",
    "                                                                       target_class=target_class,\n",
    "                                                                       method=method, device=device)\n",
    "    elapsed_time = time.time() - start\n",
    "    if global_explanation:\n",
    "        explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "        explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time}')\n",
    "        \n",
    "    # negative class\n",
    "    target_class = 0\n",
    "    start = time.time()\n",
    "    global_explanation_inv, _, _ = logic.relunn.combine_local_explanations(model, \n",
    "                                                                           x_train.to(device), y_train.to(device), \n",
    "                                                                           topk_explanations=10, \n",
    "                                                                           target_class=target_class,\n",
    "                                                                           method=method, device=device)\n",
    "    elapsed_time_inv = time.time() - start\n",
    "    if global_explanation_inv:\n",
    "        explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, target_class, x_test, y_test)\n",
    "        explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "    \n",
    "    methods.append(method)\n",
    "    splits.append(seed)\n",
    "    explanations.append(explanation)\n",
    "    explanations_inv.append(explanation_inv)\n",
    "    model_accuracies.append(model_accuracy)\n",
    "    explanation_accuracies.append(explanation_accuracy)\n",
    "    explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "    elapsed_times.append(elapsed_time)\n",
    "    elapsed_times_inv.append(elapsed_time_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>explanation_inv</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_accuracy_inv</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>elapsed_time_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weights</td>\n",
       "      <td>0</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>92.647080</td>\n",
       "      <td>157.652204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weights</td>\n",
       "      <td>1</td>\n",
       "      <td>~h2 | (~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9)</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>179.998383</td>\n",
       "      <td>150.286647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weights</td>\n",
       "      <td>2</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>189.274683</td>\n",
       "      <td>132.845899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weights</td>\n",
       "      <td>3</td>\n",
       "      <td>h6 | ~h1</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>144.592278</td>\n",
       "      <td>165.009398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weights</td>\n",
       "      <td>4</td>\n",
       "      <td>(~h1 &amp; ~h7) | (~h1 &amp; ~h3 &amp; ~h5 &amp; ~h9)</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.3031</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>181.878555</td>\n",
       "      <td>206.431067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weights</td>\n",
       "      <td>5</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h3 | h7 | h9 | (~h0 &amp; ~h2)</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.7353</td>\n",
       "      <td>217.857389</td>\n",
       "      <td>182.256019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weights</td>\n",
       "      <td>6</td>\n",
       "      <td>(~h1 &amp; ~h3 &amp; ~h6) | (~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9)</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>193.570765</td>\n",
       "      <td>181.546545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weights</td>\n",
       "      <td>7</td>\n",
       "      <td>(~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9) | (~h4 &amp; ~h5 &amp; ~h6 &amp; ~h9)</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>185.840723</td>\n",
       "      <td>375.354275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weights</td>\n",
       "      <td>8</td>\n",
       "      <td>(~h2 &amp; ~h4 &amp; ~h6) | (~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9)</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>178.685664</td>\n",
       "      <td>86.552030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weights</td>\n",
       "      <td>9</td>\n",
       "      <td>~h1 &amp; ~h3 &amp; ~h7 &amp; ~h9</td>\n",
       "      <td>h1 | h3 | h7 | h9</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>77.792167</td>\n",
       "      <td>75.933773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    method  split                                        explanation  \\\n",
       "0  weights      0                              ~h1 & ~h3 & ~h7 & ~h9   \n",
       "1  weights      1                      ~h2 | (~h1 & ~h3 & ~h7 & ~h9)   \n",
       "2  weights      2                              ~h1 & ~h3 & ~h7 & ~h9   \n",
       "3  weights      3                                           h6 | ~h1   \n",
       "4  weights      4              (~h1 & ~h7) | (~h1 & ~h3 & ~h5 & ~h9)   \n",
       "5  weights      5                              ~h1 & ~h3 & ~h7 & ~h9   \n",
       "6  weights      6        (~h1 & ~h3 & ~h6) | (~h1 & ~h3 & ~h7 & ~h9)   \n",
       "7  weights      7  (~h1 & ~h3 & ~h7 & ~h9) | (~h4 & ~h5 & ~h6 & ~h9)   \n",
       "8  weights      8        (~h2 & ~h4 & ~h6) | (~h1 & ~h3 & ~h7 & ~h9)   \n",
       "9  weights      9                              ~h1 & ~h3 & ~h7 & ~h9   \n",
       "\n",
       "              explanation_inv  model_accuracy  explanation_accuracy  \\\n",
       "0           h1 | h3 | h7 | h9          0.9929                0.9929   \n",
       "1           h1 | h3 | h7 | h9          0.9929                0.1656   \n",
       "2           h1 | h3 | h7 | h9          0.9929                0.9929   \n",
       "3           h1 | h3 | h7 | h9          0.9929                0.6519   \n",
       "4           h1 | h3 | h7 | h9          0.9929                0.3031   \n",
       "5  h3 | h7 | h9 | (~h0 & ~h2)          0.9929                0.9929   \n",
       "6           h1 | h3 | h7 | h9          0.9929                0.3512   \n",
       "7           h1 | h3 | h7 | h9          0.9929                0.3313   \n",
       "8           h1 | h3 | h7 | h9          0.9929                0.2652   \n",
       "9           h1 | h3 | h7 | h9          0.9929                0.9929   \n",
       "\n",
       "   explanation_accuracy_inv  elapsed_time  elapsed_time_inv  \n",
       "0                    0.9929     92.647080        157.652204  \n",
       "1                    0.9929    179.998383        150.286647  \n",
       "2                    0.9929    189.274683        132.845899  \n",
       "3                    0.9929    144.592278        165.009398  \n",
       "4                    0.9929    181.878555        206.431067  \n",
       "5                    0.7353    217.857389        182.256019  \n",
       "6                    0.9929    193.570765        181.546545  \n",
       "7                    0.9929    185.840723        375.354275  \n",
       "8                    0.9929    178.685664         86.552030  \n",
       "9                    0.9929     77.792167         75.933773  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_weights = pd.DataFrame({\n",
    "    'method': methods,\n",
    "    'split': splits,\n",
    "    'explanation': explanations,\n",
    "    'explanation_inv': explanations_inv,\n",
    "    'model_accuracy': model_accuracies,\n",
    "    'explanation_accuracy': explanation_accuracies,\n",
    "    'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "    'elapsed_time': elapsed_times,\n",
    "    'elapsed_time_inv': elapsed_times_inv,\n",
    "})\n",
    "results_weights.to_csv(os.path.join(results_dir, 'results_weights.csv'))\n",
    "results_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_psi_nn(x_train, y_train, need_pruning, seed, device):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device).to(torch.float)\n",
    "    layers = [\n",
    "        torch.nn.Linear(x_train.size(1), 10),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(10, 4),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(4, 1),\n",
    "        torch.nn.Sigmoid(),\n",
    "    ]\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.BCELoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train).squeeze()\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.00001 * torch.norm(module.weight, 1)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            model = prune_equal_fanin(model, 2, validate=True, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0:\n",
    "            y_pred_d = y_pred > 0.5\n",
    "            accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "            print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed [1/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7861\n",
      "\t Epoch 3000: train accuracy: 0.7861\n",
      "\t Epoch 3500: train accuracy: 0.7861\n",
      "\t Epoch 4000: train accuracy: 0.7861\n",
      "\t Model's accuracy: 0.7877\n",
      "\t Class 1 - Global explanation: \"(h2 | h6 | (~h1 & ~h3))\" - Accuracy: 0.5654\n",
      "\t Elapsed time 0.050867319107055664\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.6030\n",
      "\t Epoch 3000: train accuracy: 0.6030\n",
      "\t Epoch 3500: train accuracy: 0.6030\n",
      "\t Epoch 4000: train accuracy: 0.6030\n",
      "\t Model's accuracy: 0.5987\n",
      "\t Class 0 - Global explanation: \"(~h4 & (h7 | ~h8))\" - Accuracy: 0.5629\n",
      "\t Elapsed time 0.03989291191101074\n",
      "Seed [2/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7191\n",
      "\t Epoch 3000: train accuracy: 0.7191\n",
      "\t Epoch 3500: train accuracy: 0.7861\n",
      "\t Epoch 4000: train accuracy: 0.7861\n",
      "\t Model's accuracy: 0.7877\n",
      "\t Class 1 - Global explanation: \"((h5 | h6) & (h5 | ~h3) & (h6 | ~h1) & (~h1 | ~h3))\" - Accuracy: 0.2123\n",
      "\t Elapsed time 0.048871517181396484\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7850\n",
      "\t Epoch 3000: train accuracy: 0.7850\n",
      "\t Epoch 3500: train accuracy: 0.7850\n",
      "\t Epoch 4000: train accuracy: 0.7850\n",
      "\t Model's accuracy: 0.7840\n",
      "\t Class 0 - Global explanation: \"(~h4 & (h1 | h7) & (h1 | ~h8))\" - Accuracy: 0.5629\n",
      "\t Elapsed time 0.040895938873291016\n",
      "Seed [3/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h3 & ~h7 & ~h9 & (h2 | h8 | ~h1))\" - Accuracy: 0.4869\n",
      "\t Elapsed time 0.0827791690826416\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7861\n",
      "\t Epoch 3000: train accuracy: 0.7861\n",
      "\t Epoch 3500: train accuracy: 0.7861\n",
      "\t Epoch 4000: train accuracy: 0.7861\n",
      "\t Model's accuracy: 0.7877\n",
      "\t Class 0 - Global explanation: \"(~h0 & (h1 | h3) & (h3 | ~h2) & (h3 | ~h8))\" - Accuracy: 0.5629\n",
      "\t Elapsed time 0.058843374252319336\n",
      "Seed [4/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.8458\n",
      "\t Epoch 3000: train accuracy: 0.8458\n",
      "\t Epoch 3500: train accuracy: 0.8458\n",
      "\t Epoch 4000: train accuracy: 0.8458\n",
      "\t Model's accuracy: 0.8414\n",
      "\t Class 1 - Global explanation: \"(h0 | h8 | (h2 & ~h3) | (h4 & ~h3 & ~h7))\" - Accuracy: 0.8414\n",
      "\t Elapsed time 0.12267255783081055\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.6417\n",
      "\t Epoch 3000: train accuracy: 0.6417\n",
      "\t Epoch 3500: train accuracy: 0.6854\n",
      "\t Epoch 4000: train accuracy: 0.6854\n",
      "\t Model's accuracy: 0.6829\n",
      "\t Class 0 - Global explanation: \"(~h0 & ~h4 & (h3 | h7) & (h7 | ~h8))\" - Accuracy: 0.5629\n",
      "\t Elapsed time 0.05085897445678711\n",
      "Seed [5/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.8685\n",
      "\t Epoch 3000: train accuracy: 0.8685\n",
      "\t Epoch 3500: train accuracy: 0.8685\n",
      "\t Epoch 4000: train accuracy: 0.8685\n",
      "\t Model's accuracy: 0.8694\n",
      "\t Class 1 - Global explanation: \"((h0 & h8) | (h2 & h8) | (h0 & ~h1) | (h2 & ~h1))\" - Accuracy: 0.7353\n",
      "\t Elapsed time 0.11024284362792969\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9271\n",
      "\t Epoch 3000: train accuracy: 0.9271\n",
      "\t Epoch 3500: train accuracy: 0.9271\n",
      "\t Epoch 4000: train accuracy: 0.9271\n",
      "\t Model's accuracy: 0.9209\n",
      "\t Class 0 - Global explanation: \"(h1 | h9 | (h7 & ~h2))\" - Accuracy: 0.9209\n",
      "\t Elapsed time 0.06619405746459961\n",
      "Seed [6/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.6906\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9271\n",
      "\t Epoch 3000: train accuracy: 0.9271\n",
      "\t Epoch 3500: train accuracy: 0.9271\n",
      "\t Epoch 4000: train accuracy: 0.9271\n",
      "\t Model's accuracy: 0.9209\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h7 & ~h9)\" - Accuracy: 0.9209\n",
      "\t Elapsed time 0.0630347728729248\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.5088\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7082\n",
      "\t Epoch 3000: train accuracy: 0.7082\n",
      "\t Epoch 3500: train accuracy: 0.7082\n",
      "\t Epoch 4000: train accuracy: 0.7082\n",
      "\t Model's accuracy: 0.7109\n",
      "\t Class 0 - Global explanation: \"(~h0 & ~h6 & ~h8)\" - Accuracy: 0.7109\n",
      "\t Elapsed time 0.026928186416625977\n",
      "Seed [7/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7220\n",
      "\t Epoch 3000: train accuracy: 0.8824\n",
      "\t Epoch 3500: train accuracy: 0.8824\n",
      "\t Epoch 4000: train accuracy: 0.8824\n",
      "\t Model's accuracy: 0.8868\n",
      "\t Class 1 - Global explanation: \"(h0 | h2 | h5 | h6)\" - Accuracy: 0.8868\n",
      "\t Elapsed time 0.05136919021606445\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.8521\n",
      "\t Epoch 3000: train accuracy: 0.8521\n",
      "\t Epoch 3500: train accuracy: 0.8521\n",
      "\t Epoch 4000: train accuracy: 0.8521\n",
      "\t Model's accuracy: 0.8560\n",
      "\t Class 0 - Global explanation: \"(h1 & ~h2 & (h3 | ~h4))\" - Accuracy: 0.5284\n",
      "\t Elapsed time 0.10023975372314453\n",
      "Seed [8/10]\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7947\n",
      "\t Epoch 3000: train accuracy: 0.7947\n",
      "\t Epoch 3500: train accuracy: 0.7947\n",
      "\t Epoch 4000: train accuracy: 0.8685\n",
      "\t Model's accuracy: 0.8694\n",
      "\t Class 1 - Global explanation: \"(h0 | h2 | h6)\" - Accuracy: 0.7991\n",
      "\t Elapsed time 0.0553433895111084\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.8521\n",
      "\t Epoch 3000: train accuracy: 0.8521\n",
      "\t Epoch 3500: train accuracy: 0.8521\n",
      "\t Epoch 4000: train accuracy: 0.8521\n",
      "\t Model's accuracy: 0.8560\n",
      "\t Class 0 - Global explanation: \"(h1 | h3 | h7)\" - Accuracy: 0.8560\n",
      "\t Elapsed time 0.057842254638671875\n",
      "Seed [9/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.9941\n",
      "\t Epoch 3000: train accuracy: 0.9941\n",
      "\t Epoch 3500: train accuracy: 0.9941\n",
      "\t Epoch 4000: train accuracy: 0.9941\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h3 & ~h9)\" - Accuracy: 0.9246\n",
      "\t Elapsed time 0.0329127311706543\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.7178\n",
      "\t Epoch 3000: train accuracy: 0.7178\n",
      "\t Epoch 3500: train accuracy: 0.7178\n",
      "\t Epoch 4000: train accuracy: 0.7178\n",
      "\t Model's accuracy: 0.7163\n",
      "\t Class 0 - Global explanation: \"(h3 | ~h0 | (h9 & ~h6))\" - Accuracy: 0.5768\n",
      "\t Elapsed time 0.06282711029052734\n",
      "Seed [10/10]\n",
      "\t Epoch 0: train accuracy: 0.4913\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.4913\n",
      "\t Epoch 3000: train accuracy: 0.6516\n",
      "\t Epoch 3500: train accuracy: 0.9282\n",
      "\t Epoch 4000: train accuracy: 0.9282\n",
      "\t Model's accuracy: 0.9246\n",
      "\t Class 1 - Global explanation: \"(~h1 & ~h3 & (h2 | ~h9))\" - Accuracy: 0.4858\n",
      "\t Elapsed time 0.05485272407531738\n",
      "\t Epoch 0: train accuracy: 0.5088\n",
      "\t Epoch 500: train accuracy: 0.9941\n",
      "\t Epoch 1000: train accuracy: 0.9941\n",
      "\t Epoch 1500: train accuracy: 0.9941\n",
      "\t Epoch 2000: train accuracy: 0.9941\n",
      "\t Epoch 2500: train accuracy: 0.5088\n",
      "\t Epoch 3000: train accuracy: 0.6417\n",
      "\t Epoch 3500: train accuracy: 0.8685\n",
      "\t Epoch 4000: train accuracy: 0.8685\n",
      "\t Model's accuracy: 0.8694\n",
      "\t Class 0 - Global explanation: \"(~h6)\" - Accuracy: 0.5564\n",
      "\t Elapsed time 0.04288458824157715\n"
     ]
    }
   ],
   "source": [
    "need_pruning = True\n",
    "method = 'psi'\n",
    "methods = []\n",
    "splits = []\n",
    "explanations = []\n",
    "explanations_inv = []\n",
    "model_accuracies = []\n",
    "explanation_accuracies = []\n",
    "explanation_accuracies_inv = []\n",
    "elapsed_times = []\n",
    "elapsed_times_inv = []\n",
    "for seed in range(n_rep):\n",
    "    print(f'Seed [{seed+1}/{n_rep}]')\n",
    "    \n",
    "    # positive class\n",
    "    target_class = 1\n",
    "    model = train_psi_nn(x_train, y_train, need_pruning, seed, device)\n",
    "    \n",
    "    y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "    model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds > 0.5)\n",
    "    print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "    start = time.time()\n",
    "    global_explanation = logic.generate_fol_explanations(model, device)[0]\n",
    "    elapsed_time = time.time() - start\n",
    "    explanation_accuracy, _ = logic.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "    explanation = logic.base.replace_names(global_explanation, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time}')\n",
    "        \n",
    "    # negative class\n",
    "    target_class = 0\n",
    "    model = train_psi_nn(x_train, y_train.eq(target_class), need_pruning, seed, device)\n",
    "    \n",
    "    y_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "    model_accuracy = accuracy_score(y_test.eq(target_class).cpu().detach().numpy(), y_preds > 0.5)\n",
    "    print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "    start = time.time()\n",
    "    global_explanation_inv = logic.generate_fol_explanations(model, device)[0]\n",
    "    elapsed_time_inv = time.time() - start\n",
    "    explanation_accuracy_inv, _ = logic.base.test_explanation(global_explanation_inv, \n",
    "                                                              target_class, x_test, y_test)\n",
    "    explanation_inv = logic.base.replace_names(global_explanation_inv, concepts)\n",
    "    print(f'\\t Class {target_class} - Global explanation: \"{explanation_inv}\" - Accuracy: {explanation_accuracy_inv:.4f}')\n",
    "    print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "    \n",
    "    methods.append(method)\n",
    "    splits.append(seed)\n",
    "    explanations.append(explanation)\n",
    "    explanations_inv.append(explanation_inv)\n",
    "    model_accuracies.append(model_accuracy)\n",
    "    explanation_accuracies.append(explanation_accuracy)\n",
    "    explanation_accuracies_inv.append(explanation_accuracy_inv)\n",
    "    elapsed_times.append(elapsed_time)\n",
    "    elapsed_times_inv.append(elapsed_time_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>explanation_inv</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_accuracy_inv</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>elapsed_time_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>psi</td>\n",
       "      <td>0</td>\n",
       "      <td>(h2 | h6 | (~h1 &amp; ~h3))</td>\n",
       "      <td>(~h4 &amp; (h7 | ~h8))</td>\n",
       "      <td>0.5987</td>\n",
       "      <td>0.5654</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>0.050867</td>\n",
       "      <td>0.039893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>psi</td>\n",
       "      <td>1</td>\n",
       "      <td>((h5 | h6) &amp; (h5 | ~h3) &amp; (h6 | ~h1) &amp; (~h1 | ...</td>\n",
       "      <td>(~h4 &amp; (h1 | h7) &amp; (h1 | ~h8))</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>0.2123</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>0.048872</td>\n",
       "      <td>0.040896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psi</td>\n",
       "      <td>2</td>\n",
       "      <td>(~h3 &amp; ~h7 &amp; ~h9 &amp; (h2 | h8 | ~h1))</td>\n",
       "      <td>(~h0 &amp; (h1 | h3) &amp; (h3 | ~h2) &amp; (h3 | ~h8))</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.4869</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>0.082779</td>\n",
       "      <td>0.058843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>psi</td>\n",
       "      <td>3</td>\n",
       "      <td>(h0 | h8 | (h2 &amp; ~h3) | (h4 &amp; ~h3 &amp; ~h7))</td>\n",
       "      <td>(~h0 &amp; ~h4 &amp; (h3 | h7) &amp; (h7 | ~h8))</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>0.122673</td>\n",
       "      <td>0.050859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>psi</td>\n",
       "      <td>4</td>\n",
       "      <td>((h0 &amp; h8) | (h2 &amp; h8) | (h0 &amp; ~h1) | (h2 &amp; ~h1))</td>\n",
       "      <td>(h1 | h9 | (h7 &amp; ~h2))</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.7353</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.066194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>psi</td>\n",
       "      <td>5</td>\n",
       "      <td>(~h1 &amp; ~h7 &amp; ~h9)</td>\n",
       "      <td>(~h0 &amp; ~h6 &amp; ~h8)</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>0.063035</td>\n",
       "      <td>0.026928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>psi</td>\n",
       "      <td>6</td>\n",
       "      <td>(h0 | h2 | h5 | h6)</td>\n",
       "      <td>(h1 &amp; ~h2 &amp; (h3 | ~h4))</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.051369</td>\n",
       "      <td>0.100240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>psi</td>\n",
       "      <td>7</td>\n",
       "      <td>(h0 | h2 | h6)</td>\n",
       "      <td>(h1 | h3 | h7)</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>0.7991</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>0.055343</td>\n",
       "      <td>0.057842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psi</td>\n",
       "      <td>8</td>\n",
       "      <td>(~h1 &amp; ~h3 &amp; ~h9)</td>\n",
       "      <td>(h3 | ~h0 | (h9 &amp; ~h6))</td>\n",
       "      <td>0.7163</td>\n",
       "      <td>0.9246</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.062827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>psi</td>\n",
       "      <td>9</td>\n",
       "      <td>(~h1 &amp; ~h3 &amp; (h2 | ~h9))</td>\n",
       "      <td>(~h6)</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.4858</td>\n",
       "      <td>0.5564</td>\n",
       "      <td>0.054853</td>\n",
       "      <td>0.042885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method  split                                        explanation  \\\n",
       "0    psi      0                            (h2 | h6 | (~h1 & ~h3))   \n",
       "1    psi      1  ((h5 | h6) & (h5 | ~h3) & (h6 | ~h1) & (~h1 | ...   \n",
       "2    psi      2                (~h3 & ~h7 & ~h9 & (h2 | h8 | ~h1))   \n",
       "3    psi      3          (h0 | h8 | (h2 & ~h3) | (h4 & ~h3 & ~h7))   \n",
       "4    psi      4  ((h0 & h8) | (h2 & h8) | (h0 & ~h1) | (h2 & ~h1))   \n",
       "5    psi      5                                  (~h1 & ~h7 & ~h9)   \n",
       "6    psi      6                                (h0 | h2 | h5 | h6)   \n",
       "7    psi      7                                     (h0 | h2 | h6)   \n",
       "8    psi      8                                  (~h1 & ~h3 & ~h9)   \n",
       "9    psi      9                           (~h1 & ~h3 & (h2 | ~h9))   \n",
       "\n",
       "                               explanation_inv  model_accuracy  \\\n",
       "0                           (~h4 & (h7 | ~h8))          0.5987   \n",
       "1               (~h4 & (h1 | h7) & (h1 | ~h8))          0.7840   \n",
       "2  (~h0 & (h1 | h3) & (h3 | ~h2) & (h3 | ~h8))          0.7877   \n",
       "3         (~h0 & ~h4 & (h3 | h7) & (h7 | ~h8))          0.6829   \n",
       "4                       (h1 | h9 | (h7 & ~h2))          0.9209   \n",
       "5                            (~h0 & ~h6 & ~h8)          0.7109   \n",
       "6                      (h1 & ~h2 & (h3 | ~h4))          0.8560   \n",
       "7                               (h1 | h3 | h7)          0.8560   \n",
       "8                      (h3 | ~h0 | (h9 & ~h6))          0.7163   \n",
       "9                                        (~h6)          0.8694   \n",
       "\n",
       "   explanation_accuracy  explanation_accuracy_inv  elapsed_time  \\\n",
       "0                0.5654                    0.5629      0.050867   \n",
       "1                0.2123                    0.5629      0.048872   \n",
       "2                0.4869                    0.5629      0.082779   \n",
       "3                0.8414                    0.5629      0.122673   \n",
       "4                0.7353                    0.9209      0.110243   \n",
       "5                0.9209                    0.7109      0.063035   \n",
       "6                0.8868                    0.5284      0.051369   \n",
       "7                0.7991                    0.8560      0.055343   \n",
       "8                0.9246                    0.5768      0.032913   \n",
       "9                0.4858                    0.5564      0.054853   \n",
       "\n",
       "   elapsed_time_inv  \n",
       "0          0.039893  \n",
       "1          0.040896  \n",
       "2          0.058843  \n",
       "3          0.050859  \n",
       "4          0.066194  \n",
       "5          0.026928  \n",
       "6          0.100240  \n",
       "7          0.057842  \n",
       "8          0.062827  \n",
       "9          0.042885  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_psi = pd.DataFrame({\n",
    "    'method': methods,\n",
    "    'split': splits,\n",
    "    'explanation': explanations,\n",
    "    'explanation_inv': explanations_inv,\n",
    "    'model_accuracy': model_accuracies,\n",
    "    'explanation_accuracy': explanation_accuracies,\n",
    "    'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "    'elapsed_time': elapsed_times,\n",
    "    'elapsed_time_inv': elapsed_times_inv,\n",
    "})\n",
    "results_psi.to_csv(os.path.join(results_dir, 'results_psi.csv'))\n",
    "results_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed [1/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.001005411148071289\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [2/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [3/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0009968280792236328\n",
      "Seed [4/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [5/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0009989738464355469\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [6/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [7/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [8/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [9/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "Seed [10/10]\n",
      "\t Model's accuracy: 0.9929\n",
      "\t Class 1 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 <= 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 <= 0.50 & h8 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 <= 0.50 & h2 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= 0.50 & h5 > 0.50)\n",
      "\t Elapsed time 0.0\n",
      "\t Class 0 - Global explanation: (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0.50) | (h1 <= 0.50 & h9 <= 0.50 & h3 > 0.50) | (h1 <= 0.50 & h9 > 0.50) | (h1 > 0.50)\n",
      "\t Elapsed time 0.0\n"
     ]
    }
   ],
   "source": [
    "need_pruning = False\n",
    "method = 'decision_tree'\n",
    "methods = []\n",
    "splits = []\n",
    "explanations = []\n",
    "explanations_inv = []\n",
    "model_accuracies = []\n",
    "explanation_accuracies = []\n",
    "explanation_accuracies_inv = []\n",
    "elapsed_times = []\n",
    "elapsed_times_inv = []\n",
    "for seed in range(n_rep):\n",
    "    print(f'Seed [{seed+1}/{n_rep}]')\n",
    "    \n",
    "    classifier = DecisionTreeClassifier(random_state=seed)\n",
    "    classifier.fit(x_train.cpu().detach().numpy(), y_train.cpu().detach().numpy())\n",
    "    y_preds = classifier.predict(x_test.cpu().detach().numpy())\n",
    "    model_accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds)\n",
    "    print(f'\\t Model\\'s accuracy: {model_accuracy:.4f}')\n",
    "    \n",
    "    target_class = 1\n",
    "    start = time.time()\n",
    "    explanation = tree_to_formula(classifier, concepts, target_class)\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f'\\t Class {target_class} - Global explanation: {explanation}')\n",
    "    print(f'\\t Elapsed time {elapsed_time}')\n",
    "    \n",
    "    target_class = 0\n",
    "    start = time.time()\n",
    "    explanation_inv = tree_to_formula(classifier, concepts, target_class)\n",
    "    elapsed_time_inv = time.time() - start\n",
    "    print(f'\\t Class {target_class} - Global explanation: {explanation_inv}')\n",
    "    print(f'\\t Elapsed time {elapsed_time_inv}')\n",
    "    \n",
    "    methods.append(method)\n",
    "    splits.append(seed)\n",
    "    explanations.append(explanation)\n",
    "    explanations_inv.append(explanation_inv)\n",
    "    model_accuracies.append(model_accuracy)\n",
    "    explanation_accuracies.append(model_accuracy)\n",
    "    explanation_accuracies_inv.append(model_accuracy)\n",
    "    elapsed_times.append(0)\n",
    "    elapsed_times_inv.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>explanation_inv</th>\n",
       "      <th>model_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_accuracy_inv</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>elapsed_time_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>1</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>2</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>3</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>4</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>5</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>6</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>7</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>8</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>9</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &lt;= ...</td>\n",
       "      <td>(h1 &lt;= 0.50 &amp; h9 &lt;= 0.50 &amp; h3 &lt;= 0.50 &amp; h7 &gt; 0...</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          method  split                                        explanation  \\\n",
       "0  decision_tree      0  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "1  decision_tree      1  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "2  decision_tree      2  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "3  decision_tree      3  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "4  decision_tree      4  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "5  decision_tree      5  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "6  decision_tree      6  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "7  decision_tree      7  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "8  decision_tree      8  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "9  decision_tree      9  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 <= ...   \n",
       "\n",
       "                                     explanation_inv  model_accuracy  \\\n",
       "0  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "1  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "2  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "3  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "4  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "5  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "6  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "7  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "8  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "9  (h1 <= 0.50 & h9 <= 0.50 & h3 <= 0.50 & h7 > 0...          0.9929   \n",
       "\n",
       "   explanation_accuracy  explanation_accuracy_inv  elapsed_time  \\\n",
       "0                0.9929                    0.9929             0   \n",
       "1                0.9929                    0.9929             0   \n",
       "2                0.9929                    0.9929             0   \n",
       "3                0.9929                    0.9929             0   \n",
       "4                0.9929                    0.9929             0   \n",
       "5                0.9929                    0.9929             0   \n",
       "6                0.9929                    0.9929             0   \n",
       "7                0.9929                    0.9929             0   \n",
       "8                0.9929                    0.9929             0   \n",
       "9                0.9929                    0.9929             0   \n",
       "\n",
       "   elapsed_time_inv  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "5                 0  \n",
       "6                 0  \n",
       "7                 0  \n",
       "8                 0  \n",
       "9                 0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tree = pd.DataFrame({\n",
    "    'method': methods,\n",
    "    'split': splits,\n",
    "    'explanation': explanations,\n",
    "    'explanation_inv': explanations_inv,\n",
    "    'model_accuracy': model_accuracies,\n",
    "    'explanation_accuracy': explanation_accuracies,\n",
    "    'explanation_accuracy_inv': explanation_accuracies_inv,\n",
    "    'elapsed_time': elapsed_times,\n",
    "    'elapsed_time_inv': elapsed_times_inv,\n",
    "})\n",
    "results_tree.to_csv(os.path.join(results_dir, 'results_tree.csv'))\n",
    "results_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_accuracy_mean</th>\n",
       "      <th>explanation_accuracy_mean</th>\n",
       "      <th>explanation_accuracy_inv_mean</th>\n",
       "      <th>elapsed_time_mean</th>\n",
       "      <th>elapsed_time_inv_mean</th>\n",
       "      <th>model_accuracy_sem</th>\n",
       "      <th>explanation_accuracy_sem</th>\n",
       "      <th>explanation_accuracy_inv_sem</th>\n",
       "      <th>elapsed_time_sem</th>\n",
       "      <th>elapsed_time_inv_sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pruning</th>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>40.475988</td>\n",
       "      <td>38.487683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.117943</td>\n",
       "      <td>0.994672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weights</th>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.60399</td>\n",
       "      <td>0.96714</td>\n",
       "      <td>164.213769</td>\n",
       "      <td>171.386786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112697</td>\n",
       "      <td>0.025760</td>\n",
       "      <td>14.368703</td>\n",
       "      <td>26.144967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psi</th>\n",
       "      <td>0.77828</td>\n",
       "      <td>0.68585</td>\n",
       "      <td>0.64010</td>\n",
       "      <td>0.067295</td>\n",
       "      <td>0.054741</td>\n",
       "      <td>0.031723</td>\n",
       "      <td>0.075311</td>\n",
       "      <td>0.044448</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.006355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_accuracy_mean  explanation_accuracy_mean  \\\n",
       "pruning              0.99290                    0.99290   \n",
       "weights              0.99290                    0.60399   \n",
       "psi                  0.77828                    0.68585   \n",
       "tree                 0.99290                    0.99290   \n",
       "\n",
       "         explanation_accuracy_inv_mean  elapsed_time_mean  \\\n",
       "pruning                        0.99290          40.475988   \n",
       "weights                        0.96714         164.213769   \n",
       "psi                            0.64010           0.067295   \n",
       "tree                           0.99290           0.000000   \n",
       "\n",
       "         elapsed_time_inv_mean  model_accuracy_sem  explanation_accuracy_sem  \\\n",
       "pruning              38.487683            0.000000                  0.000000   \n",
       "weights             171.386786            0.000000                  0.112697   \n",
       "psi                   0.054741            0.031723                  0.075311   \n",
       "tree                  0.000000            0.000000                  0.000000   \n",
       "\n",
       "         explanation_accuracy_inv_sem  elapsed_time_sem  elapsed_time_inv_sem  \n",
       "pruning                      0.000000          1.117943              0.994672  \n",
       "weights                      0.025760         14.368703             26.144967  \n",
       "psi                          0.044448          0.009136              0.006355  \n",
       "tree                         0.000000          0.000000              0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['model_accuracy', 'explanation_accuracy', 'explanation_accuracy_inv', 'elapsed_time', 'elapsed_time_inv']\n",
    "mean_cols = [f'{c}_mean' for c in cols]\n",
    "sem_cols = [f'{c}_sem' for c in cols]\n",
    "\n",
    "# pruning\n",
    "df_mean = results_pruning[cols].mean()\n",
    "df_sem = results_pruning[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_pruning = pd.concat([df_mean, df_sem])\n",
    "summary_pruning.name = 'pruning'\n",
    "\n",
    "# # lime\n",
    "# df_mean = results_lime[cols].mean()\n",
    "# df_sem = results_lime[cols].sem()\n",
    "# df_mean.columns = mean_cols\n",
    "# df_sem.columns = sem_cols\n",
    "# summary_lime = pd.concat([df_mean, df_sem])\n",
    "# summary_lime.name = 'lime'\n",
    "\n",
    "# weights\n",
    "df_mean = results_weights[cols].mean()\n",
    "df_sem = results_weights[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_weights = pd.concat([df_mean, df_sem])\n",
    "summary_weights.name = 'weights'\n",
    "\n",
    "# psi\n",
    "df_mean = results_psi[cols].mean()\n",
    "df_sem = results_psi[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_psi = pd.concat([df_mean, df_sem])\n",
    "summary_psi.name = 'psi'\n",
    "\n",
    "# tree\n",
    "df_mean = results_tree[cols].mean()\n",
    "df_sem = results_tree[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_tree = pd.concat([df_mean, df_sem])\n",
    "summary_tree.name = 'tree'\n",
    "\n",
    "summary = pd.concat([summary_pruning, \n",
    "#                      summary_lime, \n",
    "                     summary_weights, summary_psi, summary_tree], axis=1).T\n",
    "summary.columns = mean_cols + sem_cols\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(os.path.join(results_dir, 'summary.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
