{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sympy import simplify_logic\n",
    "import re\n",
    "\n",
    "import deep_logic as dl\n",
    "from deep_logic.utils.base import validate_network, set_seed, tree_to_formula\n",
    "from deep_logic.utils.relu_nn import get_reduced_model, prune_features\n",
    "from deep_logic.utils.psi_nn import prune_equal_fanin\n",
    "from deep_logic.models.brl import XBRLClassifier\n",
    "from deep_logic import logic\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "#%%\n",
    "\n",
    "data = pd.read_csv('data/vdem/V-Dem-CY-Core-v10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressing-telescope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Mexico_1789\n",
       "1                   Mexico_1790\n",
       "2                   Mexico_1791\n",
       "3                   Mexico_1792\n",
       "4                   Mexico_1793\n",
       "                  ...          \n",
       "27008    Piedmont-Sardinia_1857\n",
       "27009    Piedmont-Sardinia_1858\n",
       "27010    Piedmont-Sardinia_1859\n",
       "27011    Piedmont-Sardinia_1860\n",
       "27012    Piedmont-Sardinia_1861\n",
       "Name: country_name_year, Length: 27013, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['country_name_year'] = data['country_name'] + '_' + data['year'].astype(str)\n",
    "data['country_name_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operational-mercury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2x_polyarchy</th>\n",
       "      <th>v2x_polyarchy_codelow</th>\n",
       "      <th>v2x_polyarchy_codehigh</th>\n",
       "      <th>v2x_polyarchy_sd</th>\n",
       "      <th>v2x_delibdem</th>\n",
       "      <th>v2x_delibdem_codelow</th>\n",
       "      <th>v2x_delibdem_codehigh</th>\n",
       "      <th>v2x_delibdem_sd</th>\n",
       "      <th>v2x_egaldem</th>\n",
       "      <th>v2x_egaldem_codelow</th>\n",
       "      <th>...</th>\n",
       "      <th>v2xcl_slave_sd</th>\n",
       "      <th>v2xel_elecparl</th>\n",
       "      <th>v2xel_elecpres</th>\n",
       "      <th>v2xex_elecleg</th>\n",
       "      <th>v2xlg_leginter</th>\n",
       "      <th>v2xme_altinf</th>\n",
       "      <th>v2xme_altinf_codelow</th>\n",
       "      <th>v2xme_altinf_codehigh</th>\n",
       "      <th>v2xme_altinf_sd</th>\n",
       "      <th>v2x_divparctrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.698</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.653</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.711</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25618</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.633</td>\n",
       "      <td>1.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25619</th>\n",
       "      <td>0.267</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25620</th>\n",
       "      <td>0.260</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25621</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25622</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3385 rows × 1052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2x_polyarchy  v2x_polyarchy_codelow  v2x_polyarchy_codehigh  \\\n",
       "212            0.698                  0.632                   0.763   \n",
       "213            0.711                  0.667                   0.778   \n",
       "214            0.715                  0.668                   0.762   \n",
       "215            0.720                  0.668                   0.746   \n",
       "216            0.720                  0.668                   0.746   \n",
       "...              ...                    ...                     ...   \n",
       "25618          0.316                  0.283                   0.348   \n",
       "25619          0.267                  0.236                   0.286   \n",
       "25620          0.260                  0.236                   0.282   \n",
       "25621          0.262                  0.245                   0.284   \n",
       "25622          0.245                  0.209                   0.265   \n",
       "\n",
       "       v2x_polyarchy_sd  v2x_delibdem  v2x_delibdem_codelow  \\\n",
       "212               0.067         0.539                 0.467   \n",
       "213               0.055         0.543                 0.475   \n",
       "214               0.047         0.554                 0.497   \n",
       "215               0.039         0.566                 0.519   \n",
       "216               0.039         0.585                 0.522   \n",
       "...                 ...           ...                   ...   \n",
       "25618             0.035         0.215                 0.173   \n",
       "25619             0.028         0.194                 0.152   \n",
       "25620             0.025         0.168                 0.137   \n",
       "25621             0.020         0.213                 0.181   \n",
       "25622             0.029         0.183                 0.133   \n",
       "\n",
       "       v2x_delibdem_codehigh  v2x_delibdem_sd  v2x_egaldem  \\\n",
       "212                    0.626            0.080        0.411   \n",
       "213                    0.608            0.064        0.418   \n",
       "214                    0.606            0.053        0.418   \n",
       "215                    0.605            0.043        0.423   \n",
       "216                    0.615            0.047        0.423   \n",
       "...                      ...              ...          ...   \n",
       "25618                  0.278            0.052        0.255   \n",
       "25619                  0.233            0.041        0.234   \n",
       "25620                  0.208            0.034        0.213   \n",
       "25621                  0.264            0.041        0.214   \n",
       "25622                  0.224            0.044        0.187   \n",
       "\n",
       "       v2x_egaldem_codelow  ...  v2xcl_slave_sd  v2xel_elecparl  \\\n",
       "212                  0.349  ...           0.556               0   \n",
       "213                  0.359  ...           0.556               0   \n",
       "214                  0.378  ...           0.556               1   \n",
       "215                  0.386  ...           0.556               0   \n",
       "216                  0.386  ...           0.556               0   \n",
       "...                    ...  ...             ...             ...   \n",
       "25618                0.214  ...           0.592               1   \n",
       "25619                0.207  ...           0.592               1   \n",
       "25620                0.188  ...           0.599               0   \n",
       "25621                0.192  ...           0.608               0   \n",
       "25622                0.160  ...           0.609               0   \n",
       "\n",
       "       v2xel_elecpres  v2xex_elecleg  v2xlg_leginter  v2xme_altinf  \\\n",
       "212                 0           1.00             0.0         0.767   \n",
       "213                 0           1.00             0.0         0.808   \n",
       "214                 0           1.00             0.0         0.808   \n",
       "215                 0           1.00             0.0         0.808   \n",
       "216                 0           1.00             0.0         0.808   \n",
       "...               ...            ...             ...           ...   \n",
       "25618               1           0.85             0.0         0.516   \n",
       "25619               1           0.85             0.0         0.516   \n",
       "25620               0           0.85             0.0         0.437   \n",
       "25621               0           0.85             0.0         0.502   \n",
       "25622               0           0.85             0.0         0.525   \n",
       "\n",
       "       v2xme_altinf_codelow  v2xme_altinf_codehigh  v2xme_altinf_sd  \\\n",
       "212                   0.615                  0.855            0.653   \n",
       "213                   0.683                  0.887            0.649   \n",
       "214                   0.683                  0.887            0.649   \n",
       "215                   0.683                  0.887            0.649   \n",
       "216                   0.683                  0.887            0.649   \n",
       "...                     ...                    ...              ...   \n",
       "25618                 0.381                  0.627            0.633   \n",
       "25619                 0.381                  0.627            0.633   \n",
       "25620                 0.321                  0.579            0.628   \n",
       "25621                 0.374                  0.620            0.625   \n",
       "25622                 0.378                  0.663            0.640   \n",
       "\n",
       "       v2x_divparctrl  \n",
       "212             1.616  \n",
       "213             1.616  \n",
       "214             1.616  \n",
       "215             1.616  \n",
       "216             1.616  \n",
       "...               ...  \n",
       "25618           1.602  \n",
       "25619          -0.684  \n",
       "25620          -0.448  \n",
       "25621          -0.586  \n",
       "25622          -0.586  \n",
       "\n",
       "[3385 rows x 1052 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2000 = data[data['year']>2000].iloc[:, 12:-1].dropna(axis=1)\n",
    "data_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "furnished-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main 3 - Area 14 - Raw 62\n"
     ]
    }
   ],
   "source": [
    "high_level_indicators = [\n",
    "    'v2x_polyarchy',\n",
    "    #'v2x_libdem',\n",
    "    #'v2x_partipdem',\n",
    "    'v2x_delibdem',\n",
    "    'v2x_egaldem'\n",
    "]\n",
    "mid_level_indicators = [\n",
    "    'v2x_api',\n",
    "    'v2x_mpi',\n",
    "    'v2x_freexp_altinf',\n",
    "    'v2x_frassoc_thick',\n",
    "    'v2x_suffr',\n",
    "    'v2xel_frefair',\n",
    "    'v2x_elecoff',\n",
    "    #'v2x_liberal',\n",
    "    'v2xcl_rol',\n",
    "    #'v2x_jucon',\n",
    "    #'v2xlg_legcon',\n",
    "    #'v2x_partip',\n",
    "    'v2x_cspart',\n",
    "    #'v2xdd_dd',\n",
    "    #'v2xel_locelec',\n",
    "    #'v2xel_regelec',\n",
    "    'v2xdl_delib',\n",
    "    'v2x_egal',\n",
    "    'v2xeg_eqprotec',\n",
    "    'v2xeg_eqaccess',\n",
    "    'v2xeg_eqdr',\n",
    "]\n",
    "\n",
    "drop_list = ['codelow', 'codehigh', 'sd', 'osp', 'nr', 'mean']\n",
    "low_level_indicators = []\n",
    "for f in data_2000.columns:\n",
    "    if f.endswith('_ord') and f not in high_level_indicators and f not in mid_level_indicators:\n",
    "        low_level_indicators.append(f)\n",
    "\n",
    "\n",
    "low_level_indicators_continuous = []\n",
    "for f in data_2000.columns:\n",
    "    if f.endswith('_codehigh') or f.endswith('_codelow') and f not in high_level_indicators and f not in mid_level_indicators:\n",
    "        low_level_indicators_continuous.append(f)\n",
    "\n",
    "print(f'Main {len(high_level_indicators)} - Area {len(mid_level_indicators)} - Raw {len(low_level_indicators)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "growing-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3385, 464)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_low_continuous = data_2000[low_level_indicators_continuous]\n",
    "data_low_continuous.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-synthetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2psbars_ord_0</th>\n",
       "      <th>v2psbars_ord_1</th>\n",
       "      <th>v2psbars_ord_2</th>\n",
       "      <th>v2psbars_ord_3</th>\n",
       "      <th>v2psparban_ord_0</th>\n",
       "      <th>v2psparban_ord_1</th>\n",
       "      <th>v2psparban_ord_2</th>\n",
       "      <th>v2psparban_ord_3</th>\n",
       "      <th>v2psorgs_ord_0</th>\n",
       "      <th>v2psorgs_ord_1</th>\n",
       "      <th>...</th>\n",
       "      <th>v2pepwrgen_ord_1</th>\n",
       "      <th>v2pepwrgen_ord_2</th>\n",
       "      <th>v2peedueq_ord_0</th>\n",
       "      <th>v2peedueq_ord_1</th>\n",
       "      <th>v2peedueq_ord_2</th>\n",
       "      <th>v2peedueq_ord_3</th>\n",
       "      <th>v2pehealth_ord_0</th>\n",
       "      <th>v2pehealth_ord_1</th>\n",
       "      <th>v2pehealth_ord_2</th>\n",
       "      <th>v2pehealth_ord_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "      <td>3385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.056425</td>\n",
       "      <td>0.037518</td>\n",
       "      <td>0.080059</td>\n",
       "      <td>0.825997</td>\n",
       "      <td>0.053471</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.028951</td>\n",
       "      <td>0.887740</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.150960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230133</td>\n",
       "      <td>0.730871</td>\n",
       "      <td>0.059970</td>\n",
       "      <td>0.312851</td>\n",
       "      <td>0.132939</td>\n",
       "      <td>0.494239</td>\n",
       "      <td>0.038109</td>\n",
       "      <td>0.322304</td>\n",
       "      <td>0.145643</td>\n",
       "      <td>0.493944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.230775</td>\n",
       "      <td>0.190057</td>\n",
       "      <td>0.271425</td>\n",
       "      <td>0.379168</td>\n",
       "      <td>0.225004</td>\n",
       "      <td>0.170164</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>0.315733</td>\n",
       "      <td>0.205165</td>\n",
       "      <td>0.358063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420980</td>\n",
       "      <td>0.443572</td>\n",
       "      <td>0.237467</td>\n",
       "      <td>0.463723</td>\n",
       "      <td>0.339559</td>\n",
       "      <td>0.500041</td>\n",
       "      <td>0.191488</td>\n",
       "      <td>0.467428</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.500037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2psbars_ord_0  v2psbars_ord_1  v2psbars_ord_2  v2psbars_ord_3  \\\n",
       "count     3385.000000     3385.000000     3385.000000     3385.000000   \n",
       "mean         0.056425        0.037518        0.080059        0.825997   \n",
       "std          0.230775        0.190057        0.271425        0.379168   \n",
       "min          0.000000        0.000000        0.000000        0.000000   \n",
       "25%          0.000000        0.000000        0.000000        1.000000   \n",
       "50%          0.000000        0.000000        0.000000        1.000000   \n",
       "75%          0.000000        0.000000        0.000000        1.000000   \n",
       "max          1.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       v2psparban_ord_0  v2psparban_ord_1  v2psparban_ord_2  v2psparban_ord_3  \\\n",
       "count       3385.000000       3385.000000       3385.000000       3385.000000   \n",
       "mean           0.053471          0.029838          0.028951          0.887740   \n",
       "std            0.225004          0.170164          0.167694          0.315733   \n",
       "min            0.000000          0.000000          0.000000          0.000000   \n",
       "25%            0.000000          0.000000          0.000000          1.000000   \n",
       "50%            0.000000          0.000000          0.000000          1.000000   \n",
       "75%            0.000000          0.000000          0.000000          1.000000   \n",
       "max            1.000000          1.000000          1.000000          1.000000   \n",
       "\n",
       "       v2psorgs_ord_0  v2psorgs_ord_1  ...  v2pepwrgen_ord_1  \\\n",
       "count     3385.000000     3385.000000  ...       3385.000000   \n",
       "mean         0.044018        0.150960  ...          0.230133   \n",
       "std          0.205165        0.358063  ...          0.420980   \n",
       "min          0.000000        0.000000  ...          0.000000   \n",
       "25%          0.000000        0.000000  ...          0.000000   \n",
       "50%          0.000000        0.000000  ...          0.000000   \n",
       "75%          0.000000        0.000000  ...          0.000000   \n",
       "max          1.000000        1.000000  ...          1.000000   \n",
       "\n",
       "       v2pepwrgen_ord_2  v2peedueq_ord_0  v2peedueq_ord_1  v2peedueq_ord_2  \\\n",
       "count       3385.000000      3385.000000      3385.000000      3385.000000   \n",
       "mean           0.730871         0.059970         0.312851         0.132939   \n",
       "std            0.443572         0.237467         0.463723         0.339559   \n",
       "min            0.000000         0.000000         0.000000         0.000000   \n",
       "25%            0.000000         0.000000         0.000000         0.000000   \n",
       "50%            1.000000         0.000000         0.000000         0.000000   \n",
       "75%            1.000000         0.000000         1.000000         0.000000   \n",
       "max            1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       v2peedueq_ord_3  v2pehealth_ord_0  v2pehealth_ord_1  v2pehealth_ord_2  \\\n",
       "count      3385.000000       3385.000000       3385.000000       3385.000000   \n",
       "mean          0.494239          0.038109          0.322304          0.145643   \n",
       "std           0.500041          0.191488          0.467428          0.352800   \n",
       "min           0.000000          0.000000          0.000000          0.000000   \n",
       "25%           0.000000          0.000000          0.000000          0.000000   \n",
       "50%           0.000000          0.000000          0.000000          0.000000   \n",
       "75%           1.000000          0.000000          1.000000          0.000000   \n",
       "max           1.000000          1.000000          1.000000          1.000000   \n",
       "\n",
       "       v2pehealth_ord_3  \n",
       "count       3385.000000  \n",
       "mean           0.493944  \n",
       "std            0.500037  \n",
       "min            0.000000  \n",
       "25%            0.000000  \n",
       "50%            0.000000  \n",
       "75%            1.000000  \n",
       "max            1.000000  \n",
       "\n",
       "[8 rows x 241 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_low_raw = data_2000[low_level_indicators]\n",
    "one_hots = []\n",
    "for indicator in low_level_indicators:\n",
    "    c = data_low_raw[indicator].values\n",
    "    n_bins = int(c.max())\n",
    "    kbin = KBinsDiscretizer(n_bins=n_bins, encode='onehot-dense', strategy='uniform')\n",
    "    c1h = kbin.fit_transform(c.reshape(-1, 1))\n",
    "    one_hots.append(c1h)\n",
    "\n",
    "new_indicator_names = []\n",
    "for clist, cname in zip(one_hots, low_level_indicators):\n",
    "    if clist.shape[1] > 1:\n",
    "        for i in range(clist.shape[1]):\n",
    "            new_indicator_names.append(f'{cname}_{i}')\n",
    "    else:\n",
    "        new_indicator_names.append(f'{cname}')\n",
    "\n",
    "data_low = pd.DataFrame(np.hstack(one_hots), columns=new_indicator_names)\n",
    "data_low.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifteen-concept",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2x_api</th>\n",
       "      <th>v2x_mpi</th>\n",
       "      <th>v2x_freexp_altinf</th>\n",
       "      <th>v2x_frassoc_thick</th>\n",
       "      <th>v2x_suffr</th>\n",
       "      <th>v2xel_frefair</th>\n",
       "      <th>v2x_elecoff</th>\n",
       "      <th>v2xcl_rol</th>\n",
       "      <th>v2x_cspart</th>\n",
       "      <th>v2xdl_delib</th>\n",
       "      <th>v2x_egal</th>\n",
       "      <th>v2xeg_eqprotec</th>\n",
       "      <th>v2xeg_eqaccess</th>\n",
       "      <th>v2xeg_eqdr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "      <td>3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2696</td>\n",
       "      <td>2196</td>\n",
       "      <td>2605</td>\n",
       "      <td>2622</td>\n",
       "      <td>3322</td>\n",
       "      <td>1909</td>\n",
       "      <td>2934</td>\n",
       "      <td>2607</td>\n",
       "      <td>2669</td>\n",
       "      <td>2464</td>\n",
       "      <td>2398</td>\n",
       "      <td>2527</td>\n",
       "      <td>2392</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2x_api v2x_mpi v2x_freexp_altinf v2x_frassoc_thick v2x_suffr  \\\n",
       "count     3385    3385              3385              3385      3385   \n",
       "unique       2       2                 2                 2         2   \n",
       "top       True   False              True              True      True   \n",
       "freq      2696    2196              2605              2622      3322   \n",
       "\n",
       "       v2xel_frefair v2x_elecoff v2xcl_rol v2x_cspart v2xdl_delib v2x_egal  \\\n",
       "count           3385        3385      3385       3385        3385     3385   \n",
       "unique             2           2         2          2           2        2   \n",
       "top             True        True      True       True        True     True   \n",
       "freq            1909        2934      2607       2669        2464     2398   \n",
       "\n",
       "       v2xeg_eqprotec v2xeg_eqaccess v2xeg_eqdr  \n",
       "count            3385           3385       3385  \n",
       "unique              2              2          2  \n",
       "top              True           True       True  \n",
       "freq             2527           2392       2073  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mid = data_2000[mid_level_indicators] > 0.5\n",
    "data_mid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adverse-conditions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3385\n",
       "unique       2\n",
       "top       True\n",
       "freq      1799\n",
       "Name: v2x_polyarchy, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_high = data_2000[high_level_indicators].iloc[:, 0] > 0.5\n",
    "data_high.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "undefined-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3385, 464])\n",
      "torch.Size([3385, 241])\n",
      "torch.Size([3385, 14])\n",
      "torch.Size([3385])\n"
     ]
    }
   ],
   "source": [
    "c0c = torch.FloatTensor(data_low_continuous.values)\n",
    "c0 = torch.FloatTensor(data_low.values)\n",
    "c1 = torch.FloatTensor(data_mid.values)\n",
    "c2 = torch.FloatTensor(data_high.values)\n",
    "print(c0c.shape)\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-immunology",
   "metadata": {},
   "source": [
    "# Low-to-mid level explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "front-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v2psbars_ord_0', 'v2psbars_ord_1', 'v2psbars_ord_2', 'v2psbars_ord_3', 'v2psparban_ord_0']\n",
      "['v2x_api', 'v2x_mpi', 'v2x_freexp_altinf', 'v2x_frassoc_thick', 'v2x_suffr']\n"
     ]
    }
   ],
   "source": [
    "results_dir = 'results/vdem_low2mid'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mid_concepts = list(data_mid.columns)\n",
    "low_concepts = list(data_low.columns)\n",
    "n_rep = 10\n",
    "tot_epochs = 6001\n",
    "prune_epochs = 3001\n",
    "\n",
    "n_splits = 10\n",
    "seed = 42\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "x = c0\n",
    "xh = c1\n",
    "y = c2\n",
    "\n",
    "print(low_concepts[:5])\n",
    "print(mid_concepts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-glossary",
   "metadata": {},
   "source": [
    "## Train loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "massive-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(x_train, y_train, need_pruning, seed, device, level, l1=0.001, lr=0.001, relu=False, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    \n",
    "    if level == 'mid':\n",
    "        layers = [\n",
    "            torch.nn.Linear(x_train.size(1), 100),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(50, 30),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(30, 10),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(10, y_train.size(1)),\n",
    "            torch.nn.Sigmoid(),\n",
    "        ]\n",
    "        loss_form = torch.nn.BCELoss()\n",
    "    else:\n",
    "        layers = [\n",
    "            torch.nn.Linear(x_train.size(1), 100),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(50, 30),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(30, 10),\n",
    "            torch.nn.ReLU() if relu else torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        ]\n",
    "        loss_form = torch.nn.BCELoss()\n",
    "            \n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train).squeeze()\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "#                 if level == 'mid':\n",
    "#                     loss += 0.0001 * torch.norm(module.weight, 1)\n",
    "#                     loss += 0.0001 * torch.norm(module.bias, 1)\n",
    "                if level == 'high':\n",
    "                    loss += l1 * torch.norm(module.weight, 1)\n",
    "                    loss += l1 * torch.norm(module.bias, 1)\n",
    "                break\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == prune_epochs and need_pruning and level == 'high':\n",
    "            prune_features(model, n_classes=1, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            if level == 'mid':\n",
    "                y_pred_d = y_pred > 0.5\n",
    "                accuracy = y_pred_d.eq(y_train).sum().item() / (y_train.size(0) * y_train.size(1))\n",
    "                print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "            else:\n",
    "                y_pred_d = y_pred > 0.5\n",
    "                accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "                print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "#             if level == 'mid':\n",
    "#                 y_pred_d = y_pred > 0.5\n",
    "#                 accuracy = y_pred_d.eq(y_train).sum().item() / (y_train.size(0) * y_train.size(1))\n",
    "#                 print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "#             else:\n",
    "#                 y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "#                 accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "#                 print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "systematic-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_psi_nn(x_train, y_train, need_pruning, seed, device, level, verbose=False):\n",
    "    set_seed(seed)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device).to(torch.float)\n",
    "\n",
    "    if level == 'mid':\n",
    "        layers = [\n",
    "            torch.nn.Linear(x_train.size(1), 50),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(50, 30),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(30, y_train.size(1)),\n",
    "            torch.nn.Sigmoid(),\n",
    "        ]\n",
    "    else:\n",
    "        layers = [\n",
    "            torch.nn.Linear(x_train.size(1), 10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(10, 4),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(4, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        ]\n",
    "            \n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_form = torch.nn.BCELoss()\n",
    "    model.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train).squeeze()\n",
    "        # Compute Loss\n",
    "        loss = loss_form(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.00001 * torch.norm(module.weight, 1)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > prune_epochs and need_pruning:\n",
    "            model = prune_equal_fanin(model, 2, validate=True, device=device)\n",
    "            need_pruning = False\n",
    "            \n",
    "        # compute accuracy\n",
    "        if epoch % 500 == 0 and verbose:\n",
    "            if level == 'mid':\n",
    "                y_pred_d = y_pred > 0.5\n",
    "                accuracy = y_pred_d.eq(y_train).sum().item() / (y_train.size(0) * y_train.size(1))\n",
    "                print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "            else:\n",
    "                y_pred_d = y_pred > 0.5\n",
    "                accuracy = y_pred_d.eq(y_train).sum().item() / y_train.size(0)\n",
    "                print(f'\\t Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "assisted-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_to_y(method, need_pruning, relu, l1=0.0001, lr=0.0001, verbose=False):\n",
    "    methods = []\n",
    "    splits = []\n",
    "    explanations = []\n",
    "    explanations_inv = []\n",
    "    model_accuracies_mid = []\n",
    "    model_accuracies_top = []\n",
    "    explanation_accuracies = []\n",
    "    explanation_accuracies_inv = []\n",
    "    elapsed_times = []\n",
    "    elapsed_times_inv = []\n",
    "    explanation_fidelities = []\n",
    "    explanation_complexities = []\n",
    "    for split, (train_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(), y.cpu().detach().numpy())):\n",
    "        print(f'Split [{split+1}/{n_splits}]')\n",
    "        x_train, x_test = torch.FloatTensor(x[train_index]), torch.FloatTensor(x[test_index])\n",
    "        xh_train, xh_test = torch.FloatTensor(xh[train_index]), torch.FloatTensor(xh[test_index])\n",
    "        y_train, y_test = torch.FloatTensor(y[train_index]), torch.FloatTensor(y[test_index])\n",
    "    \n",
    "        explanation, explanation_inv = '', ''\n",
    "        explanation_accuracy, explanation_accuracy_inv = 0, 0\n",
    "        elapsed_time, elapsed_time_inv = 0, 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if method == 'tree':\n",
    "            classifier = DecisionTreeRegressor(random_state=seed)\n",
    "            classifier.fit(x_train.cpu().detach().numpy(), xh_train.cpu().detach().numpy())\n",
    "            xh_train_preds = classifier.predict(x_train.cpu().detach().numpy())\n",
    "            xh_test_preds = classifier.predict(x_test.cpu().detach().numpy())\n",
    "            \n",
    "            classifier_h = DecisionTreeClassifier(random_state=seed)\n",
    "            classifier_h.fit(xh_train_preds, y_train)\n",
    "            y_preds = classifier_h.predict(xh_test)\n",
    "            \n",
    "            model_accuracy_mid = accuracy_score(xh_test.cpu().detach().numpy(), xh_test_preds>0.5)\n",
    "            model_accuracy_top = accuracy_score(y_test.cpu().detach().numpy(), y_preds)\n",
    "\n",
    "            target_class = 1\n",
    "            explanation = tree_to_formula(classifier_h, mid_concepts, target_class)\n",
    "            \n",
    "            explanation_accuracy = model_accuracy_top\n",
    "            explanation_fidelity = accuracy_score(y_preds, y_preds)\n",
    "            explanation_complexity = dl.logic.complexity(explanation)\n",
    "        \n",
    "        elif method == 'brl':\n",
    "#             level = 'mid'\n",
    "#             model = train_nn(x_train, xh_train, need_pruning=False, seed=seed, device=device, \n",
    "#                              level=level, relu=False, verbose=verbose)\n",
    "#             xh_train_preds = model(x_train.to(device)).cpu().detach().numpy()\n",
    "#             xh_test_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "\n",
    "            classifier = DecisionTreeRegressor(random_state=seed)\n",
    "            classifier.fit(x_train.cpu().detach().numpy(), xh_train.cpu().detach().numpy())\n",
    "            xh_train_preds = classifier.predict(x_train.cpu().detach().numpy())\n",
    "            xh_test_preds = classifier.predict(x_test.cpu().detach().numpy())\n",
    "            \n",
    "            xh_train_preds = torch.FloatTensor(xh_train_preds)\n",
    "            xh_test_preds = torch.FloatTensor(xh_test_preds)\n",
    "\n",
    "            train_dataset = TensorDataset(xh_train_preds, y_train)\n",
    "            test_dataset = TensorDataset(xh_test_preds, y_test)\n",
    "            dummy_concepts = [f'feature{i:010}' for i in range(len(mid_concepts))]\n",
    "            model = XBRLClassifier(name=os.path.join(results_dir, f'{method}_{split}'), \n",
    "                                   n_classes=1, \n",
    "                                   n_features=x_train.shape[1], feature_names=dummy_concepts, \n",
    "                                   class_names=['~democrat', 'democrat'], discretize=True)\n",
    "            results = model.fit(train_dataset, metric=accuracy_score, save=True)\n",
    "            model_accuracy_mid = 0\n",
    "            global_explanation, elapsed_time = model.get_global_explanation(0, concepts=dummy_concepts, return_time=True)\n",
    "            explanation = logic.base.replace_names(global_explanation, mid_concepts)\n",
    "            \n",
    "            # compute metrics\n",
    "            y_preds = model.forward(xh_test_preds)\n",
    "            model_accuracy_top = accuracy_score(y_test, y_preds)\n",
    "            target_class = 1\n",
    "            explanation_accuracy, y_formula = logic.base.test_explanation(global_explanation, \n",
    "                                                                          target_class, \n",
    "                                                                          x=xh_test, y=y_test,\n",
    "                                                                          metric=accuracy_score)\n",
    "            explanation_fidelity = accuracy_score(y_preds, y_formula)\n",
    "            explanation_complexity = dl.logic.complexity(global_explanation)\n",
    "\n",
    "        else:\n",
    "            if method == 'psi':\n",
    "                # positive class\n",
    "                target_class = 1\n",
    "                level = 'mid'\n",
    "                model = train_psi_nn(x_train, xh_train, need_pruning, split, device, level, verbose)\n",
    "                xh_train_preds = model(x_train.to(device)).cpu().detach().numpy()\n",
    "                xh_test_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                xh_train_preds = torch.FloatTensor(xh_train_preds)\n",
    "                xh_test_preds = torch.FloatTensor(xh_test_preds)\n",
    "\n",
    "                level = 'high'\n",
    "                model_h = train_psi_nn(xh_train_preds, y_train, need_pruning, split, device, level, verbose)\n",
    "                y_train_preds = model_h(xh_train_preds.to(device)).cpu().detach().numpy()\n",
    "                y_test_preds = model_h(xh_test_preds.to(device)).cpu().detach().numpy()\n",
    "\n",
    "                model_accuracy_mid = accuracy_score(xh_test.cpu().detach().numpy(), xh_test_preds>0.5)\n",
    "                model_accuracy_top = accuracy_score(y_test.cpu().detach().numpy(), y_test_preds>0.5)\n",
    "\n",
    "            else:\n",
    "                level = 'mid'\n",
    "                model = train_nn(x_train, xh_train, need_pruning, seed, device, level, l1, lr, relu, verbose)\n",
    "                xh_train_preds = model(x_train.to(device)).cpu().detach().numpy()\n",
    "                xh_test_preds = model(x_test.to(device)).cpu().detach().numpy()\n",
    "                xh_train_preds = torch.FloatTensor(xh_train_preds)\n",
    "                xh_test_preds = torch.FloatTensor(xh_test_preds)\n",
    "\n",
    "                level = 'high'\n",
    "                model_h = train_nn(xh_train_preds, y_train, need_pruning, seed, device, level, l1, lr, relu, verbose)\n",
    "                y_train_preds = model_h(xh_train_preds.to(device)).cpu().detach().numpy()\n",
    "                y_test_preds = model_h(xh_test_preds.to(device)).cpu().detach().numpy()\n",
    "\n",
    "                model_accuracy_mid = accuracy_score(xh_test.cpu().detach().numpy(), xh_test_preds>0.5)\n",
    "                model_accuracy_top = accuracy_score(y_test.cpu().detach().numpy(), y_test_preds>0.5)\n",
    "\n",
    "            # positive class\n",
    "            target_class = 1\n",
    "            if method == 'psi':\n",
    "                global_explanation = logic.generate_fol_explanations(model_h, device)[0]\n",
    "            else:\n",
    "                global_explanation, _, _ = logic.relu_nn.combine_local_explanations(model_h, \n",
    "                                                                                    xh_train.to(device), y_train.to(device), \n",
    "                                                                                    target_class=target_class,\n",
    "                                                                                    topk_explanations=2,\n",
    "                                                                                    method=method, device=device)\n",
    "            explanation_fidelity = 0\n",
    "            explanation_complexity = 0\n",
    "            if global_explanation:\n",
    "                explanation = logic.base.replace_names(global_explanation, mid_concepts)\n",
    "                explanation_accuracy, y_formula = logic.base.test_explanation(global_explanation, \n",
    "                                                                              target_class, \n",
    "                                                                              x=xh_test, y=y_test,\n",
    "                                                                              metric=accuracy_score)\n",
    "                explanation_fidelity = accuracy_score(y_test_preds>0.5, y_formula)\n",
    "                explanation_complexity = dl.logic.complexity(global_explanation)\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'\\t Model\\'s accuracy (mid): {model_accuracy_mid:.4f} - Model\\'s accuracy (top): {model_accuracy_top:.4f}')\n",
    "            print(f'\\t Class {target_class} - Global explanation: \"{explanation}\" - Accuracy: {explanation_accuracy:.4f}')\n",
    "            print(f'\\t Elapsed time {elapsed_time}')\n",
    "            print(f'\\t Fidelity: \"{explanation_fidelity:.4f}\" - Complexity: \"{explanation_complexity}\"')\n",
    "\n",
    "        methods.append(method)\n",
    "        splits.append(split)\n",
    "        explanations.append(explanation)\n",
    "        model_accuracies_mid.append(model_accuracy_mid)\n",
    "        model_accuracies_top.append(model_accuracy_top)\n",
    "        explanation_accuracies.append(explanation_accuracy)\n",
    "        explanation_fidelities.append(explanation_fidelity)\n",
    "        explanation_complexities.append(explanation_complexity)\n",
    "        elapsed_times.append(elapsed_time)\n",
    "    \n",
    "    explanation_consistency = dl.logic.formula_consistency(explanations)\n",
    "    print(f'Consistency of explanations: {explanation_consistency:.4f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'method': methods,\n",
    "        'split': splits,\n",
    "        'explanation': explanations,\n",
    "        'model_accuracy_mid': model_accuracies_mid,\n",
    "        'model_accuracy_top': model_accuracies_top,\n",
    "        'explanation_accuracy': explanation_accuracies,\n",
    "        'explanation_fidelity': explanation_fidelities,\n",
    "        'explanation_complexity': explanation_complexities,\n",
    "        'explanation_consistency': explanation_consistency,\n",
    "        'elapsed_time': elapsed_times,\n",
    "    })\n",
    "    results.to_csv(os.path.join(results_dir, f'results_{method}.csv'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-absorption",
   "metadata": {},
   "source": [
    "## General pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "naughty-stevens",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "\t Epoch 0: train accuracy: 0.5429\n",
      "\t Epoch 500: train accuracy: 0.8896\n",
      "\t Epoch 1000: train accuracy: 0.9360\n",
      "\t Epoch 1500: train accuracy: 0.9566\n",
      "\t Epoch 2000: train accuracy: 0.9652\n",
      "\t Epoch 2500: train accuracy: 0.9717\n",
      "\t Epoch 3000: train accuracy: 0.9782\n",
      "\t Epoch 3500: train accuracy: 0.9837\n",
      "\t Epoch 4000: train accuracy: 0.9876\n",
      "\t Epoch 4500: train accuracy: 0.9904\n",
      "\t Epoch 5000: train accuracy: 0.9930\n",
      "\t Epoch 5500: train accuracy: 0.9944\n",
      "\t Epoch 6000: train accuracy: 0.9954\n",
      "\t Epoch 0: train accuracy: 0.5315\n",
      "\t Epoch 500: train accuracy: 0.9176\n",
      "\t Epoch 1000: train accuracy: 0.9435\n",
      "\t Epoch 1500: train accuracy: 0.9455\n",
      "\t Epoch 2000: train accuracy: 0.9455\n",
      "\t Epoch 2500: train accuracy: 0.9465\n",
      "\t Epoch 3000: train accuracy: 0.9465\n",
      "Pruned 8/14 features\n",
      "\t Epoch 3500: train accuracy: 0.9468\n",
      "\t Epoch 4000: train accuracy: 0.9468\n",
      "\t Epoch 4500: train accuracy: 0.9468\n",
      "\t Epoch 5000: train accuracy: 0.9468\n",
      "\t Epoch 5500: train accuracy: 0.9468\n",
      "\t Epoch 6000: train accuracy: 0.9468\n",
      "\t Model's accuracy (mid): 0.7345 - Model's accuracy (top): 0.9263\n",
      "\t Class 1 - Global explanation: \"v2x_freexp_altinf & v2x_frassoc_thick & v2xel_frefair & v2x_elecoff & v2xcl_rol\" - Accuracy: 0.9233\n",
      "\t Elapsed time 45.833245277404785\n",
      "\t Fidelity: \"0.9912\" - Complexity: \"5\"\n",
      "Split [2/10]\n",
      "\t Epoch 0: train accuracy: 0.5426\n",
      "\t Epoch 500: train accuracy: 0.8918\n",
      "\t Epoch 1000: train accuracy: 0.9365\n",
      "\t Epoch 1500: train accuracy: 0.9562\n",
      "\t Epoch 2000: train accuracy: 0.9650\n",
      "\t Epoch 2500: train accuracy: 0.9725\n",
      "\t Epoch 3000: train accuracy: 0.9782\n",
      "\t Epoch 3500: train accuracy: 0.9833\n",
      "\t Epoch 4000: train accuracy: 0.9871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-d3e30394b5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mneed_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresults_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_to_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_pruning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresults_pruning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-42f15a6f33f0>\u001b[0m in \u001b[0;36mc_to_y\u001b[1;34m(method, need_pruning, relu, l1, lr, verbose)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mid'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxh_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_pruning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mxh_train_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mxh_test_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c23279ddb617>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(x_train, y_train, need_pruning, seed, device, level, l1, lr, relu, verbose)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Compute Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_form\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = 'pruning'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_pruning = c_to_y(method, need_pruning, relu, l1=0.01, lr=0.0001, verbose=True)\n",
    "results_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "under-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split                       4.500000\n",
       "model_accuracy_mid          0.728509\n",
       "model_accuracy_top          0.923477\n",
       "explanation_accuracy        0.946819\n",
       "explanation_fidelity        0.951254\n",
       "explanation_complexity      5.000000\n",
       "explanation_consistency     1.000000\n",
       "elapsed_time               31.864813\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pruning.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-discrimination",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "empirical-essex",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "Split [2/10]\n",
      "Split [3/10]\n",
      "Split [4/10]\n",
      "Split [5/10]\n",
      "Split [6/10]\n",
      "Split [7/10]\n",
      "Split [8/10]\n",
      "Split [9/10]\n",
      "Split [10/10]\n",
      "Consistency of explanations: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>model_accuracy_mid</th>\n",
       "      <th>model_accuracy_top</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_fidelity</th>\n",
       "      <th>explanation_complexity</th>\n",
       "      <th>explanation_consistency</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weights</td>\n",
       "      <td>0</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2xel_frefair &amp; v2x_elecof...</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.941003</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>36.240852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weights</td>\n",
       "      <td>1</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.731563</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.967552</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>35.648936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weights</td>\n",
       "      <td>2</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.710914</td>\n",
       "      <td>0.923304</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>34.755997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weights</td>\n",
       "      <td>3</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.696165</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.704165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weights</td>\n",
       "      <td>4</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.731563</td>\n",
       "      <td>0.917404</td>\n",
       "      <td>0.961652</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.458029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weights</td>\n",
       "      <td>5</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.718935</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.571011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weights</td>\n",
       "      <td>6</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.733728</td>\n",
       "      <td>0.920118</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.346951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weights</td>\n",
       "      <td>7</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.742604</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.487034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weights</td>\n",
       "      <td>8</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.713018</td>\n",
       "      <td>0.917160</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>33.645938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weights</td>\n",
       "      <td>9</td>\n",
       "      <td>v2x_freexp_altinf &amp; v2x_frassoc_thick &amp; v2xel_...</td>\n",
       "      <td>0.754438</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>34.565918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    method  split                                        explanation  \\\n",
       "0  weights      0  v2x_freexp_altinf & v2xel_frefair & v2x_elecof...   \n",
       "1  weights      1  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "2  weights      2  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "3  weights      3  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "4  weights      4  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "5  weights      5  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "6  weights      6  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "7  weights      7  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "8  weights      8  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "9  weights      9  v2x_freexp_altinf & v2x_frassoc_thick & v2xel_...   \n",
       "\n",
       "   model_accuracy_mid  model_accuracy_top  explanation_accuracy  \\\n",
       "0            0.752212            0.941003              0.943953   \n",
       "1            0.731563            0.929204              0.955752   \n",
       "2            0.710914            0.923304              0.938053   \n",
       "3            0.696165            0.911504              0.949853   \n",
       "4            0.731563            0.917404              0.961652   \n",
       "5            0.718935            0.899408              0.934911   \n",
       "6            0.733728            0.920118              0.943787   \n",
       "7            0.742604            0.923077              0.961538   \n",
       "8            0.713018            0.917160              0.940828   \n",
       "9            0.754438            0.905325              0.934911   \n",
       "\n",
       "   explanation_fidelity  explanation_complexity  explanation_consistency  \\\n",
       "0              0.955752                       4                     0.98   \n",
       "1              0.967552                       5                     0.98   \n",
       "2              0.938053                       5                     0.98   \n",
       "3              0.943953                       5                     0.98   \n",
       "4              0.938053                       5                     0.98   \n",
       "5              0.934911                       5                     0.98   \n",
       "6              0.946746                       5                     0.98   \n",
       "7              0.961538                       5                     0.98   \n",
       "8              0.934911                       5                     0.98   \n",
       "9              0.940828                       5                     0.98   \n",
       "\n",
       "   elapsed_time  \n",
       "0     36.240852  \n",
       "1     35.648936  \n",
       "2     34.755997  \n",
       "3     33.704165  \n",
       "4     33.458029  \n",
       "5     33.571011  \n",
       "6     33.346951  \n",
       "7     33.487034  \n",
       "8     33.645938  \n",
       "9     34.565918  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'weights'\n",
    "need_pruning = False\n",
    "relu = True\n",
    "results_weights = c_to_y(method, need_pruning, relu, l1=0.00001, lr=0.0001, True)\n",
    "results_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-crowd",
   "metadata": {},
   "source": [
    "## Psi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "frozen-stockholm",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "\t Epoch 0: train accuracy: 0.4649\n",
      "\t Epoch 500: train accuracy: 0.9104\n",
      "\t Epoch 1000: train accuracy: 0.9367\n",
      "\t Epoch 1500: train accuracy: 0.9503\n",
      "\t Epoch 2000: train accuracy: 0.9622\n",
      "\t Epoch 2500: train accuracy: 0.9742\n",
      "\t Epoch 3000: train accuracy: 0.9847\n",
      "\t Epoch 3500: train accuracy: 0.8533\n",
      "\t Epoch 4000: train accuracy: 0.8856\n",
      "\t Epoch 4500: train accuracy: 0.8930\n",
      "\t Epoch 5000: train accuracy: 0.8943\n",
      "\t Epoch 5500: train accuracy: 0.8964\n",
      "\t Epoch 6000: train accuracy: 0.8992\n",
      "\t Epoch 0: train accuracy: 0.4685\n",
      "\t Epoch 500: train accuracy: 0.7173\n",
      "\t Epoch 1000: train accuracy: 0.8546\n",
      "\t Epoch 1500: train accuracy: 0.8651\n",
      "\t Epoch 2000: train accuracy: 0.8766\n",
      "\t Epoch 2500: train accuracy: 0.8775\n",
      "\t Epoch 3000: train accuracy: 0.8805\n",
      "\t Epoch 3500: train accuracy: 0.8513\n",
      "\t Epoch 4000: train accuracy: 0.8480\n",
      "\t Epoch 4500: train accuracy: 0.8510\n",
      "\t Epoch 5000: train accuracy: 0.8595\n",
      "\t Epoch 5500: train accuracy: 0.8618\n",
      "\t Epoch 6000: train accuracy: 0.8638\n",
      "\t Model's accuracy (mid): 0.3451 - Model's accuracy (top): 0.8673\n",
      "\t Class 1 - Global explanation: \"(v2x_mpi)\" - Accuracy: 0.8142\n",
      "\t Elapsed time 20.810575246810913\n",
      "\t Fidelity: \"0.8230\" - Complexity: \"1\"\n",
      "Split [2/10]\n",
      "\t Epoch 0: train accuracy: 0.4695\n",
      "\t Epoch 500: train accuracy: 0.9072\n",
      "\t Epoch 1000: train accuracy: 0.9349\n",
      "\t Epoch 1500: train accuracy: 0.9480\n",
      "\t Epoch 2000: train accuracy: 0.9592\n",
      "\t Epoch 2500: train accuracy: 0.9750\n",
      "\t Epoch 3000: train accuracy: 0.9841\n",
      "\t Epoch 3500: train accuracy: 0.8153\n",
      "\t Epoch 4000: train accuracy: 0.8607\n",
      "\t Epoch 4500: train accuracy: 0.8692\n",
      "\t Epoch 5000: train accuracy: 0.8751\n",
      "\t Epoch 5500: train accuracy: 0.8810\n",
      "\t Epoch 6000: train accuracy: 0.8829\n",
      "\t Epoch 0: train accuracy: 0.5315\n",
      "\t Epoch 500: train accuracy: 0.7754\n",
      "\t Epoch 1000: train accuracy: 0.8450\n",
      "\t Epoch 1500: train accuracy: 0.8572\n",
      "\t Epoch 2000: train accuracy: 0.8615\n",
      "\t Epoch 2500: train accuracy: 0.8697\n",
      "\t Epoch 3000: train accuracy: 0.8710\n",
      "\t Epoch 3500: train accuracy: 0.8194\n",
      "\t Epoch 4000: train accuracy: 0.8276\n",
      "\t Epoch 4500: train accuracy: 0.8244\n",
      "\t Epoch 5000: train accuracy: 0.8273\n",
      "\t Epoch 5500: train accuracy: 0.8296\n",
      "\t Epoch 6000: train accuracy: 0.8336\n",
      "\t Model's accuracy (mid): 0.3363 - Model's accuracy (top): 0.8496\n",
      "\t Class 1 - Global explanation: \"(v2x_freexp_altinf & v2xcl_rol)\" - Accuracy: 0.8289\n",
      "\t Elapsed time 21.03696036338806\n",
      "\t Fidelity: \"0.8319\" - Complexity: \"2\"\n",
      "Split [3/10]\n",
      "\t Epoch 0: train accuracy: 0.5062\n",
      "\t Epoch 500: train accuracy: 0.9127\n",
      "\t Epoch 1000: train accuracy: 0.9378\n",
      "\t Epoch 1500: train accuracy: 0.9504\n",
      "\t Epoch 2000: train accuracy: 0.9652\n",
      "\t Epoch 2500: train accuracy: 0.9788\n",
      "\t Epoch 3000: train accuracy: 0.9865\n",
      "\t Epoch 3500: train accuracy: 0.8423\n",
      "\t Epoch 4000: train accuracy: 0.8808\n",
      "\t Epoch 4500: train accuracy: 0.8839\n",
      "\t Epoch 5000: train accuracy: 0.8922\n",
      "\t Epoch 5500: train accuracy: 0.8999\n",
      "\t Epoch 6000: train accuracy: 0.9007\n",
      "\t Epoch 0: train accuracy: 0.4685\n",
      "\t Epoch 500: train accuracy: 0.8362\n",
      "\t Epoch 1000: train accuracy: 0.8723\n",
      "\t Epoch 1500: train accuracy: 0.8802\n",
      "\t Epoch 2000: train accuracy: 0.8808\n",
      "\t Epoch 2500: train accuracy: 0.8795\n",
      "\t Epoch 3000: train accuracy: 0.8743\n",
      "\t Epoch 3500: train accuracy: 0.7991\n",
      "\t Epoch 4000: train accuracy: 0.8060\n",
      "\t Epoch 4500: train accuracy: 0.8102\n",
      "\t Epoch 5000: train accuracy: 0.8125\n",
      "\t Epoch 5500: train accuracy: 0.8178\n",
      "\t Epoch 6000: train accuracy: 0.8240\n",
      "\t Model's accuracy (mid): 0.3540 - Model's accuracy (top): 0.8348\n",
      "\t Class 1 - Global explanation: \"(v2xeg_eqaccess & (v2xdl_delib | ~v2x_suffr))\" - Accuracy: 0.8466\n",
      "\t Elapsed time 20.94809055328369\n",
      "\t Fidelity: \"0.8584\" - Complexity: \"3\"\n",
      "Split [4/10]\n",
      "\t Epoch 0: train accuracy: 0.3644\n",
      "\t Epoch 500: train accuracy: 0.9121\n",
      "\t Epoch 1000: train accuracy: 0.9362\n",
      "\t Epoch 1500: train accuracy: 0.9495\n",
      "\t Epoch 2000: train accuracy: 0.9636\n",
      "\t Epoch 2500: train accuracy: 0.9739\n",
      "\t Epoch 3000: train accuracy: 0.9837\n",
      "\t Epoch 3500: train accuracy: 0.8111\n",
      "\t Epoch 4000: train accuracy: 0.8292\n",
      "\t Epoch 4500: train accuracy: 0.8642\n",
      "\t Epoch 5000: train accuracy: 0.8708\n",
      "\t Epoch 5500: train accuracy: 0.8749\n",
      "\t Epoch 6000: train accuracy: 0.8750\n",
      "\t Epoch 0: train accuracy: 0.5315\n",
      "\t Epoch 500: train accuracy: 0.8342\n",
      "\t Epoch 1000: train accuracy: 0.8631\n",
      "\t Epoch 1500: train accuracy: 0.8707\n",
      "\t Epoch 2000: train accuracy: 0.8762\n",
      "\t Epoch 2500: train accuracy: 0.8795\n",
      "\t Epoch 3000: train accuracy: 0.8841\n",
      "\t Epoch 3500: train accuracy: 0.7180\n",
      "\t Epoch 4000: train accuracy: 0.7124\n",
      "\t Epoch 4500: train accuracy: 0.7183\n",
      "\t Epoch 5000: train accuracy: 0.7410\n",
      "\t Epoch 5500: train accuracy: 0.7387\n",
      "\t Epoch 6000: train accuracy: 0.7702\n",
      "\t Model's accuracy (mid): 0.2655 - Model's accuracy (top): 0.7640\n",
      "\t Class 1 - Global explanation: \"(v2x_elecoff)\" - Accuracy: 0.6814\n",
      "\t Elapsed time 20.736979484558105\n",
      "\t Fidelity: \"0.7050\" - Complexity: \"1\"\n",
      "Split [5/10]\n",
      "\t Epoch 0: train accuracy: 0.5734\n",
      "\t Epoch 500: train accuracy: 0.9120\n",
      "\t Epoch 1000: train accuracy: 0.9376\n",
      "\t Epoch 1500: train accuracy: 0.9500\n",
      "\t Epoch 2000: train accuracy: 0.9641\n",
      "\t Epoch 2500: train accuracy: 0.9770\n",
      "\t Epoch 3000: train accuracy: 0.9847\n",
      "\t Epoch 3500: train accuracy: 0.7999\n",
      "\t Epoch 4000: train accuracy: 0.8295\n",
      "\t Epoch 4500: train accuracy: 0.8427\n",
      "\t Epoch 5000: train accuracy: 0.8587\n",
      "\t Epoch 5500: train accuracy: 0.8615\n",
      "\t Epoch 6000: train accuracy: 0.8672\n",
      "\t Epoch 0: train accuracy: 0.5315\n",
      "\t Epoch 500: train accuracy: 0.8414\n",
      "\t Epoch 1000: train accuracy: 0.8601\n",
      "\t Epoch 1500: train accuracy: 0.8644\n",
      "\t Epoch 2000: train accuracy: 0.8664\n",
      "\t Epoch 2500: train accuracy: 0.8697\n",
      "\t Epoch 3000: train accuracy: 0.8785\n",
      "\t Epoch 3500: train accuracy: 0.6202\n",
      "\t Epoch 4000: train accuracy: 0.7981\n",
      "\t Epoch 4500: train accuracy: 0.8142\n",
      "\t Epoch 5000: train accuracy: 0.8230\n",
      "\t Epoch 5500: train accuracy: 0.8148\n",
      "\t Epoch 6000: train accuracy: 0.8135\n",
      "\t Model's accuracy (mid): 0.1681 - Model's accuracy (top): 0.8201\n",
      "\t Class 1 - Global explanation: \"(v2x_frassoc_thick & v2xeg_eqaccess)\" - Accuracy: 0.8761\n",
      "\t Elapsed time 20.86394429206848\n",
      "\t Fidelity: \"0.8260\" - Complexity: \"2\"\n",
      "Split [6/10]\n",
      "\t Epoch 0: train accuracy: 0.4149\n",
      "\t Epoch 500: train accuracy: 0.9151\n",
      "\t Epoch 1000: train accuracy: 0.9371\n",
      "\t Epoch 1500: train accuracy: 0.9476\n",
      "\t Epoch 2000: train accuracy: 0.9619\n",
      "\t Epoch 2500: train accuracy: 0.9750\n",
      "\t Epoch 3000: train accuracy: 0.9843\n",
      "\t Epoch 3500: train accuracy: 0.8164\n",
      "\t Epoch 4000: train accuracy: 0.8335\n",
      "\t Epoch 4500: train accuracy: 0.8559\n",
      "\t Epoch 5000: train accuracy: 0.8668\n",
      "\t Epoch 5500: train accuracy: 0.8738\n",
      "\t Epoch 6000: train accuracy: 0.8736\n",
      "\t Epoch 0: train accuracy: 0.4687\n",
      "\t Epoch 500: train accuracy: 0.8326\n",
      "\t Epoch 1000: train accuracy: 0.8697\n",
      "\t Epoch 1500: train accuracy: 0.8792\n",
      "\t Epoch 2000: train accuracy: 0.8825\n",
      "\t Epoch 2500: train accuracy: 0.8858\n",
      "\t Epoch 3000: train accuracy: 0.8904\n",
      "\t Epoch 3500: train accuracy: 0.7345\n",
      "\t Epoch 4000: train accuracy: 0.7565\n",
      "\t Epoch 4500: train accuracy: 0.7683\n",
      "\t Epoch 5000: train accuracy: 0.7758\n",
      "\t Epoch 5500: train accuracy: 0.7801\n",
      "\t Epoch 6000: train accuracy: 0.7896\n",
      "\t Model's accuracy (mid): 0.3787 - Model's accuracy (top): 0.7870\n",
      "\t Class 1 - Global explanation: \"(v2x_frassoc_thick & v2xeg_eqaccess)\" - Accuracy: 0.7959\n",
      "\t Elapsed time 20.888019323349\n",
      "\t Fidelity: \"0.8077\" - Complexity: \"2\"\n",
      "Split [7/10]\n",
      "\t Epoch 0: train accuracy: 0.5186\n",
      "\t Epoch 500: train accuracy: 0.9088\n",
      "\t Epoch 1000: train accuracy: 0.9361\n",
      "\t Epoch 1500: train accuracy: 0.9501\n",
      "\t Epoch 2000: train accuracy: 0.9611\n",
      "\t Epoch 2500: train accuracy: 0.9769\n",
      "\t Epoch 3000: train accuracy: 0.9853\n",
      "\t Epoch 3500: train accuracy: 0.8163\n",
      "\t Epoch 4000: train accuracy: 0.8711\n",
      "\t Epoch 4500: train accuracy: 0.8824\n",
      "\t Epoch 5000: train accuracy: 0.9013\n",
      "\t Epoch 5500: train accuracy: 0.9029\n",
      "\t Epoch 6000: train accuracy: 0.9034\n",
      "\t Epoch 0: train accuracy: 0.5313\n",
      "\t Epoch 500: train accuracy: 0.8392\n",
      "\t Epoch 1000: train accuracy: 0.8654\n",
      "\t Epoch 1500: train accuracy: 0.8694\n",
      "\t Epoch 2000: train accuracy: 0.8730\n",
      "\t Epoch 2500: train accuracy: 0.8779\n",
      "\t Epoch 3000: train accuracy: 0.8871\n",
      "\t Epoch 3500: train accuracy: 0.7384\n",
      "\t Epoch 4000: train accuracy: 0.7535\n",
      "\t Epoch 4500: train accuracy: 0.7529\n",
      "\t Epoch 5000: train accuracy: 0.7466\n",
      "\t Epoch 5500: train accuracy: 0.7496\n",
      "\t Epoch 6000: train accuracy: 0.7473\n",
      "\t Model's accuracy (mid): 0.3462 - Model's accuracy (top): 0.7337\n",
      "\t Class 1 - Global explanation: \"(v2x_egal)\" - Accuracy: 0.7101\n",
      "\t Elapsed time 20.66694474220276\n",
      "\t Fidelity: \"0.8402\" - Complexity: \"1\"\n",
      "Split [8/10]\n",
      "\t Epoch 0: train accuracy: 0.5142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 500: train accuracy: 0.9026\n",
      "\t Epoch 1000: train accuracy: 0.9372\n",
      "\t Epoch 1500: train accuracy: 0.9485\n",
      "\t Epoch 2000: train accuracy: 0.9651\n",
      "\t Epoch 2500: train accuracy: 0.9782\n",
      "\t Epoch 3000: train accuracy: 0.9863\n",
      "\t Epoch 3500: train accuracy: 0.8006\n",
      "\t Epoch 4000: train accuracy: 0.8554\n",
      "\t Epoch 4500: train accuracy: 0.8759\n",
      "\t Epoch 5000: train accuracy: 0.8800\n",
      "\t Epoch 5500: train accuracy: 0.8835\n",
      "\t Epoch 6000: train accuracy: 0.8845\n",
      "\t Epoch 0: train accuracy: 0.5313\n",
      "\t Epoch 500: train accuracy: 0.8461\n",
      "\t Epoch 1000: train accuracy: 0.8661\n",
      "\t Epoch 1500: train accuracy: 0.8677\n",
      "\t Epoch 2000: train accuracy: 0.8691\n",
      "\t Epoch 2500: train accuracy: 0.8710\n",
      "\t Epoch 3000: train accuracy: 0.8809\n",
      "\t Epoch 3500: train accuracy: 0.7460\n",
      "\t Epoch 4000: train accuracy: 0.7699\n",
      "\t Epoch 4500: train accuracy: 0.7781\n",
      "\t Epoch 5000: train accuracy: 0.7749\n",
      "\t Epoch 5500: train accuracy: 0.7870\n",
      "\t Epoch 6000: train accuracy: 0.7968\n",
      "\t Model's accuracy (mid): 0.3166 - Model's accuracy (top): 0.7811\n",
      "\t Class 1 - Global explanation: \"(v2xel_frefair)\" - Accuracy: 0.9201\n",
      "\t Elapsed time 20.79501700401306\n",
      "\t Fidelity: \"0.7959\" - Complexity: \"1\"\n",
      "Split [9/10]\n",
      "\t Epoch 0: train accuracy: 0.5690\n",
      "\t Epoch 500: train accuracy: 0.9119\n",
      "\t Epoch 1000: train accuracy: 0.9372\n",
      "\t Epoch 1500: train accuracy: 0.9493\n",
      "\t Epoch 2000: train accuracy: 0.9618\n",
      "\t Epoch 2500: train accuracy: 0.9753\n",
      "\t Epoch 3000: train accuracy: 0.9836\n",
      "\t Epoch 3500: train accuracy: 0.7855\n",
      "\t Epoch 4000: train accuracy: 0.8039\n",
      "\t Epoch 4500: train accuracy: 0.8427\n",
      "\t Epoch 5000: train accuracy: 0.8544\n",
      "\t Epoch 5500: train accuracy: 0.8617\n",
      "\t Epoch 6000: train accuracy: 0.8674\n",
      "\t Epoch 0: train accuracy: 0.5313\n",
      "\t Epoch 500: train accuracy: 0.8014\n",
      "\t Epoch 1000: train accuracy: 0.8707\n",
      "\t Epoch 1500: train accuracy: 0.8756\n",
      "\t Epoch 2000: train accuracy: 0.8773\n",
      "\t Epoch 2500: train accuracy: 0.8750\n",
      "\t Epoch 3000: train accuracy: 0.8841\n",
      "\t Epoch 3500: train accuracy: 0.8014\n",
      "\t Epoch 4000: train accuracy: 0.8303\n",
      "\t Epoch 4500: train accuracy: 0.8513\n",
      "\t Epoch 5000: train accuracy: 0.8487\n",
      "\t Epoch 5500: train accuracy: 0.8503\n",
      "\t Epoch 6000: train accuracy: 0.8563\n",
      "\t Model's accuracy (mid): 0.3047 - Model's accuracy (top): 0.8491\n",
      "\t Class 1 - Global explanation: \"(v2x_freexp_altinf & v2x_cspart & (v2xel_frefair | ~v2x_suffr) & (v2xeg_eqaccess | ~v2x_suffr))\" - Accuracy: 0.9083\n",
      "\t Elapsed time 20.954056978225708\n",
      "\t Fidelity: \"0.8225\" - Complexity: \"6\"\n",
      "Split [10/10]\n",
      "\t Epoch 0: train accuracy: 0.5914\n",
      "\t Epoch 500: train accuracy: 0.9018\n",
      "\t Epoch 1000: train accuracy: 0.9371\n",
      "\t Epoch 1500: train accuracy: 0.9494\n",
      "\t Epoch 2000: train accuracy: 0.9649\n",
      "\t Epoch 2500: train accuracy: 0.9785\n",
      "\t Epoch 3000: train accuracy: 0.9861\n",
      "\t Epoch 3500: train accuracy: 0.7995\n",
      "\t Epoch 4000: train accuracy: 0.8485\n",
      "\t Epoch 4500: train accuracy: 0.8572\n",
      "\t Epoch 5000: train accuracy: 0.8672\n",
      "\t Epoch 5500: train accuracy: 0.8754\n",
      "\t Epoch 6000: train accuracy: 0.8801\n",
      "\t Epoch 0: train accuracy: 0.4683\n",
      "\t Epoch 500: train accuracy: 0.8011\n",
      "\t Epoch 1000: train accuracy: 0.8540\n",
      "\t Epoch 1500: train accuracy: 0.8664\n",
      "\t Epoch 2000: train accuracy: 0.8664\n",
      "\t Epoch 2500: train accuracy: 0.8740\n",
      "\t Epoch 3000: train accuracy: 0.8796\n",
      "\t Epoch 3500: train accuracy: 0.6387\n",
      "\t Epoch 4000: train accuracy: 0.6387\n",
      "\t Epoch 4500: train accuracy: 0.6387\n",
      "\t Epoch 5000: train accuracy: 0.6387\n",
      "\t Epoch 5500: train accuracy: 0.6387\n",
      "\t Epoch 6000: train accuracy: 0.6387\n",
      "\t Model's accuracy (mid): 0.3314 - Model's accuracy (top): 0.6361\n",
      "\t Class 1 - Global explanation: \"(v2xeg_eqdr)\" - Accuracy: 0.6479\n",
      "\t Elapsed time 20.614274740219116\n",
      "\t Fidelity: \"0.9349\" - Complexity: \"1\"\n",
      "Consistency of explanations: 0.1583\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>model_accuracy_mid</th>\n",
       "      <th>model_accuracy_top</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_fidelity</th>\n",
       "      <th>explanation_complexity</th>\n",
       "      <th>explanation_consistency</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>psi</td>\n",
       "      <td>0</td>\n",
       "      <td>(v2x_mpi)</td>\n",
       "      <td>0.345133</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.814159</td>\n",
       "      <td>0.823009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.810575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>psi</td>\n",
       "      <td>1</td>\n",
       "      <td>(v2x_freexp_altinf &amp; v2xcl_rol)</td>\n",
       "      <td>0.336283</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.828909</td>\n",
       "      <td>0.831858</td>\n",
       "      <td>2</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>21.036960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psi</td>\n",
       "      <td>2</td>\n",
       "      <td>(v2xeg_eqaccess &amp; (v2xdl_delib | ~v2x_suffr))</td>\n",
       "      <td>0.353982</td>\n",
       "      <td>0.834808</td>\n",
       "      <td>0.846608</td>\n",
       "      <td>0.858407</td>\n",
       "      <td>3</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.948091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>psi</td>\n",
       "      <td>3</td>\n",
       "      <td>(v2x_elecoff)</td>\n",
       "      <td>0.265487</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.681416</td>\n",
       "      <td>0.705015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.736979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>psi</td>\n",
       "      <td>4</td>\n",
       "      <td>(v2x_frassoc_thick &amp; v2xeg_eqaccess)</td>\n",
       "      <td>0.168142</td>\n",
       "      <td>0.820059</td>\n",
       "      <td>0.876106</td>\n",
       "      <td>0.825959</td>\n",
       "      <td>2</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.863944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>psi</td>\n",
       "      <td>5</td>\n",
       "      <td>(v2x_frassoc_thick &amp; v2xeg_eqaccess)</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.795858</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>2</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.888019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>psi</td>\n",
       "      <td>6</td>\n",
       "      <td>(v2x_egal)</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.733728</td>\n",
       "      <td>0.710059</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.666945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>psi</td>\n",
       "      <td>7</td>\n",
       "      <td>(v2xel_frefair)</td>\n",
       "      <td>0.316568</td>\n",
       "      <td>0.781065</td>\n",
       "      <td>0.920118</td>\n",
       "      <td>0.795858</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.795017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psi</td>\n",
       "      <td>8</td>\n",
       "      <td>(v2x_freexp_altinf &amp; v2x_cspart &amp; (v2xel_frefa...</td>\n",
       "      <td>0.304734</td>\n",
       "      <td>0.849112</td>\n",
       "      <td>0.908284</td>\n",
       "      <td>0.822485</td>\n",
       "      <td>6</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.954057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>psi</td>\n",
       "      <td>9</td>\n",
       "      <td>(v2xeg_eqdr)</td>\n",
       "      <td>0.331361</td>\n",
       "      <td>0.636095</td>\n",
       "      <td>0.647929</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>20.614275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method  split                                        explanation  \\\n",
       "0    psi      0                                          (v2x_mpi)   \n",
       "1    psi      1                    (v2x_freexp_altinf & v2xcl_rol)   \n",
       "2    psi      2      (v2xeg_eqaccess & (v2xdl_delib | ~v2x_suffr))   \n",
       "3    psi      3                                      (v2x_elecoff)   \n",
       "4    psi      4               (v2x_frassoc_thick & v2xeg_eqaccess)   \n",
       "5    psi      5               (v2x_frassoc_thick & v2xeg_eqaccess)   \n",
       "6    psi      6                                         (v2x_egal)   \n",
       "7    psi      7                                    (v2xel_frefair)   \n",
       "8    psi      8  (v2x_freexp_altinf & v2x_cspart & (v2xel_frefa...   \n",
       "9    psi      9                                       (v2xeg_eqdr)   \n",
       "\n",
       "   model_accuracy_mid  model_accuracy_top  explanation_accuracy  \\\n",
       "0            0.345133            0.867257              0.814159   \n",
       "1            0.336283            0.849558              0.828909   \n",
       "2            0.353982            0.834808              0.846608   \n",
       "3            0.265487            0.764012              0.681416   \n",
       "4            0.168142            0.820059              0.876106   \n",
       "5            0.378698            0.786982              0.795858   \n",
       "6            0.346154            0.733728              0.710059   \n",
       "7            0.316568            0.781065              0.920118   \n",
       "8            0.304734            0.849112              0.908284   \n",
       "9            0.331361            0.636095              0.647929   \n",
       "\n",
       "   explanation_fidelity  explanation_complexity  explanation_consistency  \\\n",
       "0              0.823009                       1                 0.158333   \n",
       "1              0.831858                       2                 0.158333   \n",
       "2              0.858407                       3                 0.158333   \n",
       "3              0.705015                       1                 0.158333   \n",
       "4              0.825959                       2                 0.158333   \n",
       "5              0.807692                       2                 0.158333   \n",
       "6              0.840237                       1                 0.158333   \n",
       "7              0.795858                       1                 0.158333   \n",
       "8              0.822485                       6                 0.158333   \n",
       "9              0.934911                       1                 0.158333   \n",
       "\n",
       "   elapsed_time  \n",
       "0     20.810575  \n",
       "1     21.036960  \n",
       "2     20.948091  \n",
       "3     20.736979  \n",
       "4     20.863944  \n",
       "5     20.888019  \n",
       "6     20.666945  \n",
       "7     20.795017  \n",
       "8     20.954057  \n",
       "9     20.614275  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'psi'\n",
    "need_pruning = True\n",
    "relu = False\n",
    "results_psi = c_to_y(method, need_pruning, relu, verbose=True)\n",
    "results_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-addition",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "scheduled-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "Split [2/10]\n",
      "Split [3/10]\n",
      "Split [4/10]\n",
      "Split [5/10]\n",
      "Split [6/10]\n",
      "Split [7/10]\n",
      "Split [8/10]\n",
      "Split [9/10]\n",
      "Split [10/10]\n",
      "Consistency of explanations: 0.3686\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>split</th>\n",
       "      <th>explanation</th>\n",
       "      <th>model_accuracy_mid</th>\n",
       "      <th>model_accuracy_top</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_fidelity</th>\n",
       "      <th>explanation_complexity</th>\n",
       "      <th>explanation_consistency</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree</td>\n",
       "      <td>0</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.33 &amp; v2xcl_rol &gt; 0.90 &amp; v2...</td>\n",
       "      <td>0.749263</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>305</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.090787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>1</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.33 &amp; v2xdl_delib &gt; 0.58 &amp; ...</td>\n",
       "      <td>0.784661</td>\n",
       "      <td>0.952802</td>\n",
       "      <td>0.952802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>294</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.072805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tree</td>\n",
       "      <td>2</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.58 &amp; v2xcl_rol &gt; 0.88 &amp; v2...</td>\n",
       "      <td>0.746313</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>236</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.078789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tree</td>\n",
       "      <td>3</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.88 &amp; v2xcl_rol &gt; 0.90 &amp; v2...</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.069811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tree</td>\n",
       "      <td>4</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.44 &amp; v2xcl_rol &gt; 0.90 &amp; v2...</td>\n",
       "      <td>0.722714</td>\n",
       "      <td>0.958702</td>\n",
       "      <td>0.958702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.070817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tree</td>\n",
       "      <td>5</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.46 &amp; v2xdl_delib &gt; 0.61 &amp; ...</td>\n",
       "      <td>0.745562</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>319</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.082806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tree</td>\n",
       "      <td>6</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.47 &amp; v2xcl_rol &gt; 0.90 &amp; v2...</td>\n",
       "      <td>0.760355</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>313</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.073838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tree</td>\n",
       "      <td>7</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.33 &amp; v2xcl_rol &gt; 0.88 &amp; v2...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>309</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.075795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tree</td>\n",
       "      <td>8</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.88 &amp; v2xcl_rol &gt; 0.90 &amp; v2...</td>\n",
       "      <td>0.754438</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.073803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tree</td>\n",
       "      <td>9</td>\n",
       "      <td>(v2xel_frefair &lt;= 0.58 &amp; v2xcl_rol &gt; 0.88 &amp; v2...</td>\n",
       "      <td>0.789941</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>314</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.071769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method  split                                        explanation  \\\n",
       "0   tree      0  (v2xel_frefair <= 0.33 & v2xcl_rol > 0.90 & v2...   \n",
       "1   tree      1  (v2xel_frefair <= 0.33 & v2xdl_delib > 0.58 & ...   \n",
       "2   tree      2  (v2xel_frefair <= 0.58 & v2xcl_rol > 0.88 & v2...   \n",
       "3   tree      3  (v2xel_frefair <= 0.88 & v2xcl_rol > 0.90 & v2...   \n",
       "4   tree      4  (v2xel_frefair <= 0.44 & v2xcl_rol > 0.90 & v2...   \n",
       "5   tree      5  (v2xel_frefair <= 0.46 & v2xdl_delib > 0.61 & ...   \n",
       "6   tree      6  (v2xel_frefair <= 0.47 & v2xcl_rol > 0.90 & v2...   \n",
       "7   tree      7  (v2xel_frefair <= 0.33 & v2xcl_rol > 0.88 & v2...   \n",
       "8   tree      8  (v2xel_frefair <= 0.88 & v2xcl_rol > 0.90 & v2...   \n",
       "9   tree      9  (v2xel_frefair <= 0.58 & v2xcl_rol > 0.88 & v2...   \n",
       "\n",
       "   model_accuracy_mid  model_accuracy_top  explanation_accuracy  \\\n",
       "0            0.749263            0.946903              0.946903   \n",
       "1            0.784661            0.952802              0.952802   \n",
       "2            0.746313            0.938053              0.938053   \n",
       "3            0.734513            0.949853              0.949853   \n",
       "4            0.722714            0.958702              0.958702   \n",
       "5            0.745562            0.931953              0.931953   \n",
       "6            0.760355            0.943787              0.943787   \n",
       "7            0.769231            0.961538              0.961538   \n",
       "8            0.754438            0.940828              0.940828   \n",
       "9            0.789941            0.934911              0.934911   \n",
       "\n",
       "   explanation_fidelity  explanation_complexity  explanation_consistency  \\\n",
       "0                   1.0                     305                 0.368571   \n",
       "1                   1.0                     294                 0.368571   \n",
       "2                   1.0                     236                 0.368571   \n",
       "3                   1.0                     289                 0.368571   \n",
       "4                   1.0                     289                 0.368571   \n",
       "5                   1.0                     319                 0.368571   \n",
       "6                   1.0                     313                 0.368571   \n",
       "7                   1.0                     309                 0.368571   \n",
       "8                   1.0                     273                 0.368571   \n",
       "9                   1.0                     314                 0.368571   \n",
       "\n",
       "   elapsed_time  \n",
       "0      0.090787  \n",
       "1      0.072805  \n",
       "2      0.078789  \n",
       "3      0.069811  \n",
       "4      0.070817  \n",
       "5      0.082806  \n",
       "6      0.073838  \n",
       "7      0.075795  \n",
       "8      0.073803  \n",
       "9      0.071769  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'tree'\n",
    "need_pruning = False\n",
    "relu = False\n",
    "results_tree = c_to_y(method, need_pruning, relu, verbose=False)\n",
    "results_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-agent",
   "metadata": {},
   "source": [
    "## BRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "binding-salmon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/10]\n",
      "Labels (3046, 1)\n",
      "Discretized features\n",
      "Completed model 1/1!\n",
      "Train_acc: 0.9, Val_acc: 0.0\n",
      "\t Model's accuracy (mid): 0.0000 - Model's accuracy (top): 0.9233\n",
      "\t Class 1 - Global explanation: \"v2x_mpi | (v2x_freexp_altinf & v2x_frassoc_thick & v2xel_frefair & v2x_elecoff & v2xcl_rol)\" - Accuracy: 0.9469\n",
      "\t Elapsed time 40.07798767089844\n",
      "\t Fidelity: \"0.9469\" - Complexity: \"6\"\n",
      "Split [2/10]\n",
      "Labels (3046, 1)\n",
      "Discretized features\n",
      "Completed model 1/1!\n",
      "Train_acc: 0.9, Val_acc: 0.0\n",
      "\t Model's accuracy (mid): 0.0000 - Model's accuracy (top): 0.9322\n",
      "\t Class 1 - Global explanation: \"(v2x_mpi & v2x_freexp_altinf) | (v2x_freexp_altinf & v2x_frassoc_thick & v2xel_frefair & v2x_elecoff & v2xcl_rol & ~v2x_mpi) | (v2x_freexp_altinf & v2x_frassoc_thick & v2x_elecoff & v2xcl_rol & ~v2x_api & ~v2x_mpi & ~v2xel_frefair & ~v2xeg_eqdr & ~(v2xdl_delib & v2xeg_eqaccess))\" - Accuracy: 0.9558\n",
      "\t Elapsed time 39.23343467712402\n",
      "\t Fidelity: \"0.9764\" - Complexity: \"18\"\n",
      "Split [3/10]\n",
      "Labels (3046, 1)\n",
      "Discretized features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-42574ae5f7a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mneed_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresults_brl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_to_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_pruning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresults_brl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-769b3ab6913c>\u001b[0m in \u001b[0;36mc_to_y\u001b[1;34m(method, need_pruning, relu, l1, lr, verbose)\u001b[0m\n\u001b[0;32m     66\u001b[0m                                    \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_concepts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                                    class_names=['~democrat', 'democrat'], discretize=True)\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mmodel_accuracy_mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mglobal_explanation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_concepts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Mega\\research\\coding\\neural_networks\\deep-logic\\deep_logic\\models\\brl.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_set, val_set, metric, verbose, save, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRuleListClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Completed model {i + 1}/{self.n_classes}!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = 'brl'\n",
    "need_pruning = False\n",
    "relu = False\n",
    "results_brl = c_to_y(method, need_pruning, relu, verbose=True)\n",
    "results_brl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-liberia",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competent-measure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_accuracy_top_mean</th>\n",
       "      <th>explanation_accuracy_mean</th>\n",
       "      <th>explanation_fidelity_mean</th>\n",
       "      <th>explanation_complexity_mean</th>\n",
       "      <th>elapsed_time_mean</th>\n",
       "      <th>explanation_consistency_mean</th>\n",
       "      <th>model_accuracy_top_sem</th>\n",
       "      <th>explanation_accuracy_sem</th>\n",
       "      <th>explanation_fidelity_sem</th>\n",
       "      <th>explanation_complexity_sem</th>\n",
       "      <th>elapsed_time_sem</th>\n",
       "      <th>explanation_consistency_sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pruning</th>\n",
       "      <td>0.919639</td>\n",
       "      <td>0.908110</td>\n",
       "      <td>0.915208</td>\n",
       "      <td>9.6</td>\n",
       "      <td>34.403531</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>1.166190</td>\n",
       "      <td>0.784260</td>\n",
       "      <td>3.700743e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weights</th>\n",
       "      <td>0.918751</td>\n",
       "      <td>0.946524</td>\n",
       "      <td>0.946230</td>\n",
       "      <td>4.9</td>\n",
       "      <td>34.242483</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.323599</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psi</th>\n",
       "      <td>0.792268</td>\n",
       "      <td>0.802945</td>\n",
       "      <td>0.824543</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.831486</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.029882</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.494413</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>9.251859e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>0.921115</td>\n",
       "      <td>0.921115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>294.1</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.368571</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.878875</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>1.850372e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRL</th>\n",
       "      <td>0.922293</td>\n",
       "      <td>0.946819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.7</td>\n",
       "      <td>53.689056</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.406669</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_accuracy_top_mean  explanation_accuracy_mean  \\\n",
       "pruning                 0.919639                   0.908110   \n",
       "weights                 0.918751                   0.946524   \n",
       "psi                     0.792268                   0.802945   \n",
       "tree                    0.921115                   0.921115   \n",
       "BRL                     0.922293                   0.946819   \n",
       "\n",
       "         explanation_fidelity_mean  explanation_complexity_mean  \\\n",
       "pruning                   0.915208                          9.6   \n",
       "weights                   0.946230                          4.9   \n",
       "psi                       0.824543                          2.0   \n",
       "tree                      1.000000                        294.1   \n",
       "BRL                       1.000000                          8.7   \n",
       "\n",
       "         elapsed_time_mean  explanation_consistency_mean  \\\n",
       "pruning          34.403531                      0.716667   \n",
       "weights          34.242483                      0.980000   \n",
       "psi              20.831486                      0.158333   \n",
       "tree              0.064096                      0.368571   \n",
       "BRL              53.689056                      0.762500   \n",
       "\n",
       "         model_accuracy_top_sem  explanation_accuracy_sem  \\\n",
       "pruning                0.004208                  0.012440   \n",
       "weights                0.003737                  0.003229   \n",
       "psi                    0.021982                  0.029882   \n",
       "tree                   0.003808                  0.003808   \n",
       "BRL                    0.003764                  0.003217   \n",
       "\n",
       "         explanation_fidelity_sem  explanation_complexity_sem  \\\n",
       "pruning                  0.011309                    1.166190   \n",
       "weights                  0.003658                    0.100000   \n",
       "psi                      0.017960                    0.494413   \n",
       "tree                     0.000000                    7.878875   \n",
       "BRL                      0.000000                    0.966667   \n",
       "\n",
       "         elapsed_time_sem  explanation_consistency_sem  \n",
       "pruning          0.784260                 3.700743e-17  \n",
       "weights          0.323599                 0.000000e+00  \n",
       "psi              0.042169                 9.251859e-18  \n",
       "tree             0.001341                 1.850372e-17  \n",
       "BRL              0.406669                 0.000000e+00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['model_accuracy_top', 'explanation_accuracy', 'explanation_fidelity', \n",
    "        'explanation_complexity', 'elapsed_time', 'explanation_consistency']\n",
    "mean_cols = [f'{c}_mean' for c in cols]\n",
    "sem_cols = [f'{c}_sem' for c in cols]\n",
    "\n",
    "# pruning\n",
    "df_mean = results_pruning[cols].mean()\n",
    "df_sem = results_pruning[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_pruning = pd.concat([df_mean, df_sem])\n",
    "summary_pruning.name = 'pruning'\n",
    "\n",
    "# # lime\n",
    "# df_mean = results_lime[cols].mean()\n",
    "# df_sem = results_lime[cols].sem()\n",
    "# df_mean.columns = mean_cols\n",
    "# df_sem.columns = sem_cols\n",
    "# summary_lime = pd.concat([df_mean, df_sem])\n",
    "# summary_lime.name = 'lime'\n",
    "\n",
    "# weights\n",
    "df_mean = results_weights[cols].mean()\n",
    "df_sem = results_weights[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_weights = pd.concat([df_mean, df_sem])\n",
    "summary_weights.name = 'weights'\n",
    "\n",
    "# psi\n",
    "df_mean = results_psi[cols].mean()\n",
    "df_sem = results_psi[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_psi = pd.concat([df_mean, df_sem])\n",
    "summary_psi.name = 'psi'\n",
    "\n",
    "# tree\n",
    "df_mean = results_tree[cols].mean()\n",
    "df_sem = results_tree[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_tree = pd.concat([df_mean, df_sem])\n",
    "summary_tree.name = 'tree'\n",
    "\n",
    "# BRL\n",
    "df_mean = results_brl[cols].mean()\n",
    "df_sem = results_brl[cols].sem()\n",
    "df_mean.columns = mean_cols\n",
    "df_sem.columns = sem_cols\n",
    "summary_brl = pd.concat([df_mean, df_sem])\n",
    "summary_brl.name = 'BRL'\n",
    "\n",
    "summary = pd.concat([summary_pruning,\n",
    "                     summary_weights, \n",
    "                     summary_psi, \n",
    "                     summary_tree,\n",
    "                     summary_brl], axis=1).T\n",
    "summary.columns = mean_cols + sem_cols\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stylish-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(os.path.join(results_dir, 'summary.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
