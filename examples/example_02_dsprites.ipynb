{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import simplify_logic\n",
    "\n",
    "from deep_logic.utils.base import validate_network\n",
    "from deep_logic.utils.relunn import get_reduced_model, prune_features\n",
    "from deep_logic import fol\n",
    "import deep_logic as dl\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('dsprites/dsprites_c_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('dsprites/dsprites_y_train.csv', index_col=0)\n",
    "x_test = pd.read_csv('dsprites/dsprites_c_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('dsprites/dsprites_y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['color', 'shape', 'scale', 'rotation', 'x_pos', 'y_pos']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_concepts = ['color', 'shape', 'scale', 'rotation', 'x_pos', 'y_pos']\n",
    "base_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['white']\n",
    "shapes = ['square', 'ellipse', 'heart']\n",
    "scale = ['very small', 'small', 's-medium', 'b-medium', 'big', 'very big']\n",
    "rotation = ['0°', '5°', '10°', '15°', '20°', '25°', '30°', '35°']\n",
    "x_pos = ['x0', 'x2', 'x4', 'x6', 'x8', 'x10', 'x12', 'x14', 'x16', 'x18', 'x20', 'x22', 'x24', 'x26', 'x28', 'x30']\n",
    "y_pos = ['y0', 'y2', 'y4', 'y6', 'y8', 'y10', 'y12', 'y14', 'y16', 'y18', 'y20', 'y22', 'y24', 'y26', 'y28', 'y30']\n",
    "concepts = colors + shapes + scale + rotation + x_pos + y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20993, 50])\n",
      "torch.Size([5530, 50])\n",
      "torch.Size([5530, 18])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(x_train.values, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20993, 18])\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([1167., 1173., 1163., 1170., 1179., 1161., 1164., 1194., 1189., 1164.,\n        1149., 1199., 1194., 1159., 1121., 1140., 1112., 1195.])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train = torch.zeros((y_train.shape[0], y_train.shape[1]), dtype=torch.float)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "print(y_train.shape)\n",
    "n_classes = y_train.size(1)\n",
    "print(n_classes)\n",
    "y_train.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train accuracy: 0.0534\n",
      "Epoch 500: train accuracy: 1.0000\n",
      "Epoch 1000: train accuracy: 1.0000\n",
      "Epoch 1500: train accuracy: 1.0000\n",
      "Epoch 2000: train accuracy: 1.0000\n",
      "Epoch 2500: train accuracy: 1.0000\n",
      "Epoch 3000: train accuracy: 1.0000\n",
      "Epoch 3500: train accuracy: 1.0000\n",
      "Epoch 4000: train accuracy: 1.0000\n",
      "Epoch 4500: train accuracy: 1.0000\n",
      "Epoch 5000: train accuracy: 1.0000\n",
      "Epoch 5500: train accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "layers = [\n",
    "    torch.nn.Linear(x_train.size(1), 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, n_classes),\n",
    "    torch.nn.Softmax(),\n",
    "]\n",
    "model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_form = torch.nn.BCELoss()\n",
    "model.train()\n",
    "need_pruning = False\n",
    "for epoch in range(6000):\n",
    "    # forward pass\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = loss_form(y_pred, y_train)\n",
    "\n",
    "    for module in model.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            loss += 0.0001 * torch.norm(module.weight, 1)\n",
    "            loss += 0.0001 * torch.norm(module.bias, 1)\n",
    "            break\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch > 3000 and need_pruning:\n",
    "        prune_features(model, n_classes, device)\n",
    "        need_pruning = False\n",
    "\n",
    "    # compute accuracy\n",
    "    if epoch % 500 == 0:\n",
    "        y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "        y_train_d = torch.argmax(y_train, dim=1)\n",
    "        accuracy = y_pred_d.eq(y_train_d).sum().item() / y_train.size(0)\n",
    "        print(f'Epoch {epoch}: train accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.19.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "PyDev console: using IPython 7.19.0\n",
      "\n",
      "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] on win32\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 127\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[2.4799e-01, 1.5274e+01, 8.3626e-03, 2.9867e+00, 6.3794e+00, 1.5824e+00,\n         1.6263e-02, 8.2743e-01, 2.2112e+01, 2.1042e-01, 3.2466e-03, 1.2069e-02,\n         2.8885e-02, 1.3216e-03, 1.2234e-02, 7.8642e-04, 7.6051e-03, 4.8397e-03,\n         3.4731e-03, 6.3829e-03, 2.2688e-03, 2.1170e-04, 3.5201e-03, 6.7903e-03,\n         4.0503e-03, 2.4064e-02, 3.8355e-03, 5.1106e-03, 8.1686e-03, 3.5161e-03,\n         9.8357e-03, 1.2965e-02, 6.4965e-03, 1.9232e-02, 1.4675e-04, 1.3629e-02,\n         1.1343e-02, 1.9440e-03, 3.4923e-03, 3.4671e-04, 5.2135e-03, 6.2713e-03,\n         9.9784e-04, 1.7147e-02, 6.2565e-03, 1.3670e-02, 8.8014e-03, 6.9079e-03,\n         8.0553e-03, 4.0583e-03],\n        [3.7659e-01, 2.7694e+01, 9.6622e-03, 2.1661e+01, 1.4869e+01, 2.7241e+01,\n         1.6233e-02, 1.5196e+00, 1.7714e-01, 7.9669e-01, 1.3957e-02, 6.8783e-03,\n         8.3452e-03, 5.3155e-03, 8.7228e-03, 7.7247e-04, 9.7830e-04, 2.8356e-02,\n         1.6911e-02, 2.4140e-03, 1.1366e-02, 2.3551e-03, 4.0370e-02, 3.8906e-03,\n         1.7597e-02, 1.2802e-02, 1.1162e-02, 2.4029e-02, 1.8458e-02, 3.5270e-02,\n         8.4573e-03, 1.8601e-02, 8.6496e-03, 2.3530e-03, 1.5738e-02, 3.5562e-03,\n         1.8599e-03, 1.3547e-02, 2.2563e-02, 3.7149e-03, 2.0618e-02, 1.2106e-02,\n         5.3415e-03, 3.5500e-03, 3.1846e-02, 2.6417e-03, 6.0979e-03, 1.4381e-02,\n         3.3089e-03, 1.3671e-02],\n        [2.1851e+00, 1.0151e+01, 7.7470e-04, 1.9912e+01, 1.6515e+01, 2.1925e+01,\n         2.5651e-02, 2.5015e+00, 1.8956e+01, 4.1460e+00, 7.5061e-03, 1.1371e-03,\n         1.7351e-02, 5.8224e-03, 1.9193e-02, 1.8228e-03, 7.2812e-03, 1.9824e-02,\n         1.1163e-02, 5.8204e-03, 6.2287e-03, 1.0956e-03, 3.2655e-02, 3.7153e-04,\n         1.7281e-02, 9.5659e-03, 1.7465e-02, 1.5260e-02, 8.1063e-03, 2.1620e-02,\n         2.3098e-03, 5.4672e-03, 1.6402e-02, 1.9136e-02, 9.8246e-03, 1.1774e-02,\n         9.4763e-03, 1.6532e-02, 1.6530e-02, 5.5989e-03, 1.9399e-02, 4.5875e-03,\n         2.2753e-03, 7.6029e-03, 2.9251e-02, 1.5263e-02, 1.2234e-02, 1.0880e-02,\n         6.3682e-04, 5.1172e-03],\n        [2.7625e+00, 1.3389e+01, 3.5325e-03, 1.3490e+01, 5.9846e+00, 1.2739e+01,\n         2.6169e-03, 3.2599e+00, 2.1144e+00, 5.0404e+00, 6.0490e-03, 1.2086e-02,\n         6.0888e-03, 5.6239e-03, 7.7767e-03, 3.5216e-03, 1.4227e-03, 1.4321e-02,\n         9.0873e-03, 1.8903e-03, 7.0038e-03, 9.5010e-04, 1.8200e-02, 5.7829e-03,\n         7.9033e-03, 1.1245e-02, 1.0444e-02, 1.1875e-02, 9.9678e-03, 1.4544e-02,\n         5.2682e-03, 1.0695e-02, 8.2576e-03, 1.1579e-03, 6.3034e-03, 3.9038e-03,\n         1.9889e-03, 8.5959e-03, 1.1804e-02, 6.2641e-03, 7.1492e-03, 5.4394e-03,\n         1.6381e-03, 8.5155e-03, 1.6015e-02, 5.3603e-04, 2.6996e-03, 1.2076e-02,\n         5.4902e-03, 4.9218e-03],\n        [5.4504e+00, 2.5762e+01, 2.0268e-02, 4.6831e+00, 7.4945e+00, 9.6451e-01,\n         3.0300e-02, 7.7616e+00, 4.4087e+01, 8.1677e+00, 1.0781e-02, 1.2743e-02,\n         5.3320e-02, 4.4763e-03, 2.1355e-02, 6.2107e-04, 1.8104e-02, 7.6789e-03,\n         7.4332e-03, 1.5339e-02, 1.2599e-02, 4.8569e-03, 9.8589e-03, 9.6925e-03,\n         3.8594e-03, 5.0784e-02, 2.0551e-02, 1.0185e-02, 1.8081e-02, 2.3855e-02,\n         2.4521e-02, 2.2602e-02, 2.0621e-02, 3.6771e-02, 1.2643e-02, 2.1978e-02,\n         2.7253e-02, 1.6318e-02, 4.3619e-03, 1.9106e-03, 4.4788e-03, 1.0572e-02,\n         1.1828e-02, 2.4879e-02, 3.1295e-03, 3.2674e-02, 1.4665e-02, 4.4006e-04,\n         5.5695e-03, 1.8113e-02],\n        [3.5283e+00, 3.8291e+01, 2.0213e-02, 2.0412e+01, 7.3217e+00, 2.1485e+01,\n         5.6390e-03, 5.8160e+00, 2.7334e+01, 4.1929e+00, 1.7868e-02, 1.3845e-02,\n         3.9682e-02, 7.1860e-03, 6.0219e-03, 3.8298e-04, 1.0437e-02, 2.7685e-02,\n         1.8260e-02, 1.1211e-02, 1.6857e-02, 4.8719e-03, 3.8496e-02, 9.2818e-03,\n         1.1837e-02, 4.1703e-02, 3.3778e-03, 2.5643e-02, 2.6089e-02, 4.2973e-02,\n         2.1931e-02, 2.9140e-02, 5.4958e-03, 2.0703e-02, 2.0340e-02, 1.0865e-02,\n         1.8166e-02, 1.1244e-03, 2.0989e-02, 1.9770e-03, 1.3773e-02, 1.6296e-02,\n         1.1491e-02, 1.8504e-02, 2.7566e-02, 1.7963e-02, 4.1580e-03, 1.1665e-02,\n         6.3410e-03, 2.2122e-02],\n        [6.7242e-02, 1.8537e+01, 6.7330e-03, 2.1680e+00, 1.5103e+00, 1.2304e+01,\n         5.3446e-04, 7.8901e-01, 9.1833e+00, 7.9989e-01, 5.6507e-03, 7.8921e-03,\n         1.6635e-02, 1.6617e-03, 4.8784e-03, 3.0004e-05, 1.6166e-03, 1.2930e-02,\n         7.1884e-03, 4.0481e-03, 1.8387e-03, 5.9576e-04, 8.6519e-03, 4.0341e-03,\n         3.8140e-03, 1.2414e-02, 3.9284e-03, 1.0768e-02, 9.5913e-03, 1.0600e-02,\n         5.6171e-03, 1.3446e-02, 7.9346e-04, 8.1658e-03, 2.6367e-03, 3.9536e-03,\n         3.4936e-03, 7.1324e-03, 1.0324e-02, 3.7717e-04, 4.6657e-03, 8.0475e-03,\n         2.0315e-03, 9.0388e-03, 3.9562e-03, 3.3424e-03, 2.6398e-03, 1.0565e-02,\n         6.4365e-03, 4.8706e-03],\n        [2.9412e+00, 1.0854e+01, 1.1433e-02, 1.4883e+01, 1.7906e+01, 1.3391e+01,\n         3.5889e-02, 3.9600e+00, 3.9692e+01, 4.7884e+00, 1.2911e-04, 1.2105e-02,\n         4.6250e-02, 3.1166e-03, 2.7596e-02, 5.7286e-04, 1.5320e-02, 7.9737e-03,\n         3.3754e-03, 1.1684e-02, 5.3461e-04, 4.1602e-04, 2.3151e-02, 7.7945e-03,\n         1.5318e-02, 3.5869e-02, 1.9575e-02, 4.7616e-03, 5.0401e-03, 6.8129e-03,\n         1.4619e-02, 1.2591e-02, 2.0652e-02, 3.5691e-02, 4.0881e-03, 2.3289e-02,\n         2.0048e-02, 1.5721e-02, 7.3604e-03, 4.6179e-03, 1.8202e-02, 4.1475e-03,\n         5.6649e-04, 2.4253e-02, 2.4172e-02, 2.8255e-02, 1.8746e-02, 1.7421e-03,\n         8.2608e-03, 4.2604e-03],\n        [9.4708e-01, 4.0485e+00, 6.3763e-03, 1.3168e+01, 1.5189e+01, 1.3476e+01,\n         2.9864e-02, 1.3310e+00, 2.8696e+01, 1.5166e+00, 2.4359e-03, 9.8692e-03,\n         3.2155e-02, 3.3490e-03, 1.9632e-02, 2.3910e-04, 1.0863e-02, 9.2108e-03,\n         4.3200e-03, 7.6049e-03, 1.1427e-03, 5.9480e-04, 2.2657e-02, 6.0921e-03,\n         1.3983e-02, 2.5346e-02, 1.3170e-02, 5.9360e-03, 1.0730e-03, 1.0983e-02,\n         9.2627e-03, 5.8616e-03, 1.4400e-02, 2.6016e-02, 5.7020e-03, 1.8839e-02,\n         1.5134e-02, 1.2334e-02, 8.0414e-03, 2.4321e-03, 1.6708e-02, 1.2607e-03,\n         1.5674e-03, 1.8694e-02, 2.1937e-02, 2.0208e-02, 1.3968e-02, 1.4271e-03,\n         6.6543e-03, 3.4525e-04],\n        [2.8734e+00, 2.1665e+00, 4.4114e-03, 1.7069e+01, 1.2023e+01, 1.0152e+01,\n         1.7231e-02, 3.9411e+00, 1.8623e+01, 4.5619e+00, 2.9499e-03, 3.3666e-04,\n         2.1660e-02, 7.6349e-03, 1.9821e-02, 2.8744e-03, 5.3360e-03, 8.9518e-03,\n         5.5460e-03, 8.0336e-03, 5.5188e-03, 1.5269e-03, 2.1976e-02, 6.9106e-04,\n         1.1608e-02, 1.1048e-02, 1.3201e-02, 6.5355e-03, 2.0453e-03, 1.1628e-02,\n         3.6917e-03, 2.2287e-03, 1.3940e-02, 1.9438e-02, 6.7786e-03, 8.7971e-03,\n         8.1915e-03, 9.1791e-03, 7.8108e-03, 6.8652e-03, 1.1836e-02, 1.0060e-03,\n         3.5685e-03, 7.9599e-03, 2.2650e-02, 1.2871e-02, 1.1150e-02, 4.4303e-03,\n         2.7752e-03, 1.4298e-03],\n        [1.6106e-02, 4.3552e+00, 4.0783e-03, 6.7075e-01, 5.1011e+00, 3.7849e+00,\n         1.3598e-02, 6.2750e-02, 1.4170e+01, 5.0667e-02, 8.4816e-04, 7.7590e-03,\n         1.6191e-02, 1.9169e-03, 6.2057e-03, 1.3348e-03, 6.4359e-03, 1.0839e-03,\n         3.3508e-04, 2.5759e-03, 2.3149e-03, 8.5179e-04, 4.0441e-03, 4.4431e-03,\n         3.9045e-03, 1.5631e-02, 4.7215e-03, 1.9634e-04, 3.4226e-03, 3.8360e-04,\n         6.5409e-03, 5.6192e-03, 5.0539e-03, 1.1365e-02, 3.6001e-04, 9.9327e-03,\n         7.8870e-03, 4.9550e-03, 1.1008e-03, 1.0261e-03, 5.7210e-03, 1.4910e-03,\n         1.7589e-03, 1.0733e-02, 3.8914e-03, 1.0269e-02, 5.5996e-03, 1.1679e-03,\n         3.6823e-03, 2.4782e-03],\n        [2.6323e+00, 1.8436e+01, 6.6444e-03, 2.5442e+00, 5.6745e+00, 6.4051e+00,\n         1.4235e-02, 2.7299e+00, 1.9330e+01, 5.2208e+00, 2.7716e-03, 1.8145e-02,\n         2.7701e-02, 2.3420e-03, 8.9534e-03, 2.5954e-03, 5.6255e-03, 8.9608e-03,\n         5.4219e-03, 3.8749e-03, 1.1577e-03, 2.0088e-03, 4.1166e-03, 8.9836e-03,\n         2.9001e-03, 2.2794e-02, 4.9988e-03, 7.9137e-03, 9.0366e-03, 6.2339e-04,\n         7.9657e-03, 1.4927e-02, 8.0144e-07, 1.6556e-02, 2.7497e-03, 1.4873e-02,\n         8.7578e-03, 5.5005e-03, 7.3104e-03, 2.2144e-03, 4.7230e-03, 7.5433e-03,\n         5.7793e-03, 2.0657e-02, 6.5572e-03, 9.2903e-03, 7.6326e-03, 1.4837e-02,\n         1.2440e-02, 7.4396e-04],\n        [3.6544e+00, 8.9067e+00, 2.0402e-03, 1.8806e+01, 1.4779e+01, 2.0499e+01,\n         2.2657e-02, 4.4998e+00, 1.8245e+01, 6.4540e+00, 6.0778e-03, 4.7385e-03,\n         1.6763e-02, 5.9712e-03, 1.9676e-02, 3.0609e-03, 6.8092e-03, 1.8529e-02,\n         1.0445e-02, 7.2394e-03, 5.4322e-03, 4.4610e-04, 2.8410e-02, 1.0882e-03,\n         1.5669e-02, 8.7623e-03, 1.9654e-02, 1.3945e-02, 7.2318e-03, 1.7258e-02,\n         2.8005e-03, 5.2162e-03, 1.7956e-02, 1.8860e-02, 7.4154e-03, 9.1497e-03,\n         9.5619e-03, 1.7473e-02, 1.5726e-02, 7.0515e-03, 1.6697e-02, 3.8890e-03,\n         4.7283e-04, 4.6509e-03, 2.6288e-02, 1.5294e-02, 1.1551e-02, 1.3007e-02,\n         1.4024e-03, 2.8734e-03],\n        [2.1580e+00, 3.9361e+00, 2.0546e-03, 8.7315e-01, 3.2154e+00, 8.0700e+00,\n         8.6113e-03, 2.5728e+00, 7.2985e+00, 3.8828e+00, 2.8478e-04, 2.6300e-03,\n         5.4648e-03, 1.8001e-03, 4.3479e-03, 7.8857e-04, 4.1890e-03, 6.1311e-03,\n         2.6074e-03, 2.6128e-03, 2.0673e-03, 1.8496e-03, 2.8001e-03, 3.1766e-04,\n         3.6090e-03, 6.4961e-03, 9.7388e-03, 4.0163e-03, 8.6481e-04, 5.8655e-04,\n         3.5986e-03, 1.8195e-03, 7.5944e-03, 6.1103e-03, 2.1271e-03, 3.1929e-03,\n         5.6694e-03, 9.7634e-03, 5.4802e-03, 1.2770e-03, 4.0014e-03, 1.8197e-03,\n         4.9072e-03, 6.9850e-04, 2.1580e-03, 7.7158e-03, 2.9249e-03, 7.6985e-03,\n         2.9679e-03, 2.6569e-03],\n        [1.7737e+00, 1.7652e+01, 8.0504e-03, 1.0415e+01, 5.1782e-01, 8.2981e+00,\n         1.0503e-02, 1.8479e+00, 1.6415e+01, 3.5427e+00, 6.7108e-03, 1.6556e-02,\n         2.2424e-02, 5.2971e-03, 6.2653e-04, 3.5312e-03, 7.0456e-03, 1.2408e-02,\n         8.7415e-03, 2.3942e-03, 8.5910e-03, 1.4592e-03, 1.2933e-02, 8.8465e-03,\n         3.1716e-03, 2.5578e-02, 3.7298e-03, 1.1412e-02, 1.2572e-02, 1.4496e-02,\n         1.1292e-02, 1.5056e-02, 1.2210e-03, 1.1356e-02, 6.4424e-03, 1.2446e-02,\n         1.0321e-02, 2.4544e-03, 9.8100e-03, 5.4106e-03, 1.5423e-03, 7.0564e-03,\n         2.9026e-03, 1.7237e-02, 1.0125e-02, 9.8357e-03, 3.2646e-03, 1.1320e-02,\n         8.0130e-03, 7.1666e-03],\n        [1.1850e+00, 1.0724e+01, 5.4070e-03, 7.8937e+00, 4.0022e+00, 7.8050e+00,\n         2.0069e-03, 1.8052e+00, 4.9248e+00, 1.5941e+00, 5.7537e-03, 2.7959e-03,\n         8.1432e-03, 3.1650e-03, 3.4265e-04, 6.9369e-05, 2.2325e-03, 9.0853e-03,\n         5.8203e-03, 2.3475e-03, 5.1105e-03, 1.8431e-03, 1.4390e-02, 1.9610e-03,\n         5.3625e-03, 9.4716e-03, 2.3323e-04, 7.9496e-03, 7.7455e-03, 1.4565e-02,\n         5.6878e-03, 8.3272e-03, 3.7273e-04, 3.0751e-03, 7.0384e-03, 8.7852e-04,\n         3.4314e-03, 1.3593e-03, 7.0203e-03, 7.4264e-04, 6.1363e-03, 4.6060e-03,\n         4.2946e-03, 3.1363e-03, 1.1147e-02, 3.3829e-03, 2.6310e-04, 3.1122e-03,\n         8.2115e-04, 7.2531e-03],\n        [5.2067e+00, 9.5829e+00, 1.2714e-02, 1.4866e+01, 1.6800e+01, 1.4994e+01,\n         3.4637e-02, 7.0843e+00, 3.9411e+01, 8.2945e+00, 7.0760e-04, 5.3185e-03,\n         4.4827e-02, 4.2754e-03, 2.8939e-02, 2.6681e-03, 1.4728e-02, 9.1796e-03,\n         3.8791e-03, 1.4348e-02, 8.6383e-04, 1.1771e-03, 2.0013e-02, 5.1870e-03,\n         1.5167e-02, 3.4850e-02, 2.4546e-02, 4.6941e-03, 4.4713e-03, 3.0816e-03,\n         1.5041e-02, 9.8963e-03, 2.4481e-02, 3.6343e-02, 1.2223e-03, 2.0231e-02,\n         2.1835e-02, 1.9966e-02, 8.8749e-03, 6.9572e-03, 1.6382e-02, 3.8281e-03,\n         3.1558e-03, 1.9394e-02, 2.2348e-02, 2.9281e-02, 1.8121e-02, 7.3313e-03,\n         4.2703e-03, 6.1521e-03],\n        [5.3833e+00, 3.0580e+01, 7.2779e-03, 1.8521e+01, 9.1375e+00, 2.7462e+01,\n         6.0122e-03, 6.0729e+00, 5.8253e+00, 1.0159e+01, 1.0801e-02, 2.4221e-02,\n         1.6680e-02, 5.4890e-03, 9.0446e-03, 5.6792e-03, 1.2933e-03, 2.9041e-02,\n         1.7206e-02, 1.8856e-03, 8.6940e-03, 3.9008e-04, 2.7840e-02, 1.1143e-02,\n         1.3442e-02, 2.0085e-02, 2.1836e-02, 2.3399e-02, 1.9064e-02, 2.2817e-02,\n         8.5285e-03, 2.3481e-02, 1.5608e-02, 1.2029e-03, 7.3856e-03, 7.7125e-03,\n         2.0526e-03, 2.0933e-02, 2.4090e-02, 9.2572e-03, 1.2518e-02, 1.2750e-02,\n         2.5601e-03, 1.8408e-02, 2.2425e-02, 2.0680e-03, 2.6325e-03, 2.7406e-02,\n         1.3955e-02, 7.0409e-03]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "tensor([[1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n       device='cuda:0')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\Mega\\research\\coding\\neural_networks\\deep-logic\\deep_logic\\fol\\relunn.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;31m# generate local explanation only if the prediction is correct\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;31m#if pred_class.eq(true_class):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m     local_explanation = fol.relunn.explain_local(model, x_test, y_test, xin, true_class, True,\n\u001B[0m\u001B[0;32m     20\u001B[0m                                                  False, concepts, device)\n\u001B[0;32m     21\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'Input {(i+1)}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Mega\\research\\coding\\neural_networks\\deep-logic\\deep_logic\\fol\\relunn.py\u001B[0m in \u001B[0;36mexplain_local\u001B[1;34m(model, x, y, x_sample, target_class, simplify, is_pruned, concept_names, device)\u001B[0m\n\u001B[0;32m    184\u001B[0m         \u001B[0mw_abs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    185\u001B[0m         \u001B[0mw_max\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_abs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 186\u001B[1;33m         \u001B[0mw_bool\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_abs\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mw_max\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    187\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_bool\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Mega\\research\\coding\\neural_networks\\deep-logic\\deep_logic\\fol\\relunn.py\u001B[0m in \u001B[0;36mexplain_local\u001B[1;34m(model, x, y, x_sample, target_class, simplify, is_pruned, concept_names, device)\u001B[0m\n\u001B[0;32m    184\u001B[0m         \u001B[0mw_abs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    185\u001B[0m         \u001B[0mw_max\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_abs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 186\u001B[1;33m         \u001B[0mw_bool\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_abs\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mw_max\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    187\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_bool\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2020.3.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1140\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1141\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1142\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1143\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2020.3.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1155\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1156\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1158\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "outputs = []\n",
    "for i, (xin, yin) in enumerate(zip(x_test, y_test)):\n",
    "    model_reduced = get_reduced_model(model, xin).to(device)\n",
    "    for module in model_reduced.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            wa = module.weight.cpu().detach().numpy()\n",
    "            break\n",
    "    output = model_reduced(xin)\n",
    "    \n",
    "    pred_class = torch.argmax(output)\n",
    "    true_class = torch.argmax(y_test[i])\n",
    "\n",
    "    # generate local explanation only if the prediction is correct\n",
    "    #if pred_class.eq(true_class):\n",
    "    local_explanation = fol.relunn.explain_local(model, x_test, y_test, xin, true_class, True,\n",
    "                                                 False, concepts, device)\n",
    "    print(f'Input {(i+1)}')\n",
    "    print(f'\\tx={xin.cpu().detach().numpy()}')\n",
    "    print(f'\\ty={y_train[i].cpu().detach().numpy()}')\n",
    "    print(f'\\ty={output.cpu().detach().numpy()}')\n",
    "    #print(f'\\tw={wa}')\n",
    "    print(f'\\tExplanation: {local_explanation}')\n",
    "    print()\n",
    "    outputs.append(output)\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_explanation, predictions, counter = fol.combine_local_explanations(model, \n",
    "                                                                          x=x_train, y=y_train, \n",
    "                                                                          target_class=0,\n",
    "                                                                          topk_explanations=1,\n",
    "                                                                          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, preds = fol.base.test_explanation(global_explanation, 0, x_test, y_test)\n",
    "final_formula = fol.base.replace_names(global_explanation, concepts)\n",
    "print(f'Accuracy when using the formula \"{final_formula}\": {accuracy:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_d = torch.argmax(y_train, dim=1)\n",
    "for i, target_class in enumerate(range(n_classes)):\n",
    "    global_explanation, predictions, counter = fol.combine_local_explanations(model, x=x_train, y=y_train, \n",
    "                                                                          target_class=target_class,\n",
    "                                                                          topk_explanations=2,\n",
    "                                                                          device=device)\n",
    "    print(i, counter)\n",
    "    if global_explanation:\n",
    "        accuracy, preds = fol.base.test_explanation(global_explanation, target_class, x_test, y_test)\n",
    "        final_formula = fol.base.replace_names(global_explanation, concepts)\n",
    "        print(f'Class {target_class} - Global explanation: \"{final_formula}\" - Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_d = torch.argmax(y_train, dim=1)\n",
    "for target_class in range(n_classes):\n",
    "    global_explanation = fol.explain_global(model, n_classes, \n",
    "                                            target_class=target_class, \n",
    "                                            concept_names=concepts, device=device)\n",
    "\n",
    "    explanation = fol.relunn.explain_global(model, n_classes, target_class, device=device)\n",
    "    if explanation not in ['False', 'True', 'The formula is too complex!']:\n",
    "        accuracy, _ = fol.relunn.test_explanation(explanation, target_class, x_train.cpu(), y_train.cpu())\n",
    "        print(f'Class {target_class} - Global explanation: \"{global_explanation}\" - Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}